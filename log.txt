W0220 02:01:42.458000 16911 torch/distributed/run.py:852] 
W0220 02:01:42.458000 16911 torch/distributed/run.py:852] *****************************************
W0220 02:01:42.458000 16911 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:01:42.458000 16911 torch/distributed/run.py:852] *****************************************
Usage: nanoplm pretrain from-yaml [OPTIONS] [CONFIG]

Error: No such option: --pure-te
Usage: nanoplm pretrain from-yaml [OPTIONS] [CONFIG]

Error: No such option: --pure-te
W0220 02:01:48.926000 16911 torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 16991 closing signal SIGTERM
E0220 02:01:49.042000 16911 torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: 2) local_rank: 0 (pid: 16990) of binary: /workspace/.venv/bin/python3
Traceback (most recent call last):
  File "/workspace/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 991, in main
    run(args)
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 982, in run
    elastic_launch(
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 170, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 317, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/workspace/.venv/bin/nanoplm FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2026-02-20_02:01:49
  host      : f8173ef52a94
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 16991)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 16991
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-20_02:01:48
  host      : f8173ef52a94
  rank      : 0 (local_rank: 0)
  exitcode  : 2 (pid: 16990)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0220 02:03:55.489000 17838 torch/distributed/run.py:852] 
W0220 02:03:55.489000 17838 torch/distributed/run.py:852] *****************************************
W0220 02:03:55.489000 17838 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:03:55.489000 17838 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 62601.55it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 60787.01it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 61455.00it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1067.80it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 168888352
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Created directory: output/pretraining_checkpoints
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020204
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 168888352
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020204-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 1,107,492,864 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run o7jfcn9s
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_020405-o7jfcn9s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020204-2
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/o7jfcn9s
MFU estimation: 1,107,492,864 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.3, <= 2.8.3 by pip3 install flash-attn==<version>.
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 1/8250] loss=3.5664 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=7.6558 tok/s=1,993 raw_tok/s=2,001 step_tokens=130,519 waste=0.4% h100_mfu=0.13% vram=754/31,434MB peak=29,266/31,434MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 2/8250] loss=3.5779 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=14.2323 tok/s=116,263 raw_tok/s=116,514 step_tokens=130,790 waste=0.2% h100_mfu=7.72% vram=754/31,434MB peak=29,542/31,434MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 3/8250] loss=3.5840 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=14.1388 tok/s=115,237 raw_tok/s=115,516 step_tokens=130,755 waste=0.2% h100_mfu=7.66% vram=754/31,892MB peak=29,590/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 4/8250] loss=3.5705 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=14.1119 tok/s=776,034 raw_tok/s=776,976 step_tokens=130,913 waste=0.1% h100_mfu=51.50% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 5/8250] loss=3.5646 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=14.0940 tok/s=793,097 raw_tok/s=795,957 step_tokens=130,601 waste=0.4% h100_mfu=52.76% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 6/8250] loss=3.5600 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=14.1323 tok/s=743,943 raw_tok/s=746,004 step_tokens=130,710 waste=0.3% h100_mfu=49.45% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 7/8250] loss=3.5639 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=14.1196 tok/s=746,367 raw_tok/s=747,605 step_tokens=130,855 waste=0.2% h100_mfu=49.56% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 8/8250] loss=3.5547 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=13.9778 tok/s=746,718 raw_tok/s=748,631 step_tokens=130,737 waste=0.3% h100_mfu=49.62% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 9/8250] loss=3.5272 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=13.8111 tok/s=792,039 raw_tok/s=793,504 step_tokens=130,830 waste=0.2% h100_mfu=52.60% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 10/8250] loss=3.5389 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=13.7895 tok/s=744,543 raw_tok/s=745,829 step_tokens=130,846 waste=0.2% h100_mfu=49.44% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 11/8250] loss=3.5072 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=13.3981 tok/s=755,456 raw_tok/s=757,473 step_tokens=130,723 waste=0.3% h100_mfu=50.21% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 12/8250] loss=3.4662 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=13.0058 tok/s=746,008 raw_tok/s=746,926 step_tokens=130,911 waste=0.1% h100_mfu=49.51% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 13/8250] loss=3.4314 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=12.7147 tok/s=758,524 raw_tok/s=759,741 step_tokens=130,862 waste=0.2% h100_mfu=50.36% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 14/8250] loss=3.4116 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=12.5022 tok/s=739,918 raw_tok/s=743,161 step_tokens=130,500 waste=0.4% h100_mfu=49.26% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 15/8250] loss=3.3653 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=12.0995 tok/s=759,918 raw_tok/s=761,149 step_tokens=130,860 waste=0.2% h100_mfu=50.45% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 16/8250] loss=3.3408 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=11.6414 tok/s=736,352 raw_tok/s=736,745 step_tokens=131,002 waste=0.1% h100_mfu=48.84% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 17/8250] loss=3.2910 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=11.1040 tok/s=793,012 raw_tok/s=794,509 step_tokens=130,825 waste=0.2% h100_mfu=52.66% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 18/8250] loss=3.2560 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=10.5420 tok/s=753,311 raw_tok/s=755,524 step_tokens=130,688 waste=0.3% h100_mfu=50.08% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 19/8250] loss=3.1942 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=9.9419 tok/s=785,852 raw_tok/s=790,544 step_tokens=130,294 waste=0.6% h100_mfu=52.40% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 20/8250] loss=3.1661 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=9.4145 tok/s=723,768 raw_tok/s=725,544 step_tokens=130,751 waste=0.2% h100_mfu=48.09% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 21/8250] loss=3.1276 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=8.8214 tok/s=758,674 raw_tok/s=760,415 step_tokens=130,772 waste=0.2% h100_mfu=50.40% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 22/8250] loss=3.0860 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=8.1867 tok/s=788,851 raw_tok/s=790,074 step_tokens=130,869 waste=0.2% h100_mfu=52.37% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 23/8250] loss=3.0355 lr=7.28e-06 muon_lr=7.28e-05 grad_norm=7.5369 tok/s=751,929 raw_tok/s=755,358 step_tokens=130,477 waste=0.5% h100_mfu=50.07% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 24/8250] loss=2.9980 lr=7.62e-06 muon_lr=7.62e-05 grad_norm=6.8678 tok/s=773,391 raw_tok/s=775,083 step_tokens=130,786 waste=0.2% h100_mfu=51.38% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 25/8250] loss=2.9705 lr=7.95e-06 muon_lr=7.95e-05 grad_norm=6.3038 tok/s=755,635 raw_tok/s=758,384 step_tokens=130,597 waste=0.4% h100_mfu=50.27% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 26/8250] loss=2.9383 lr=8.28e-06 muon_lr=8.28e-05 grad_norm=5.7421 tok/s=754,110 raw_tok/s=756,338 step_tokens=130,686 waste=0.3% h100_mfu=50.13% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 27/8250] loss=2.9129 lr=8.61e-06 muon_lr=8.61e-05 grad_norm=5.3099 tok/s=749,397 raw_tok/s=753,647 step_tokens=130,333 waste=0.6% h100_mfu=49.96% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 28/8250] loss=2.8959 lr=8.94e-06 muon_lr=8.94e-05 grad_norm=4.9677 tok/s=743,013 raw_tok/s=745,509 step_tokens=130,633 waste=0.3% h100_mfu=49.42% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 29/8250] loss=2.8682 lr=9.27e-06 muon_lr=9.27e-05 grad_norm=4.7253 tok/s=747,224 raw_tok/s=748,698 step_tokens=130,814 waste=0.2% h100_mfu=49.63% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 30/8250] loss=2.8521 lr=9.60e-06 muon_lr=9.60e-05 grad_norm=4.4938 tok/s=720,702 raw_tok/s=723,146 step_tokens=130,629 waste=0.3% h100_mfu=47.93% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 31/8250] loss=2.8310 lr=9.93e-06 muon_lr=9.93e-05 grad_norm=4.1842 tok/s=744,915 raw_tok/s=746,944 step_tokens=130,716 waste=0.3% h100_mfu=49.51% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 32/8250] loss=2.8430 lr=1.03e-05 muon_lr=1.03e-04 grad_norm=4.0886 tok/s=787,391 raw_tok/s=789,384 step_tokens=130,741 waste=0.3% h100_mfu=52.32% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 33/8250] loss=2.8093 lr=1.06e-05 muon_lr=1.06e-04 grad_norm=3.9344 tok/s=742,530 raw_tok/s=744,290 step_tokens=130,762 waste=0.2% h100_mfu=49.34% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 34/8250] loss=2.7940 lr=1.09e-05 muon_lr=1.09e-04 grad_norm=3.5105 tok/s=745,627 raw_tok/s=746,361 step_tokens=130,943 waste=0.1% h100_mfu=49.47% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 35/8250] loss=2.8084 lr=1.13e-05 muon_lr=1.13e-04 grad_norm=3.3546 tok/s=746,820 raw_tok/s=748,705 step_tokens=130,742 waste=0.3% h100_mfu=49.63% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 36/8250] loss=2.8075 lr=1.16e-05 muon_lr=1.16e-04 grad_norm=3.1564 tok/s=753,838 raw_tok/s=756,048 step_tokens=130,689 waste=0.3% h100_mfu=50.11% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 37/8250] loss=2.8067 lr=1.19e-05 muon_lr=1.19e-04 grad_norm=2.8111 tok/s=755,347 raw_tok/s=755,664 step_tokens=131,017 waste=0.0% h100_mfu=50.09% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 38/8250] loss=2.7862 lr=1.23e-05 muon_lr=1.23e-04 grad_norm=2.8834 tok/s=741,446 raw_tok/s=745,490 step_tokens=130,361 waste=0.5% h100_mfu=49.41% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 39/8250] loss=2.7961 lr=1.26e-05 muon_lr=1.26e-04 grad_norm=2.6950 tok/s=726,535 raw_tok/s=729,278 step_tokens=130,579 waste=0.4% h100_mfu=48.34% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 40/8250] loss=2.7842 lr=1.29e-05 muon_lr=1.29e-04 grad_norm=2.7445 tok/s=753,714 raw_tok/s=754,030 step_tokens=131,017 waste=0.0% h100_mfu=49.98% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 41/8250] loss=2.7962 lr=1.32e-05 muon_lr=1.32e-04 grad_norm=2.7494 tok/s=753,530 raw_tok/s=754,543 step_tokens=130,896 waste=0.1% h100_mfu=50.02% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 42/8250] loss=2.7804 lr=1.36e-05 muon_lr=1.36e-04 grad_norm=2.5163 tok/s=757,248 raw_tok/s=758,063 step_tokens=130,931 waste=0.1% h100_mfu=50.25% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 43/8250] loss=2.7755 lr=1.39e-05 muon_lr=1.39e-04 grad_norm=2.3097 tok/s=772,953 raw_tok/s=774,069 step_tokens=130,883 waste=0.1% h100_mfu=51.31% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 44/8250] loss=2.7719 lr=1.42e-05 muon_lr=1.42e-04 grad_norm=2.1254 tok/s=763,853 raw_tok/s=765,242 step_tokens=130,834 waste=0.2% h100_mfu=50.72% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 45/8250] loss=2.7375 lr=1.46e-05 muon_lr=1.46e-04 grad_norm=1.8092 tok/s=729,954 raw_tok/s=733,192 step_tokens=130,493 waste=0.4% h100_mfu=48.60% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 46/8250] loss=2.7390 lr=1.49e-05 muon_lr=1.49e-04 grad_norm=1.8980 tok/s=736,699 raw_tok/s=739,905 step_tokens=130,504 waste=0.4% h100_mfu=49.04% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 47/8250] loss=2.7494 lr=1.52e-05 muon_lr=1.52e-04 grad_norm=1.5376 tok/s=728,769 raw_tok/s=730,542 step_tokens=130,754 waste=0.2% h100_mfu=48.42% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 48/8250] loss=2.7390 lr=1.56e-05 muon_lr=1.56e-04 grad_norm=1.4337 tok/s=722,408 raw_tok/s=724,270 step_tokens=130,735 waste=0.3% h100_mfu=48.01% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 49/8250] loss=2.7272 lr=1.59e-05 muon_lr=1.59e-04 grad_norm=1.3638 tok/s=745,019 raw_tok/s=745,503 step_tokens=130,987 waste=0.1% h100_mfu=49.42% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 50/8250] loss=2.7223 lr=1.62e-05 muon_lr=1.62e-04 grad_norm=1.3052 tok/s=705,832 raw_tok/s=706,948 step_tokens=130,865 waste=0.2% h100_mfu=46.86% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 51/8250] loss=2.7127 lr=1.66e-05 muon_lr=1.66e-04 grad_norm=1.5367 tok/s=755,970 raw_tok/s=756,986 step_tokens=130,896 waste=0.1% h100_mfu=50.18% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 52/8250] loss=2.7214 lr=1.69e-05 muon_lr=1.69e-04 grad_norm=1.4619 tok/s=732,941 raw_tok/s=733,994 step_tokens=130,884 waste=0.1% h100_mfu=48.65% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 53/8250] loss=2.7077 lr=1.72e-05 muon_lr=1.72e-04 grad_norm=1.3780 tok/s=775,604 raw_tok/s=778,133 step_tokens=130,646 waste=0.3% h100_mfu=51.58% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 54/8250] loss=2.7165 lr=1.75e-05 muon_lr=1.75e-04 grad_norm=1.1904 tok/s=752,225 raw_tok/s=752,587 step_tokens=131,009 waste=0.0% h100_mfu=49.89% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 55/8250] loss=2.7090 lr=1.79e-05 muon_lr=1.79e-04 grad_norm=0.9922 tok/s=713,669 raw_tok/s=714,711 step_tokens=130,881 waste=0.1% h100_mfu=47.37% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 56/8250] loss=2.7084 lr=1.82e-05 muon_lr=1.82e-04 grad_norm=0.9814 tok/s=723,294 raw_tok/s=724,532 step_tokens=130,848 waste=0.2% h100_mfu=48.03% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 57/8250] loss=2.7062 lr=1.85e-05 muon_lr=1.85e-04 grad_norm=0.8793 tok/s=690,477 raw_tok/s=692,728 step_tokens=130,646 waste=0.3% h100_mfu=45.92% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 58/8250] loss=2.7019 lr=1.89e-05 muon_lr=1.89e-04 grad_norm=0.8890 tok/s=743,218 raw_tok/s=744,360 step_tokens=130,871 waste=0.2% h100_mfu=49.34% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 59/8250] loss=2.7075 lr=1.92e-05 muon_lr=1.92e-04 grad_norm=0.8557 tok/s=712,060 raw_tok/s=714,223 step_tokens=130,675 waste=0.3% h100_mfu=47.34% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 60/8250] loss=2.6850 lr=1.95e-05 muon_lr=1.95e-04 grad_norm=0.8159 tok/s=787,493 raw_tok/s=788,131 step_tokens=130,966 waste=0.1% h100_mfu=52.24% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 61/8250] loss=2.7082 lr=1.99e-05 muon_lr=1.99e-04 grad_norm=1.0580 tok/s=744,169 raw_tok/s=746,853 step_tokens=130,601 waste=0.4% h100_mfu=49.51% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 62/8250] loss=2.7054 lr=2.02e-05 muon_lr=2.02e-04 grad_norm=0.6796 tok/s=748,399 raw_tok/s=749,846 step_tokens=130,819 waste=0.2% h100_mfu=49.70% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 63/8250] loss=2.7082 lr=2.05e-05 muon_lr=2.05e-04 grad_norm=0.7394 tok/s=741,793 raw_tok/s=743,148 step_tokens=130,833 waste=0.2% h100_mfu=49.26% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 64/8250] loss=2.7090 lr=2.09e-05 muon_lr=2.09e-04 grad_norm=0.7449 tok/s=783,594 raw_tok/s=785,260 step_tokens=130,794 waste=0.2% h100_mfu=52.05% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 65/8250] loss=2.6822 lr=2.12e-05 muon_lr=2.12e-04 grad_norm=0.7212 tok/s=748,267 raw_tok/s=752,262 step_tokens=130,376 waste=0.5% h100_mfu=49.86% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 66/8250] loss=2.6783 lr=2.15e-05 muon_lr=2.15e-04 grad_norm=0.7097 tok/s=751,092 raw_tok/s=754,367 step_tokens=130,503 waste=0.4% h100_mfu=50.00% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 67/8250] loss=2.6862 lr=2.19e-05 muon_lr=2.19e-04 grad_norm=0.6337 tok/s=736,683 raw_tok/s=738,181 step_tokens=130,806 waste=0.2% h100_mfu=48.93% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 68/8250] loss=2.6844 lr=2.22e-05 muon_lr=2.22e-04 grad_norm=0.6831 tok/s=736,241 raw_tok/s=736,309 step_tokens=131,060 waste=0.0% h100_mfu=48.81% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 69/8250] loss=2.6804 lr=2.25e-05 muon_lr=2.25e-04 grad_norm=0.8864 tok/s=788,429 raw_tok/s=791,188 step_tokens=130,615 waste=0.3% h100_mfu=52.44% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 70/8250] loss=2.6849 lr=2.28e-05 muon_lr=2.28e-04 grad_norm=0.8795 tok/s=755,177 raw_tok/s=757,402 step_tokens=130,687 waste=0.3% h100_mfu=50.20% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 71/8250] loss=2.6742 lr=2.32e-05 muon_lr=2.32e-04 grad_norm=0.8337 tok/s=753,199 raw_tok/s=757,110 step_tokens=130,395 waste=0.5% h100_mfu=50.19% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 72/8250] loss=2.6636 lr=2.35e-05 muon_lr=2.35e-04 grad_norm=0.6382 tok/s=753,517 raw_tok/s=756,576 step_tokens=130,542 waste=0.4% h100_mfu=50.15% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 73/8250] loss=2.6893 lr=2.38e-05 muon_lr=2.38e-04 grad_norm=0.7326 tok/s=786,364 raw_tok/s=789,526 step_tokens=130,547 waste=0.4% h100_mfu=52.33% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 74/8250] loss=2.6748 lr=2.42e-05 muon_lr=2.42e-04 grad_norm=0.8023 tok/s=751,864 raw_tok/s=752,484 step_tokens=130,964 waste=0.1% h100_mfu=49.88% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 75/8250] loss=2.6646 lr=2.45e-05 muon_lr=2.45e-04 grad_norm=0.8071 tok/s=786,397 raw_tok/s=790,292 step_tokens=130,426 waste=0.5% h100_mfu=52.38% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 76/8250] loss=2.6837 lr=2.48e-05 muon_lr=2.48e-04 grad_norm=0.7724 tok/s=768,553 raw_tok/s=771,336 step_tokens=130,599 waste=0.4% h100_mfu=51.13% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 77/8250] loss=2.6718 lr=2.52e-05 muon_lr=2.52e-04 grad_norm=0.6535 tok/s=773,617 raw_tok/s=774,681 step_tokens=130,892 waste=0.1% h100_mfu=51.35% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 78/8250] loss=2.6794 lr=2.55e-05 muon_lr=2.55e-04 grad_norm=0.6925 tok/s=747,656 raw_tok/s=749,434 step_tokens=130,761 waste=0.2% h100_mfu=49.68% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 79/8250] loss=2.6866 lr=2.58e-05 muon_lr=2.58e-04 grad_norm=0.7359 tok/s=755,021 raw_tok/s=756,516 step_tokens=130,813 waste=0.2% h100_mfu=50.15% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 80/8250] loss=2.6877 lr=2.62e-05 muon_lr=2.62e-04 grad_norm=0.7751 tok/s=748,073 raw_tok/s=750,558 step_tokens=130,638 waste=0.3% h100_mfu=49.75% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 81/8250] loss=2.6649 lr=2.65e-05 muon_lr=2.65e-04 grad_norm=0.8326 tok/s=725,584 raw_tok/s=726,753 step_tokens=130,861 waste=0.2% h100_mfu=48.17% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 82/8250] loss=2.6662 lr=2.68e-05 muon_lr=2.68e-04 grad_norm=0.7560 tok/s=733,914 raw_tok/s=734,687 step_tokens=130,934 waste=0.1% h100_mfu=48.70% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 83/8250] loss=2.6781 lr=2.72e-05 muon_lr=2.72e-04 grad_norm=1.1930 tok/s=740,471 raw_tok/s=741,229 step_tokens=130,938 waste=0.1% h100_mfu=49.13% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 84/8250] loss=2.6687 lr=2.75e-05 muon_lr=2.75e-04 grad_norm=1.1763 tok/s=781,940 raw_tok/s=783,147 step_tokens=130,870 waste=0.2% h100_mfu=51.91% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 85/8250] loss=2.6700 lr=2.78e-05 muon_lr=2.78e-04 grad_norm=1.2016 tok/s=751,537 raw_tok/s=752,622 step_tokens=130,883 waste=0.1% h100_mfu=49.89% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 86/8250] loss=2.6757 lr=2.81e-05 muon_lr=2.81e-04 grad_norm=1.2643 tok/s=707,994 raw_tok/s=709,526 step_tokens=130,789 waste=0.2% h100_mfu=47.03% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 87/8250] loss=2.6560 lr=2.85e-05 muon_lr=2.85e-04 grad_norm=0.9753 tok/s=778,200 raw_tok/s=779,866 step_tokens=130,792 waste=0.2% h100_mfu=51.69% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 88/8250] loss=2.6598 lr=2.88e-05 muon_lr=2.88e-04 grad_norm=0.8701 tok/s=755,770 raw_tok/s=756,664 step_tokens=130,917 waste=0.1% h100_mfu=50.16% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 89/8250] loss=2.6693 lr=2.91e-05 muon_lr=2.91e-04 grad_norm=0.9662 tok/s=777,044 raw_tok/s=780,486 step_tokens=130,494 waste=0.4% h100_mfu=51.73% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 90/8250] loss=2.6709 lr=2.95e-05 muon_lr=2.95e-04 grad_norm=0.9537 tok/s=768,279 raw_tok/s=770,089 step_tokens=130,764 waste=0.2% h100_mfu=51.05% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 91/8250] loss=2.6627 lr=2.98e-05 muon_lr=2.98e-04 grad_norm=0.9048 tok/s=786,772 raw_tok/s=789,725 step_tokens=130,582 waste=0.4% h100_mfu=52.35% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 92/8250] loss=2.6553 lr=3.01e-05 muon_lr=3.01e-04 grad_norm=1.0803 tok/s=789,210 raw_tok/s=790,736 step_tokens=130,819 waste=0.2% h100_mfu=52.41% vram=754/31,892MB peak=29,542/31,892MB
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[step 93/8250] loss=2.6606 lr=3.05e-05 muon_lr=3.05e-04 grad_norm=0.9345 tok/s=752,133 raw_tok/s=753,864 step_tokens=130,771 waste=0.2% h100_mfu=49.97% vram=754/31,892MB peak=29,588/31,892MB
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
[INFO     | DotProductAttention]: Running with FlashAttention backend (version 3.0.0)
[INFO     | DotProductAttention]: Running with FusedAttention backend (sub-backend 1)
W0220 02:05:36.504000 19383 torch/distributed/run.py:852] 
W0220 02:05:36.504000 19383 torch/distributed/run.py:852] *****************************************
W0220 02:05:36.504000 19383 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:05:36.504000 19383 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1222.47it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 49200.05it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 49200.05it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 19217.89it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 168888352
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020205
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 168888352
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020205-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 1,107,492,864 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 422lbpk5
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_020546-422lbpk5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020205-2
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/422lbpk5
MFU estimation: 1,107,492,864 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8250] loss=3.5661 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=7.6552 tok/s=5,418 raw_tok/s=5,441 step_tokens=130,519 waste=0.4% h100_mfu=0.36% vram=754/30,316MB peak=29,192/30,316MB
[step 2/8250] loss=3.5765 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=14.2346 tok/s=808,843 raw_tok/s=810,587 step_tokens=130,790 waste=0.2% h100_mfu=53.73% vram=754/30,572MB peak=29,514/30,572MB
[step 3/8250] loss=3.5818 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=14.1384 tok/s=778,808 raw_tok/s=780,696 step_tokens=130,755 waste=0.2% h100_mfu=51.75% vram=754/30,572MB peak=29,514/30,572MB
[step 4/8250] loss=3.5734 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=14.1238 tok/s=780,864 raw_tok/s=781,812 step_tokens=130,913 waste=0.1% h100_mfu=51.82% vram=754/30,572MB peak=29,516/30,572MB
[step 5/8250] loss=3.5638 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=14.0908 tok/s=768,487 raw_tok/s=771,258 step_tokens=130,601 waste=0.4% h100_mfu=51.12% vram=754/30,572MB peak=29,516/30,572MB
[step 6/8250] loss=3.5563 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=14.1327 tok/s=761,483 raw_tok/s=763,592 step_tokens=130,710 waste=0.3% h100_mfu=50.61% vram=754/30,572MB peak=29,516/30,572MB
[step 7/8250] loss=3.5633 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=14.1186 tok/s=761,981 raw_tok/s=763,244 step_tokens=130,855 waste=0.2% h100_mfu=50.59% vram=754/30,572MB peak=29,516/30,572MB
[step 8/8250] loss=3.5529 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=13.9835 tok/s=756,803 raw_tok/s=758,742 step_tokens=130,737 waste=0.3% h100_mfu=50.29% vram=754/30,572MB peak=29,516/30,572MB
[step 9/8250] loss=3.5276 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=13.8079 tok/s=758,799 raw_tok/s=760,203 step_tokens=130,830 waste=0.2% h100_mfu=50.39% vram=754/30,572MB peak=29,516/30,572MB
[step 10/8250] loss=3.5374 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=13.7870 tok/s=761,106 raw_tok/s=762,420 step_tokens=130,846 waste=0.2% h100_mfu=50.54% vram=754/30,572MB peak=29,516/30,572MB
[step 11/8250] loss=3.5091 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=13.4121 tok/s=758,843 raw_tok/s=760,869 step_tokens=130,723 waste=0.3% h100_mfu=50.43% vram=754/30,572MB peak=29,516/30,572MB
[step 12/8250] loss=3.4675 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=13.0116 tok/s=761,374 raw_tok/s=762,311 step_tokens=130,911 waste=0.1% h100_mfu=50.53% vram=754/30,572MB peak=29,516/30,572MB
[step 13/8250] loss=3.4298 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=12.7080 tok/s=730,510 raw_tok/s=731,682 step_tokens=130,862 waste=0.2% h100_mfu=48.50% vram=754/30,572MB peak=29,516/30,572MB
[step 14/8250] loss=3.4141 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=12.4973 tok/s=740,680 raw_tok/s=743,926 step_tokens=130,500 waste=0.4% h100_mfu=49.31% vram=754/30,572MB peak=29,516/30,572MB
[step 15/8250] loss=3.3665 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=12.1063 tok/s=760,188 raw_tok/s=761,420 step_tokens=130,860 waste=0.2% h100_mfu=50.47% vram=754/30,572MB peak=29,516/30,572MB
[step 16/8250] loss=3.3414 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=11.6536 tok/s=763,348 raw_tok/s=763,756 step_tokens=131,002 waste=0.1% h100_mfu=50.63% vram=754/30,572MB peak=29,516/30,572MB
[step 17/8250] loss=3.2919 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=11.1172 tok/s=776,167 raw_tok/s=777,633 step_tokens=130,825 waste=0.2% h100_mfu=51.55% vram=754/30,572MB peak=29,516/30,572MB
[step 18/8250] loss=3.2576 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=10.5478 tok/s=735,181 raw_tok/s=737,341 step_tokens=130,688 waste=0.3% h100_mfu=48.87% vram=754/30,572MB peak=29,516/30,572MB
[step 19/8250] loss=3.1940 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=9.9431 tok/s=765,118 raw_tok/s=769,687 step_tokens=130,294 waste=0.6% h100_mfu=51.02% vram=754/30,572MB peak=29,516/30,572MB
[step 20/8250] loss=3.1662 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=9.4175 tok/s=786,394 raw_tok/s=788,325 step_tokens=130,751 waste=0.2% h100_mfu=52.25% vram=754/30,572MB peak=29,516/30,572MB
[step 21/8250] loss=3.1268 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=8.8289 tok/s=783,709 raw_tok/s=785,507 step_tokens=130,772 waste=0.2% h100_mfu=52.07% vram=754/30,572MB peak=29,516/30,572MB
[step 22/8250] loss=3.0834 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=8.1855 tok/s=762,304 raw_tok/s=763,487 step_tokens=130,869 waste=0.2% h100_mfu=50.61% vram=754/30,572MB peak=29,516/30,572MB
[step 23/8250] loss=3.0397 lr=7.28e-06 muon_lr=7.28e-05 grad_norm=7.5387 tok/s=751,358 raw_tok/s=754,785 step_tokens=130,477 waste=0.5% h100_mfu=50.03% vram=754/30,572MB peak=29,516/30,572MB
[step 24/8250] loss=2.9987 lr=7.62e-06 muon_lr=7.62e-05 grad_norm=6.8720 tok/s=771,806 raw_tok/s=773,493 step_tokens=130,786 waste=0.2% h100_mfu=51.27% vram=754/30,572MB peak=29,516/30,572MB
[step 25/8250] loss=2.9687 lr=7.95e-06 muon_lr=7.95e-05 grad_norm=6.2996 tok/s=752,966 raw_tok/s=755,705 step_tokens=130,597 waste=0.4% h100_mfu=50.09% vram=754/30,572MB peak=29,516/30,572MB
[step 26/8250] loss=2.9377 lr=8.28e-06 muon_lr=8.28e-05 grad_norm=5.7474 tok/s=735,522 raw_tok/s=737,695 step_tokens=130,686 waste=0.3% h100_mfu=48.90% vram=754/30,572MB peak=29,516/30,572MB
[step 27/8250] loss=2.9126 lr=8.61e-06 muon_lr=8.61e-05 grad_norm=5.3085 tok/s=713,451 raw_tok/s=717,497 step_tokens=130,333 waste=0.6% h100_mfu=47.56% vram=754/30,572MB peak=29,516/30,572MB
[step 28/8250] loss=2.8927 lr=8.94e-06 muon_lr=8.94e-05 grad_norm=4.9673 tok/s=758,604 raw_tok/s=761,154 step_tokens=130,633 waste=0.3% h100_mfu=50.45% vram=754/30,572MB peak=29,516/30,572MB
[step 29/8250] loss=2.8655 lr=9.27e-06 muon_lr=9.27e-05 grad_norm=4.7192 tok/s=756,587 raw_tok/s=758,079 step_tokens=130,814 waste=0.2% h100_mfu=50.25% vram=754/30,572MB peak=29,516/30,572MB
[step 30/8250] loss=2.8498 lr=9.60e-06 muon_lr=9.60e-05 grad_norm=4.4851 tok/s=689,462 raw_tok/s=691,800 step_tokens=130,629 waste=0.3% h100_mfu=45.86% vram=754/30,572MB peak=29,516/30,572MB
[step 31/8250] loss=2.8331 lr=9.93e-06 muon_lr=9.93e-05 grad_norm=4.1848 tok/s=753,102 raw_tok/s=755,153 step_tokens=130,716 waste=0.3% h100_mfu=50.06% vram=754/30,572MB peak=29,516/30,572MB
[step 32/8250] loss=2.8423 lr=1.03e-05 muon_lr=1.03e-04 grad_norm=4.0877 tok/s=774,755 raw_tok/s=776,716 step_tokens=130,741 waste=0.3% h100_mfu=51.48% vram=754/30,572MB peak=29,516/30,572MB
[step 33/8250] loss=2.8127 lr=1.06e-05 muon_lr=1.06e-04 grad_norm=3.9325 tok/s=736,433 raw_tok/s=738,179 step_tokens=130,762 waste=0.2% h100_mfu=48.93% vram=754/30,572MB peak=29,516/30,572MB
[step 34/8250] loss=2.7937 lr=1.09e-05 muon_lr=1.09e-04 grad_norm=3.5226 tok/s=731,479 raw_tok/s=732,199 step_tokens=130,943 waste=0.1% h100_mfu=48.53% vram=754/30,572MB peak=29,516/30,572MB
[step 35/8250] loss=2.8133 lr=1.13e-05 muon_lr=1.13e-04 grad_norm=3.3822 tok/s=752,367 raw_tok/s=754,267 step_tokens=130,742 waste=0.3% h100_mfu=50.00% vram=754/30,572MB peak=29,516/30,572MB
[step 36/8250] loss=2.8086 lr=1.16e-05 muon_lr=1.16e-04 grad_norm=3.1429 tok/s=756,750 raw_tok/s=758,967 step_tokens=130,689 waste=0.3% h100_mfu=50.31% vram=754/30,572MB peak=29,516/30,572MB
[step 37/8250] loss=2.8119 lr=1.19e-05 muon_lr=1.19e-04 grad_norm=2.8111 tok/s=753,572 raw_tok/s=753,889 step_tokens=131,017 waste=0.0% h100_mfu=49.97% vram=754/30,572MB peak=29,516/30,572MB
[step 38/8250] loss=2.7756 lr=1.23e-05 muon_lr=1.23e-04 grad_norm=2.8777 tok/s=738,410 raw_tok/s=742,437 step_tokens=130,361 waste=0.5% h100_mfu=49.21% vram=754/30,572MB peak=29,516/30,572MB
[step 39/8250] loss=2.8010 lr=1.26e-05 muon_lr=1.26e-04 grad_norm=2.6926 tok/s=737,168 raw_tok/s=739,951 step_tokens=130,579 waste=0.4% h100_mfu=49.05% vram=754/30,572MB peak=29,516/30,572MB
[step 40/8250] loss=2.7930 lr=1.29e-05 muon_lr=1.29e-04 grad_norm=2.7564 tok/s=757,904 raw_tok/s=758,222 step_tokens=131,017 waste=0.0% h100_mfu=50.26% vram=754/30,572MB peak=29,516/30,572MB
[step 41/8250] loss=2.7881 lr=1.32e-05 muon_lr=1.32e-04 grad_norm=2.7739 tok/s=758,725 raw_tok/s=759,745 step_tokens=130,896 waste=0.1% h100_mfu=50.36% vram=754/30,572MB peak=29,516/30,572MB
[step 42/8250] loss=2.7740 lr=1.36e-05 muon_lr=1.36e-04 grad_norm=2.5181 tok/s=762,032 raw_tok/s=762,852 step_tokens=130,931 waste=0.1% h100_mfu=50.57% vram=754/30,572MB peak=29,516/30,572MB
[step 43/8250] loss=2.7730 lr=1.39e-05 muon_lr=1.39e-04 grad_norm=2.3051 tok/s=754,285 raw_tok/s=755,374 step_tokens=130,883 waste=0.1% h100_mfu=50.07% vram=754/30,572MB peak=29,516/30,572MB
[step 44/8250] loss=2.7725 lr=1.42e-05 muon_lr=1.42e-04 grad_norm=2.1275 tok/s=780,687 raw_tok/s=782,107 step_tokens=130,834 waste=0.2% h100_mfu=51.84% vram=754/30,572MB peak=29,516/30,572MB
[step 45/8250] loss=2.7329 lr=1.46e-05 muon_lr=1.46e-04 grad_norm=1.8069 tok/s=758,942 raw_tok/s=762,309 step_tokens=130,493 waste=0.4% h100_mfu=50.53% vram=754/30,572MB peak=29,516/30,572MB
[step 46/8250] loss=2.7447 lr=1.49e-05 muon_lr=1.49e-04 grad_norm=1.8917 tok/s=757,316 raw_tok/s=760,612 step_tokens=130,504 waste=0.4% h100_mfu=50.42% vram=754/30,572MB peak=29,516/30,572MB
[step 47/8250] loss=2.7505 lr=1.52e-05 muon_lr=1.52e-04 grad_norm=1.5476 tok/s=717,414 raw_tok/s=719,159 step_tokens=130,754 waste=0.2% h100_mfu=47.67% vram=754/30,572MB peak=29,516/30,572MB
[step 48/8250] loss=2.7337 lr=1.56e-05 muon_lr=1.56e-04 grad_norm=1.4341 tok/s=784,811 raw_tok/s=786,834 step_tokens=130,735 waste=0.3% h100_mfu=52.16% vram=754/30,572MB peak=29,516/30,572MB
[step 49/8250] loss=2.7287 lr=1.59e-05 muon_lr=1.59e-04 grad_norm=1.3746 tok/s=743,665 raw_tok/s=744,148 step_tokens=130,987 waste=0.1% h100_mfu=49.33% vram=754/30,572MB peak=29,516/30,572MB
[step 50/8250] loss=2.7185 lr=1.62e-05 muon_lr=1.62e-04 grad_norm=1.3070 tok/s=759,604 raw_tok/s=760,806 step_tokens=130,865 waste=0.2% h100_mfu=50.43% vram=754/30,572MB peak=29,516/30,572MB
[step 51/8250] loss=2.7137 lr=1.66e-05 muon_lr=1.66e-04 grad_norm=1.5333 tok/s=789,292 raw_tok/s=790,354 step_tokens=130,896 waste=0.1% h100_mfu=52.39% vram=754/30,572MB peak=29,516/30,572MB
[step 52/8250] loss=2.7219 lr=1.69e-05 muon_lr=1.69e-04 grad_norm=1.4648 tok/s=732,216 raw_tok/s=733,268 step_tokens=130,884 waste=0.1% h100_mfu=48.60% vram=754/30,572MB peak=29,516/30,572MB
[step 53/8250] loss=2.7104 lr=1.72e-05 muon_lr=1.72e-04 grad_norm=1.3735 tok/s=759,219 raw_tok/s=761,694 step_tokens=130,646 waste=0.3% h100_mfu=50.49% vram=754/30,572MB peak=29,516/30,572MB
[step 54/8250] loss=2.7150 lr=1.75e-05 muon_lr=1.75e-04 grad_norm=1.1882 tok/s=760,561 raw_tok/s=760,927 step_tokens=131,009 waste=0.0% h100_mfu=50.44% vram=754/30,572MB peak=29,516/30,572MB
[step 55/8250] loss=2.7088 lr=1.79e-05 muon_lr=1.79e-04 grad_norm=0.9878 tok/s=760,028 raw_tok/s=761,137 step_tokens=130,881 waste=0.1% h100_mfu=50.45% vram=754/30,572MB peak=29,516/30,572MB
[step 56/8250] loss=2.7097 lr=1.82e-05 muon_lr=1.82e-04 grad_norm=0.9889 tok/s=759,856 raw_tok/s=761,157 step_tokens=130,848 waste=0.2% h100_mfu=50.45% vram=754/30,572MB peak=29,516/30,572MB
[step 57/8250] loss=2.7062 lr=1.85e-05 muon_lr=1.85e-04 grad_norm=0.8923 tok/s=758,573 raw_tok/s=761,046 step_tokens=130,646 waste=0.3% h100_mfu=50.45% vram=754/30,572MB peak=29,516/30,572MB
[step 58/8250] loss=2.7030 lr=1.89e-05 muon_lr=1.89e-04 grad_norm=0.9002 tok/s=756,414 raw_tok/s=757,576 step_tokens=130,871 waste=0.2% h100_mfu=50.22% vram=754/30,572MB peak=29,516/30,572MB
[step 59/8250] loss=2.7078 lr=1.92e-05 muon_lr=1.92e-04 grad_norm=0.8564 tok/s=787,582 raw_tok/s=789,975 step_tokens=130,675 waste=0.3% h100_mfu=52.36% vram=754/30,572MB peak=29,516/30,572MB
[step 60/8250] loss=2.6833 lr=1.95e-05 muon_lr=1.95e-04 grad_norm=0.8077 tok/s=786,920 raw_tok/s=787,557 step_tokens=130,966 waste=0.1% h100_mfu=52.20% vram=754/30,572MB peak=29,516/30,572MB
[step 61/8250] loss=2.7072 lr=1.99e-05 muon_lr=1.99e-04 grad_norm=1.0430 tok/s=755,484 raw_tok/s=758,209 step_tokens=130,601 waste=0.4% h100_mfu=50.26% vram=754/30,572MB peak=29,516/30,572MB
[step 62/8250] loss=2.7065 lr=2.02e-05 muon_lr=2.02e-04 grad_norm=0.6719 tok/s=748,556 raw_tok/s=750,003 step_tokens=130,819 waste=0.2% h100_mfu=49.71% vram=754/30,572MB peak=29,516/30,572MB
[step 63/8250] loss=2.7064 lr=2.05e-05 muon_lr=2.05e-04 grad_norm=0.7361 tok/s=761,097 raw_tok/s=762,488 step_tokens=130,833 waste=0.2% h100_mfu=50.54% vram=754/30,572MB peak=29,516/30,572MB
[step 64/8250] loss=2.7069 lr=2.09e-05 muon_lr=2.09e-04 grad_norm=0.7356 tok/s=788,501 raw_tok/s=790,177 step_tokens=130,794 waste=0.2% h100_mfu=52.38% vram=754/30,572MB peak=29,516/30,572MB
[step 65/8250] loss=2.6834 lr=2.12e-05 muon_lr=2.12e-04 grad_norm=0.6944 tok/s=750,281 raw_tok/s=754,286 step_tokens=130,376 waste=0.5% h100_mfu=50.00% vram=754/30,572MB peak=29,516/30,572MB
[step 66/8250] loss=2.6745 lr=2.15e-05 muon_lr=2.15e-04 grad_norm=0.7011 tok/s=752,507 raw_tok/s=755,788 step_tokens=130,503 waste=0.4% h100_mfu=50.10% vram=754/30,572MB peak=29,516/30,572MB
[step 67/8250] loss=2.6857 lr=2.19e-05 muon_lr=2.19e-04 grad_norm=0.6304 tok/s=786,152 raw_tok/s=787,751 step_tokens=130,806 waste=0.2% h100_mfu=52.22% vram=754/30,572MB peak=29,516/30,572MB
[step 68/8250] loss=2.6840 lr=2.22e-05 muon_lr=2.22e-04 grad_norm=0.7621 tok/s=760,864 raw_tok/s=760,934 step_tokens=131,060 waste=0.0% h100_mfu=50.44% vram=754/30,572MB peak=29,516/30,572MB
[step 69/8250] loss=2.6815 lr=2.25e-05 muon_lr=2.25e-04 grad_norm=0.8943 tok/s=778,337 raw_tok/s=781,061 step_tokens=130,615 waste=0.3% h100_mfu=51.77% vram=754/30,572MB peak=29,516/30,572MB
[step 70/8250] loss=2.6834 lr=2.28e-05 muon_lr=2.28e-04 grad_norm=0.8806 tok/s=749,949 raw_tok/s=752,159 step_tokens=130,687 waste=0.3% h100_mfu=49.86% vram=754/30,572MB peak=29,516/30,572MB
[step 71/8250] loss=2.6790 lr=2.32e-05 muon_lr=2.32e-04 grad_norm=0.8584 tok/s=756,657 raw_tok/s=760,586 step_tokens=130,395 waste=0.5% h100_mfu=50.42% vram=754/30,572MB peak=29,516/30,572MB
[step 72/8250] loss=2.6624 lr=2.35e-05 muon_lr=2.35e-04 grad_norm=0.6318 tok/s=759,368 raw_tok/s=762,451 step_tokens=130,542 waste=0.4% h100_mfu=50.54% vram=754/30,572MB peak=29,516/30,572MB
[step 73/8250] loss=2.6822 lr=2.38e-05 muon_lr=2.38e-04 grad_norm=0.7195 tok/s=780,289 raw_tok/s=783,427 step_tokens=130,547 waste=0.4% h100_mfu=51.93% vram=754/30,572MB peak=29,516/30,572MB
[step 74/8250] loss=2.6743 lr=2.42e-05 muon_lr=2.42e-04 grad_norm=0.8473 tok/s=759,284 raw_tok/s=759,910 step_tokens=130,964 waste=0.1% h100_mfu=50.37% vram=754/30,572MB peak=29,516/30,572MB
[step 75/8250] loss=2.6631 lr=2.45e-05 muon_lr=2.45e-04 grad_norm=0.8617 tok/s=753,740 raw_tok/s=757,473 step_tokens=130,426 waste=0.5% h100_mfu=50.21% vram=754/30,572MB peak=29,516/30,572MB
[step 76/8250] loss=2.6830 lr=2.48e-05 muon_lr=2.48e-04 grad_norm=0.7870 tok/s=784,379 raw_tok/s=787,220 step_tokens=130,599 waste=0.4% h100_mfu=52.18% vram=754/30,572MB peak=29,516/30,572MB
[step 77/8250] loss=2.6776 lr=2.52e-05 muon_lr=2.52e-04 grad_norm=0.6140 tok/s=772,354 raw_tok/s=773,416 step_tokens=130,892 waste=0.1% h100_mfu=51.27% vram=754/30,572MB peak=29,516/30,572MB
[step 78/8250] loss=2.6766 lr=2.55e-05 muon_lr=2.55e-04 grad_norm=0.6753 tok/s=735,543 raw_tok/s=737,292 step_tokens=130,761 waste=0.2% h100_mfu=48.87% vram=754/30,572MB peak=29,516/30,572MB
[step 79/8250] loss=2.6868 lr=2.58e-05 muon_lr=2.58e-04 grad_norm=0.7656 tok/s=758,439 raw_tok/s=759,941 step_tokens=130,813 waste=0.2% h100_mfu=50.37% vram=754/30,572MB peak=29,516/30,572MB
[step 80/8250] loss=2.6887 lr=2.62e-05 muon_lr=2.62e-04 grad_norm=0.8038 tok/s=784,691 raw_tok/s=787,298 step_tokens=130,638 waste=0.3% h100_mfu=52.19% vram=754/30,572MB peak=29,516/30,572MB
[step 81/8250] loss=2.6654 lr=2.65e-05 muon_lr=2.65e-04 grad_norm=0.8684 tok/s=756,130 raw_tok/s=757,349 step_tokens=130,861 waste=0.2% h100_mfu=50.20% vram=754/30,572MB peak=29,516/30,572MB
[step 82/8250] loss=2.6655 lr=2.68e-05 muon_lr=2.68e-04 grad_norm=0.7338 tok/s=775,453 raw_tok/s=776,270 step_tokens=130,934 waste=0.1% h100_mfu=51.46% vram=754/30,572MB peak=29,516/30,572MB
[step 83/8250] loss=2.6755 lr=2.72e-05 muon_lr=2.72e-04 grad_norm=1.1147 tok/s=754,462 raw_tok/s=755,234 step_tokens=130,938 waste=0.1% h100_mfu=50.06% vram=754/30,572MB peak=29,516/30,572MB
[step 84/8250] loss=2.6684 lr=2.75e-05 muon_lr=2.75e-04 grad_norm=1.1430 tok/s=781,181 raw_tok/s=782,387 step_tokens=130,870 waste=0.2% h100_mfu=51.86% vram=754/30,572MB peak=29,516/30,572MB
[step 85/8250] loss=2.6675 lr=2.78e-05 muon_lr=2.78e-04 grad_norm=1.1596 tok/s=789,571 raw_tok/s=790,712 step_tokens=130,883 waste=0.1% h100_mfu=52.41% vram=754/30,572MB peak=29,516/30,572MB
[step 86/8250] loss=2.6738 lr=2.81e-05 muon_lr=2.81e-04 grad_norm=1.1988 tok/s=754,093 raw_tok/s=755,725 step_tokens=130,789 waste=0.2% h100_mfu=50.09% vram=754/30,572MB peak=29,516/30,572MB
[step 87/8250] loss=2.6547 lr=2.85e-05 muon_lr=2.85e-04 grad_norm=0.9869 tok/s=743,808 raw_tok/s=745,400 step_tokens=130,792 waste=0.2% h100_mfu=49.41% vram=754/30,572MB peak=29,516/30,572MB
[step 88/8250] loss=2.6625 lr=2.88e-05 muon_lr=2.88e-04 grad_norm=0.8684 tok/s=746,849 raw_tok/s=747,733 step_tokens=130,917 waste=0.1% h100_mfu=49.56% vram=754/30,572MB peak=29,516/30,572MB
[step 89/8250] loss=2.6664 lr=2.91e-05 muon_lr=2.91e-04 grad_norm=0.9647 tok/s=777,547 raw_tok/s=780,991 step_tokens=130,494 waste=0.4% h100_mfu=51.77% vram=754/30,572MB peak=29,516/30,572MB
[step 90/8250] loss=2.6704 lr=2.95e-05 muon_lr=2.95e-04 grad_norm=1.0122 tok/s=785,335 raw_tok/s=787,185 step_tokens=130,764 waste=0.2% h100_mfu=52.18% vram=754/30,572MB peak=29,516/30,572MB
[step 91/8250] loss=2.6612 lr=2.98e-05 muon_lr=2.98e-04 grad_norm=0.9052 tok/s=763,189 raw_tok/s=766,053 step_tokens=130,582 waste=0.4% h100_mfu=50.78% vram=754/30,572MB peak=29,516/30,572MB
[step 92/8250] loss=2.6532 lr=3.01e-05 muon_lr=3.01e-04 grad_norm=1.0697 tok/s=788,254 raw_tok/s=789,779 step_tokens=130,819 waste=0.2% h100_mfu=52.35% vram=754/30,572MB peak=29,516/30,572MB
[step 93/8250] loss=2.6653 lr=3.05e-05 muon_lr=3.05e-04 grad_norm=0.9356 tok/s=756,912 raw_tok/s=758,655 step_tokens=130,771 waste=0.2% h100_mfu=50.29% vram=754/30,572MB peak=29,516/30,572MB
[step 94/8250] loss=2.6462 lr=3.08e-05 muon_lr=3.08e-04 grad_norm=1.0779 tok/s=778,345 raw_tok/s=780,650 step_tokens=130,685 waste=0.3% h100_mfu=51.75% vram=754/30,572MB peak=29,516/30,572MB
[step 95/8250] loss=2.6782 lr=3.11e-05 muon_lr=3.11e-04 grad_norm=1.1793 tok/s=747,649 raw_tok/s=749,892 step_tokens=130,680 waste=0.3% h100_mfu=49.71% vram=754/30,572MB peak=29,516/30,572MB
[step 96/8250] loss=2.6602 lr=3.15e-05 muon_lr=3.15e-04 grad_norm=0.9893 tok/s=785,862 raw_tok/s=787,713 step_tokens=130,764 waste=0.2% h100_mfu=52.21% vram=754/30,572MB peak=29,516/30,572MB
[step 97/8250] loss=2.6626 lr=3.18e-05 muon_lr=3.18e-04 grad_norm=1.1937 tok/s=754,651 raw_tok/s=755,343 step_tokens=130,952 waste=0.1% h100_mfu=50.07% vram=754/30,572MB peak=29,516/30,572MB
[step 98/8250] loss=2.6538 lr=3.21e-05 muon_lr=3.21e-04 grad_norm=1.0665 tok/s=760,655 raw_tok/s=765,032 step_tokens=130,322 waste=0.6% h100_mfu=50.71% vram=754/30,572MB peak=29,516/30,572MB
[step 99/8250] loss=2.6585 lr=3.25e-05 muon_lr=3.25e-04 grad_norm=1.1576 tok/s=749,759 raw_tok/s=750,629 step_tokens=130,920 waste=0.1% h100_mfu=49.76% vram=754/30,572MB peak=29,516/30,572MB
[step 100/8250] loss=2.6605 lr=3.28e-05 muon_lr=3.28e-04 grad_norm=1.1349 tok/s=773,173 raw_tok/s=774,272 step_tokens=130,886 waste=0.1% h100_mfu=51.32% vram=754/30,572MB peak=29,516/30,572MB
[step 101/8250] loss=2.6636 lr=3.31e-05 muon_lr=3.31e-04 grad_norm=1.0177 tok/s=785,487 raw_tok/s=788,664 step_tokens=130,544 waste=0.4% h100_mfu=52.28% vram=754/30,572MB peak=29,516/30,572MB
[step 102/8250] loss=2.6662 lr=3.34e-05 muon_lr=3.34e-04 grad_norm=1.0292 tok/s=787,499 raw_tok/s=788,329 step_tokens=130,934 waste=0.1% h100_mfu=52.25% vram=754/30,572MB peak=29,516/30,572MB
[step 103/8250] loss=2.6477 lr=3.38e-05 muon_lr=3.38e-04 grad_norm=0.9028 tok/s=744,126 raw_tok/s=744,956 step_tokens=130,926 waste=0.1% h100_mfu=49.38% vram=754/30,572MB peak=29,516/30,572MB
[step 104/8250] loss=2.6707 lr=3.41e-05 muon_lr=3.41e-04 grad_norm=0.9944 tok/s=734,950 raw_tok/s=736,101 step_tokens=130,867 waste=0.2% h100_mfu=48.79% vram=754/30,572MB peak=29,516/30,572MB
[step 105/8250] loss=2.6787 lr=3.44e-05 muon_lr=3.44e-04 grad_norm=0.9997 tok/s=757,708 raw_tok/s=758,240 step_tokens=130,980 waste=0.1% h100_mfu=50.26% vram=754/30,572MB peak=29,516/30,572MB
[step 106/8250] loss=2.6580 lr=3.48e-05 muon_lr=3.48e-04 grad_norm=0.9075 tok/s=755,885 raw_tok/s=758,001 step_tokens=130,706 waste=0.3% h100_mfu=50.24% vram=754/30,572MB peak=29,516/30,572MB
[step 107/8250] loss=2.6644 lr=3.51e-05 muon_lr=3.51e-04 grad_norm=0.8282 tok/s=755,394 raw_tok/s=757,486 step_tokens=130,710 waste=0.3% h100_mfu=50.21% vram=754/30,572MB peak=29,516/30,572MB
[step 108/8250] loss=2.6569 lr=3.54e-05 muon_lr=3.54e-04 grad_norm=0.6563 tok/s=756,354 raw_tok/s=758,257 step_tokens=130,743 waste=0.3% h100_mfu=50.26% vram=754/30,572MB peak=29,516/30,572MB
[step 109/8250] loss=2.6571 lr=3.58e-05 muon_lr=3.58e-04 grad_norm=0.6830 tok/s=754,905 raw_tok/s=756,324 step_tokens=130,826 waste=0.2% h100_mfu=50.13% vram=754/30,572MB peak=29,516/30,572MB
[step 110/8250] loss=2.6654 lr=3.61e-05 muon_lr=3.61e-04 grad_norm=0.7227 tok/s=760,897 raw_tok/s=763,045 step_tokens=130,703 waste=0.3% h100_mfu=50.58% vram=754/30,572MB peak=29,516/30,572MB
[step 111/8250] loss=2.6559 lr=3.64e-05 muon_lr=3.64e-04 grad_norm=0.8752 tok/s=761,568 raw_tok/s=762,289 step_tokens=130,948 waste=0.1% h100_mfu=50.53% vram=754/30,572MB peak=29,516/30,572MB
[step 112/8250] loss=2.6512 lr=3.68e-05 muon_lr=3.68e-04 grad_norm=1.0151 tok/s=732,364 raw_tok/s=734,072 step_tokens=130,767 waste=0.2% h100_mfu=48.66% vram=754/30,572MB peak=29,516/30,572MB
[step 113/8250] loss=2.6618 lr=3.71e-05 muon_lr=3.71e-04 grad_norm=1.0469 tok/s=733,253 raw_tok/s=735,999 step_tokens=130,583 waste=0.4% h100_mfu=48.79% vram=754/30,572MB peak=29,516/30,572MB
[step 114/8250] loss=2.6698 lr=3.74e-05 muon_lr=3.74e-04 grad_norm=1.2306 tok/s=753,908 raw_tok/s=754,772 step_tokens=130,922 waste=0.1% h100_mfu=50.03% vram=754/30,572MB peak=29,516/30,572MB
[step 115/8250] loss=2.6682 lr=3.77e-05 muon_lr=3.77e-04 grad_norm=0.9884 tok/s=784,274 raw_tok/s=786,868 step_tokens=130,640 waste=0.3% h100_mfu=52.16% vram=754/30,572MB peak=29,516/30,572MB
[step 116/8250] loss=2.6678 lr=3.81e-05 muon_lr=3.81e-04 grad_norm=0.7514 tok/s=785,374 raw_tok/s=788,859 step_tokens=130,493 waste=0.4% h100_mfu=52.29% vram=754/30,572MB peak=29,516/30,572MB
[step 117/8250] loss=2.6779 lr=3.84e-05 muon_lr=3.84e-04 grad_norm=1.0261 tok/s=744,502 raw_tok/s=746,399 step_tokens=130,739 waste=0.3% h100_mfu=49.48% vram=754/30,572MB peak=29,516/30,572MB
W0220 02:06:36.596000 20210 torch/distributed/run.py:852] 
W0220 02:06:36.596000 20210 torch/distributed/run.py:852] *****************************************
W0220 02:06:36.596000 20210 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:06:36.596000 20210 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 64280.52it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 40820.48it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 50533.78it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3052.07it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 675418144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020206
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 675418144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020206-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 4,228,251,648 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run u1o5ahwp
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_020645-u1o5ahwp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020206
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/u1o5ahwp
MFU estimation: 4,228,251,648 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8250] loss=3.9225 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=14.8020 tok/s=3,301 raw_tok/s=3,315 step_tokens=130,519 waste=0.4% h100_mfu=0.84% vram=2,690/63,104MB peak=60,145/63,104MB
[step 2/8250] loss=3.9246 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=32.8946 tok/s=326,867 raw_tok/s=327,572 step_tokens=130,790 waste=0.2% h100_mfu=82.90% vram=2,690/63,616MB peak=61,434/63,616MB
[step 3/8250] loss=3.9379 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=32.6260 tok/s=320,260 raw_tok/s=321,037 step_tokens=130,755 waste=0.2% h100_mfu=81.24% vram=2,690/64,128MB peak=61,434/64,128MB
[step 4/8250] loss=3.9150 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=32.7948 tok/s=318,800 raw_tok/s=319,187 step_tokens=130,913 waste=0.1% h100_mfu=80.78% vram=2,690/64,128MB peak=61,434/64,128MB
[step 5/8250] loss=3.9201 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=32.7021 tok/s=305,429 raw_tok/s=306,530 step_tokens=130,601 waste=0.4% h100_mfu=77.57% vram=2,690/64,128MB peak=61,434/64,128MB
[step 6/8250] loss=3.9014 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=32.6327 tok/s=293,864 raw_tok/s=294,678 step_tokens=130,710 waste=0.3% h100_mfu=74.57% vram=2,690/64,128MB peak=61,434/64,128MB
[step 7/8250] loss=3.8883 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=32.3055 tok/s=299,938 raw_tok/s=300,436 step_tokens=130,855 waste=0.2% h100_mfu=76.03% vram=2,690/64,128MB peak=61,434/64,128MB
[step 8/8250] loss=3.8337 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=31.1096 tok/s=296,699 raw_tok/s=297,459 step_tokens=130,737 waste=0.3% h100_mfu=75.28% vram=2,690/64,128MB peak=61,434/64,128MB
[step 9/8250] loss=3.7959 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=30.6556 tok/s=298,963 raw_tok/s=299,516 step_tokens=130,830 waste=0.2% h100_mfu=75.80% vram=2,690/64,128MB peak=61,434/64,128MB
[step 10/8250] loss=3.7464 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=29.4190 tok/s=295,286 raw_tok/s=295,796 step_tokens=130,846 waste=0.2% h100_mfu=74.86% vram=2,690/64,128MB peak=61,434/64,128MB
[step 11/8250] loss=3.6768 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=27.8885 tok/s=295,783 raw_tok/s=296,573 step_tokens=130,723 waste=0.3% h100_mfu=75.05% vram=2,690/64,128MB peak=61,434/64,128MB
[step 12/8250] loss=3.5727 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=26.5818 tok/s=297,406 raw_tok/s=297,772 step_tokens=130,911 waste=0.1% h100_mfu=75.36% vram=2,690/64,128MB peak=61,434/64,128MB
[step 13/8250] loss=3.4998 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=25.6534 tok/s=295,521 raw_tok/s=295,996 step_tokens=130,862 waste=0.2% h100_mfu=74.91% vram=2,690/64,128MB peak=61,434/64,128MB
[step 14/8250] loss=3.4018 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=24.0411 tok/s=294,636 raw_tok/s=295,927 step_tokens=130,500 waste=0.4% h100_mfu=74.89% vram=2,690/64,128MB peak=61,434/64,128MB
[step 15/8250] loss=3.3075 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=22.1562 tok/s=296,413 raw_tok/s=296,893 step_tokens=130,860 waste=0.2% h100_mfu=75.13% vram=2,690/64,128MB peak=61,434/64,128MB
[step 16/8250] loss=3.2245 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=20.4189 tok/s=299,929 raw_tok/s=300,089 step_tokens=131,002 waste=0.1% h100_mfu=75.94% vram=2,690/64,128MB peak=61,434/64,128MB
[step 17/8250] loss=3.1278 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=18.1172 tok/s=299,318 raw_tok/s=299,883 step_tokens=130,825 waste=0.2% h100_mfu=75.89% vram=2,690/64,128MB peak=61,434/64,128MB
[step 18/8250] loss=3.0689 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=16.6912 tok/s=296,090 raw_tok/s=296,960 step_tokens=130,688 waste=0.3% h100_mfu=75.15% vram=2,690/64,128MB peak=61,434/64,128MB
[step 19/8250] loss=2.9817 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=14.5491 tok/s=298,121 raw_tok/s=299,902 step_tokens=130,294 waste=0.6% h100_mfu=75.90% vram=2,690/64,128MB peak=61,434/64,128MB
[step 20/8250] loss=2.9635 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=12.7098 tok/s=299,223 raw_tok/s=299,958 step_tokens=130,751 waste=0.2% h100_mfu=75.91% vram=2,690/64,128MB peak=61,434/64,128MB
[step 21/8250] loss=2.9270 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=10.6869 tok/s=299,595 raw_tok/s=300,283 step_tokens=130,772 waste=0.2% h100_mfu=75.99% vram=2,690/64,128MB peak=61,434/64,128MB
[step 22/8250] loss=2.8993 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=9.3671 tok/s=298,552 raw_tok/s=299,015 step_tokens=130,869 waste=0.2% h100_mfu=75.67% vram=2,690/64,128MB peak=61,434/64,128MB
[step 23/8250] loss=2.8823 lr=7.28e-06 muon_lr=7.28e-05 grad_norm=9.5284 tok/s=299,897 raw_tok/s=301,264 step_tokens=130,477 waste=0.5% h100_mfu=76.24% vram=2,690/64,128MB peak=61,434/64,128MB
[step 24/8250] loss=2.9074 lr=7.62e-06 muon_lr=7.62e-05 grad_norm=9.9717 tok/s=298,068 raw_tok/s=298,720 step_tokens=130,786 waste=0.2% h100_mfu=75.60% vram=2,690/64,128MB peak=61,434/64,128MB
[step 25/8250] loss=2.9139 lr=7.95e-06 muon_lr=7.95e-05 grad_norm=11.6347 tok/s=299,339 raw_tok/s=300,427 step_tokens=130,597 waste=0.4% h100_mfu=76.03% vram=2,690/64,128MB peak=61,434/64,128MB
[step 26/8250] loss=2.9413 lr=8.28e-06 muon_lr=8.28e-05 grad_norm=12.7237 tok/s=294,393 raw_tok/s=295,263 step_tokens=130,686 waste=0.3% h100_mfu=74.72% vram=2,690/64,128MB peak=61,434/64,128MB
[step 27/8250] loss=2.9475 lr=8.61e-06 muon_lr=8.61e-05 grad_norm=13.8359 tok/s=294,474 raw_tok/s=296,143 step_tokens=130,333 waste=0.6% h100_mfu=74.94% vram=2,690/64,128MB peak=61,434/64,128MB
[step 28/8250] loss=2.9587 lr=8.94e-06 muon_lr=8.94e-05 grad_norm=14.9332 tok/s=295,235 raw_tok/s=296,227 step_tokens=130,633 waste=0.3% h100_mfu=74.97% vram=2,690/64,128MB peak=61,434/64,128MB
[step 29/8250] loss=2.9430 lr=9.27e-06 muon_lr=9.27e-05 grad_norm=15.5535 tok/s=298,991 raw_tok/s=299,580 step_tokens=130,814 waste=0.2% h100_mfu=75.81% vram=2,690/64,128MB peak=61,434/64,128MB
[step 30/8250] loss=2.9223 lr=9.60e-06 muon_lr=9.60e-05 grad_norm=14.2063 tok/s=294,314 raw_tok/s=295,312 step_tokens=130,629 waste=0.3% h100_mfu=74.73% vram=2,690/64,128MB peak=61,434/64,128MB
[step 31/8250] loss=2.8985 lr=9.93e-06 muon_lr=9.93e-05 grad_norm=12.0410 tok/s=300,402 raw_tok/s=301,220 step_tokens=130,716 waste=0.3% h100_mfu=76.23% vram=2,690/64,128MB peak=61,434/64,128MB
[step 32/8250] loss=2.8852 lr=1.03e-05 muon_lr=1.03e-04 grad_norm=9.4694 tok/s=297,511 raw_tok/s=298,264 step_tokens=130,741 waste=0.3% h100_mfu=75.48% vram=2,690/64,128MB peak=61,434/64,128MB
[step 33/8250] loss=2.8267 lr=1.06e-05 muon_lr=1.06e-04 grad_norm=7.4939 tok/s=295,359 raw_tok/s=296,059 step_tokens=130,762 waste=0.2% h100_mfu=74.92% vram=2,690/64,128MB peak=61,434/64,128MB
[step 34/8250] loss=2.7990 lr=1.09e-05 muon_lr=1.09e-04 grad_norm=5.6779 tok/s=298,977 raw_tok/s=299,272 step_tokens=130,943 waste=0.1% h100_mfu=75.74% vram=2,690/64,128MB peak=61,434/64,128MB
[step 35/8250] loss=2.7896 lr=1.13e-05 muon_lr=1.13e-04 grad_norm=5.2137 tok/s=294,107 raw_tok/s=294,849 step_tokens=130,742 waste=0.3% h100_mfu=74.62% vram=2,690/64,128MB peak=61,434/64,128MB
[step 36/8250] loss=2.7693 lr=1.16e-05 muon_lr=1.16e-04 grad_norm=5.1479 tok/s=295,680 raw_tok/s=296,546 step_tokens=130,689 waste=0.3% h100_mfu=75.05% vram=2,690/64,128MB peak=61,434/64,128MB
[step 37/8250] loss=2.7761 lr=1.19e-05 muon_lr=1.19e-04 grad_norm=5.0774 tok/s=295,842 raw_tok/s=295,966 step_tokens=131,017 waste=0.0% h100_mfu=74.90% vram=2,690/64,128MB peak=61,434/64,128MB
[step 38/8250] loss=2.7420 lr=1.23e-05 muon_lr=1.23e-04 grad_norm=5.1143 tok/s=298,837 raw_tok/s=300,467 step_tokens=130,361 waste=0.5% h100_mfu=76.04% vram=2,690/64,128MB peak=61,434/64,128MB
[step 39/8250] loss=2.7474 lr=1.26e-05 muon_lr=1.26e-04 grad_norm=5.0328 tok/s=292,815 raw_tok/s=293,921 step_tokens=130,579 waste=0.4% h100_mfu=74.38% vram=2,690/64,128MB peak=61,434/64,128MB
[step 40/8250] loss=2.7460 lr=1.29e-05 muon_lr=1.29e-04 grad_norm=5.7313 tok/s=296,503 raw_tok/s=296,628 step_tokens=131,017 waste=0.0% h100_mfu=75.07% vram=2,690/64,128MB peak=61,434/64,128MB
[step 41/8250] loss=2.7423 lr=1.32e-05 muon_lr=1.32e-04 grad_norm=5.8808 tok/s=295,717 raw_tok/s=296,115 step_tokens=130,896 waste=0.1% h100_mfu=74.94% vram=2,690/64,128MB peak=61,434/64,128MB
[step 42/8250] loss=2.7280 lr=1.36e-05 muon_lr=1.36e-04 grad_norm=5.8284 tok/s=301,164 raw_tok/s=301,488 step_tokens=130,931 waste=0.1% h100_mfu=76.30% vram=2,690/64,128MB peak=61,434/64,128MB
[step 43/8250] loss=2.7270 lr=1.39e-05 muon_lr=1.39e-04 grad_norm=4.5870 tok/s=299,669 raw_tok/s=300,102 step_tokens=130,883 waste=0.1% h100_mfu=75.95% vram=2,690/64,128MB peak=61,434/64,128MB
[step 44/8250] loss=2.7094 lr=1.42e-05 muon_lr=1.42e-04 grad_norm=3.0549 tok/s=300,170 raw_tok/s=300,716 step_tokens=130,834 waste=0.2% h100_mfu=76.10% vram=2,690/64,128MB peak=61,434/64,128MB
[step 45/8250] loss=2.6831 lr=1.46e-05 muon_lr=1.46e-04 grad_norm=1.9744 tok/s=297,124 raw_tok/s=298,442 step_tokens=130,493 waste=0.4% h100_mfu=75.53% vram=2,690/64,128MB peak=61,434/64,128MB
[step 46/8250] loss=2.6949 lr=1.49e-05 muon_lr=1.49e-04 grad_norm=2.0084 tok/s=293,108 raw_tok/s=294,384 step_tokens=130,504 waste=0.4% h100_mfu=74.50% vram=2,690/64,128MB peak=61,434/64,128MB
[step 47/8250] loss=2.7026 lr=1.52e-05 muon_lr=1.52e-04 grad_norm=1.9153 tok/s=294,473 raw_tok/s=295,189 step_tokens=130,754 waste=0.2% h100_mfu=74.70% vram=2,690/64,128MB peak=61,434/64,128MB
[step 48/8250] loss=2.6955 lr=1.56e-05 muon_lr=1.56e-04 grad_norm=2.3297 tok/s=300,118 raw_tok/s=300,891 step_tokens=130,735 waste=0.3% h100_mfu=76.15% vram=2,690/64,128MB peak=61,434/64,128MB
[step 49/8250] loss=2.6933 lr=1.59e-05 muon_lr=1.59e-04 grad_norm=2.5081 tok/s=295,420 raw_tok/s=295,612 step_tokens=130,987 waste=0.1% h100_mfu=74.81% vram=2,690/64,128MB peak=61,434/64,128MB
[step 50/8250] loss=2.6955 lr=1.62e-05 muon_lr=1.62e-04 grad_norm=2.3143 tok/s=291,241 raw_tok/s=291,702 step_tokens=130,865 waste=0.2% h100_mfu=73.82% vram=2,690/64,128MB peak=61,434/64,128MB
[step 51/8250] loss=2.6820 lr=1.66e-05 muon_lr=1.66e-04 grad_norm=1.8739 tok/s=300,533 raw_tok/s=300,937 step_tokens=130,896 waste=0.1% h100_mfu=76.16% vram=2,690/64,128MB peak=61,434/64,128MB
[step 52/8250] loss=2.6910 lr=1.69e-05 muon_lr=1.69e-04 grad_norm=1.5478 tok/s=293,951 raw_tok/s=294,373 step_tokens=130,884 waste=0.1% h100_mfu=74.50% vram=2,690/64,128MB peak=61,434/64,128MB
[step 53/8250] loss=2.6775 lr=1.72e-05 muon_lr=1.72e-04 grad_norm=1.1505 tok/s=299,691 raw_tok/s=300,668 step_tokens=130,646 waste=0.3% h100_mfu=76.09% vram=2,690/64,128MB peak=61,434/64,128MB
[step 54/8250] loss=2.6824 lr=1.75e-05 muon_lr=1.75e-04 grad_norm=0.8444 tok/s=296,909 raw_tok/s=297,051 step_tokens=131,009 waste=0.0% h100_mfu=75.17% vram=2,690/64,128MB peak=61,434/64,128MB
[step 55/8250] loss=2.6819 lr=1.79e-05 muon_lr=1.79e-04 grad_norm=1.1978 tok/s=294,749 raw_tok/s=295,179 step_tokens=130,881 waste=0.1% h100_mfu=74.70% vram=2,690/64,128MB peak=61,434/64,128MB
[step 56/8250] loss=2.6817 lr=1.82e-05 muon_lr=1.82e-04 grad_norm=1.4454 tok/s=298,233 raw_tok/s=298,744 step_tokens=130,848 waste=0.2% h100_mfu=75.60% vram=2,690/64,128MB peak=61,434/64,128MB
[step 57/8250] loss=2.6814 lr=1.85e-05 muon_lr=1.85e-04 grad_norm=1.3564 tok/s=294,777 raw_tok/s=295,739 step_tokens=130,646 waste=0.3% h100_mfu=74.84% vram=2,690/64,128MB peak=61,434/64,128MB
[step 58/8250] loss=2.6759 lr=1.89e-05 muon_lr=1.89e-04 grad_norm=1.3518 tok/s=295,744 raw_tok/s=296,198 step_tokens=130,871 waste=0.2% h100_mfu=74.96% vram=2,690/64,128MB peak=61,434/64,128MB
[step 59/8250] loss=2.6827 lr=1.92e-05 muon_lr=1.92e-04 grad_norm=1.2997 tok/s=297,557 raw_tok/s=298,461 step_tokens=130,675 waste=0.3% h100_mfu=75.53% vram=2,690/64,128MB peak=61,434/64,128MB
[step 60/8250] loss=2.6573 lr=1.95e-05 muon_lr=1.95e-04 grad_norm=1.2211 tok/s=301,324 raw_tok/s=301,568 step_tokens=130,966 waste=0.1% h100_mfu=76.32% vram=2,690/64,128MB peak=61,434/64,128MB
[step 61/8250] loss=2.6900 lr=1.99e-05 muon_lr=1.99e-04 grad_norm=1.3682 tok/s=292,990 raw_tok/s=294,046 step_tokens=130,601 waste=0.4% h100_mfu=74.41% vram=2,690/64,128MB peak=61,434/64,128MB
[step 62/8250] loss=2.6812 lr=2.02e-05 muon_lr=2.02e-04 grad_norm=1.4380 tok/s=298,698 raw_tok/s=299,275 step_tokens=130,819 waste=0.2% h100_mfu=75.74% vram=2,690/64,128MB peak=61,434/64,128MB
[step 63/8250] loss=2.6870 lr=2.05e-05 muon_lr=2.05e-04 grad_norm=1.4875 tok/s=296,491 raw_tok/s=297,033 step_tokens=130,833 waste=0.2% h100_mfu=75.17% vram=2,690/64,128MB peak=61,434/64,128MB
[step 64/8250] loss=2.6867 lr=2.09e-05 muon_lr=2.09e-04 grad_norm=1.7634 tok/s=301,108 raw_tok/s=301,748 step_tokens=130,794 waste=0.2% h100_mfu=76.36% vram=2,690/64,128MB peak=61,434/64,128MB
[step 65/8250] loss=2.6583 lr=2.12e-05 muon_lr=2.12e-04 grad_norm=1.7045 tok/s=292,582 raw_tok/s=294,144 step_tokens=130,376 waste=0.5% h100_mfu=74.44% vram=2,690/64,128MB peak=61,434/64,128MB
[step 66/8250] loss=2.6585 lr=2.15e-05 muon_lr=2.15e-04 grad_norm=1.8236 tok/s=299,432 raw_tok/s=300,737 step_tokens=130,503 waste=0.4% h100_mfu=76.11% vram=2,690/64,128MB peak=61,434/64,128MB
[step 67/8250] loss=2.6699 lr=2.19e-05 muon_lr=2.19e-04 grad_norm=1.6450 tok/s=298,857 raw_tok/s=299,465 step_tokens=130,806 waste=0.2% h100_mfu=75.78% vram=2,690/64,128MB peak=61,434/64,128MB
[step 68/8250] loss=2.6663 lr=2.22e-05 muon_lr=2.22e-04 grad_norm=1.5180 tok/s=298,739 raw_tok/s=298,766 step_tokens=131,060 waste=0.0% h100_mfu=75.61% vram=2,690/64,128MB peak=61,434/64,128MB
[step 69/8250] loss=2.6604 lr=2.25e-05 muon_lr=2.25e-04 grad_norm=1.6991 tok/s=299,742 raw_tok/s=300,791 step_tokens=130,615 waste=0.3% h100_mfu=76.12% vram=2,690/64,128MB peak=61,434/64,128MB
[step 70/8250] loss=2.6746 lr=2.28e-05 muon_lr=2.28e-04 grad_norm=1.7644 tok/s=296,406 raw_tok/s=297,279 step_tokens=130,687 waste=0.3% h100_mfu=75.23% vram=2,690/64,128MB peak=61,434/64,128MB
[step 71/8250] loss=2.6601 lr=2.32e-05 muon_lr=2.32e-04 grad_norm=1.7765 tok/s=297,987 raw_tok/s=299,534 step_tokens=130,395 waste=0.5% h100_mfu=75.80% vram=2,690/64,128MB peak=61,434/64,128MB
[step 72/8250] loss=2.6556 lr=2.35e-05 muon_lr=2.35e-04 grad_norm=1.5572 tok/s=295,448 raw_tok/s=296,647 step_tokens=130,542 waste=0.4% h100_mfu=75.07% vram=2,690/64,128MB peak=61,434/64,128MB
[step 73/8250] loss=2.6750 lr=2.38e-05 muon_lr=2.38e-04 grad_norm=1.6216 tok/s=300,277 raw_tok/s=301,484 step_tokens=130,547 waste=0.4% h100_mfu=76.30% vram=2,690/64,128MB peak=61,434/64,128MB
[step 74/8250] loss=2.6639 lr=2.42e-05 muon_lr=2.42e-04 grad_norm=1.7369 tok/s=299,057 raw_tok/s=299,303 step_tokens=130,964 waste=0.1% h100_mfu=75.74% vram=2,690/64,128MB peak=61,434/64,128MB
[step 75/8250] loss=2.6513 lr=2.45e-05 muon_lr=2.45e-04 grad_norm=1.5673 tok/s=298,562 raw_tok/s=300,040 step_tokens=130,426 waste=0.5% h100_mfu=75.93% vram=2,690/64,128MB peak=61,434/64,128MB
[step 76/8250] loss=2.6725 lr=2.48e-05 muon_lr=2.48e-04 grad_norm=1.7935 tok/s=299,606 raw_tok/s=300,691 step_tokens=130,599 waste=0.4% h100_mfu=76.10% vram=2,690/64,128MB peak=61,434/64,128MB
[step 77/8250] loss=2.6634 lr=2.52e-05 muon_lr=2.52e-04 grad_norm=1.5640 tok/s=297,694 raw_tok/s=298,103 step_tokens=130,892 waste=0.1% h100_mfu=75.44% vram=2,690/64,128MB peak=61,434/64,128MB
[step 78/8250] loss=2.6637 lr=2.55e-05 muon_lr=2.55e-04 grad_norm=1.5141 tok/s=295,690 raw_tok/s=296,393 step_tokens=130,761 waste=0.2% h100_mfu=75.01% vram=2,690/64,128MB peak=61,434/64,128MB
[step 79/8250] loss=2.6760 lr=2.58e-05 muon_lr=2.58e-04 grad_norm=1.6790 tok/s=296,684 raw_tok/s=297,271 step_tokens=130,813 waste=0.2% h100_mfu=75.23% vram=2,690/64,128MB peak=61,434/64,128MB
[step 80/8250] loss=2.6765 lr=2.62e-05 muon_lr=2.62e-04 grad_norm=1.7028 tok/s=300,972 raw_tok/s=301,972 step_tokens=130,638 waste=0.3% h100_mfu=76.42% vram=2,690/64,128MB peak=61,434/64,128MB
[step 81/8250] loss=2.6570 lr=2.65e-05 muon_lr=2.65e-04 grad_norm=1.7448 tok/s=292,980 raw_tok/s=293,452 step_tokens=130,861 waste=0.2% h100_mfu=74.26% vram=2,690/64,128MB peak=61,434/64,128MB
[step 82/8250] loss=2.6564 lr=2.68e-05 muon_lr=2.68e-04 grad_norm=1.5451 tok/s=298,157 raw_tok/s=298,471 step_tokens=130,934 waste=0.1% h100_mfu=75.53% vram=2,690/64,128MB peak=61,434/64,128MB
[step 83/8250] loss=2.6635 lr=2.72e-05 muon_lr=2.72e-04 grad_norm=1.6586 tok/s=294,352 raw_tok/s=294,653 step_tokens=130,938 waste=0.1% h100_mfu=74.57% vram=2,690/64,128MB peak=61,434/64,128MB
[step 84/8250] loss=2.6571 lr=2.75e-05 muon_lr=2.75e-04 grad_norm=1.9651 tok/s=297,166 raw_tok/s=297,625 step_tokens=130,870 waste=0.2% h100_mfu=75.32% vram=2,690/64,128MB peak=61,434/64,128MB
[step 85/8250] loss=2.6660 lr=2.78e-05 muon_lr=2.78e-04 grad_norm=2.0504 tok/s=299,105 raw_tok/s=299,537 step_tokens=130,883 waste=0.1% h100_mfu=75.80% vram=2,690/64,128MB peak=61,434/64,128MB
[step 86/8250] loss=2.6639 lr=2.81e-05 muon_lr=2.81e-04 grad_norm=2.0217 tok/s=291,586 raw_tok/s=292,217 step_tokens=130,789 waste=0.2% h100_mfu=73.95% vram=2,690/64,128MB peak=61,434/64,128MB
[step 87/8250] loss=2.6512 lr=2.85e-05 muon_lr=2.85e-04 grad_norm=2.0172 tok/s=300,199 raw_tok/s=300,842 step_tokens=130,792 waste=0.2% h100_mfu=76.13% vram=2,690/64,128MB peak=61,434/64,128MB
[step 88/8250] loss=2.6521 lr=2.88e-05 muon_lr=2.88e-04 grad_norm=1.9439 tok/s=297,361 raw_tok/s=297,713 step_tokens=130,917 waste=0.1% h100_mfu=75.34% vram=2,690/64,128MB peak=61,434/64,128MB
[step 89/8250] loss=2.6596 lr=2.91e-05 muon_lr=2.91e-04 grad_norm=1.8653 tok/s=299,168 raw_tok/s=300,493 step_tokens=130,494 waste=0.4% h100_mfu=76.05% vram=2,690/64,128MB peak=61,434/64,128MB
[step 90/8250] loss=2.6676 lr=2.95e-05 muon_lr=2.95e-04 grad_norm=1.8063 tok/s=296,761 raw_tok/s=297,460 step_tokens=130,764 waste=0.2% h100_mfu=75.28% vram=2,690/64,128MB peak=61,434/64,128MB
[step 91/8250] loss=2.6525 lr=2.98e-05 muon_lr=2.98e-04 grad_norm=1.8054 tok/s=301,189 raw_tok/s=302,319 step_tokens=130,582 waste=0.4% h100_mfu=76.51% vram=2,690/64,128MB peak=61,434/64,128MB
[step 92/8250] loss=2.6471 lr=3.01e-05 muon_lr=3.01e-04 grad_norm=1.7037 tok/s=299,123 raw_tok/s=299,701 step_tokens=130,819 waste=0.2% h100_mfu=75.84% vram=2,690/64,128MB peak=61,434/64,128MB
[step 93/8250] loss=2.6574 lr=3.05e-05 muon_lr=3.05e-04 grad_norm=1.8274 tok/s=296,958 raw_tok/s=297,642 step_tokens=130,771 waste=0.2% h100_mfu=75.32% vram=2,690/64,128MB peak=61,434/64,128MB
[step 94/8250] loss=2.6447 lr=3.08e-05 muon_lr=3.08e-04 grad_norm=1.8915 tok/s=301,340 raw_tok/s=302,232 step_tokens=130,685 waste=0.3% h100_mfu=76.49% vram=2,690/64,128MB peak=61,434/64,128MB
[step 95/8250] loss=2.6685 lr=3.11e-05 muon_lr=3.11e-04 grad_norm=1.8025 tok/s=296,993 raw_tok/s=297,884 step_tokens=130,680 waste=0.3% h100_mfu=75.38% vram=2,690/64,128MB peak=61,434/64,128MB
[step 96/8250] loss=2.6583 lr=3.15e-05 muon_lr=3.15e-04 grad_norm=1.8442 tok/s=299,815 raw_tok/s=300,521 step_tokens=130,764 waste=0.2% h100_mfu=76.05% vram=2,690/64,128MB peak=61,434/64,128MB
[step 97/8250] loss=2.6549 lr=3.18e-05 muon_lr=3.18e-04 grad_norm=1.8028 tok/s=295,332 raw_tok/s=295,603 step_tokens=130,952 waste=0.1% h100_mfu=74.81% vram=2,690/64,128MB peak=61,434/64,128MB
[step 98/8250] loss=2.6494 lr=3.21e-05 muon_lr=3.21e-04 grad_norm=1.7344 tok/s=297,441 raw_tok/s=299,153 step_tokens=130,322 waste=0.6% h100_mfu=75.71% vram=2,690/64,128MB peak=61,434/64,128MB
[step 99/8250] loss=2.6555 lr=3.25e-05 muon_lr=3.25e-04 grad_norm=1.7297 tok/s=296,878 raw_tok/s=297,222 step_tokens=130,920 waste=0.1% h100_mfu=75.22% vram=2,690/64,128MB peak=61,434/64,128MB
[step 100/8250] loss=2.6564 lr=3.28e-05 muon_lr=3.28e-04 grad_norm=1.4298 tok/s=297,330 raw_tok/s=297,753 step_tokens=130,886 waste=0.1% h100_mfu=75.35% vram=2,690/64,128MB peak=61,434/64,128MB
[step 101/8250] loss=2.6558 lr=3.31e-05 muon_lr=3.31e-04 grad_norm=1.4712 tok/s=298,015 raw_tok/s=299,220 step_tokens=130,544 waste=0.4% h100_mfu=75.72% vram=2,690/64,128MB peak=61,434/64,128MB
[step 102/8250] loss=2.6607 lr=3.34e-05 muon_lr=3.34e-04 grad_norm=1.3110 tok/s=299,737 raw_tok/s=300,053 step_tokens=130,934 waste=0.1% h100_mfu=75.93% vram=2,690/64,128MB peak=61,434/64,128MB
[step 103/8250] loss=2.6426 lr=3.38e-05 muon_lr=3.38e-04 grad_norm=1.4852 tok/s=296,708 raw_tok/s=297,039 step_tokens=130,926 waste=0.1% h100_mfu=75.17% vram=2,690/64,128MB peak=61,434/64,128MB
[step 104/8250] loss=2.6660 lr=3.41e-05 muon_lr=3.41e-04 grad_norm=1.7235 tok/s=295,569 raw_tok/s=296,032 step_tokens=130,867 waste=0.2% h100_mfu=74.92% vram=2,690/64,128MB peak=61,434/64,128MB
[step 105/8250] loss=2.6731 lr=3.44e-05 muon_lr=3.44e-04 grad_norm=1.4270 tok/s=299,248 raw_tok/s=299,458 step_tokens=130,980 waste=0.1% h100_mfu=75.78% vram=2,690/64,128MB peak=61,434/64,128MB
[step 106/8250] loss=2.6546 lr=3.48e-05 muon_lr=3.48e-04 grad_norm=1.7474 tok/s=295,864 raw_tok/s=296,693 step_tokens=130,706 waste=0.3% h100_mfu=75.08% vram=2,690/64,128MB peak=61,434/64,128MB
[step 107/8250] loss=2.6588 lr=3.51e-05 muon_lr=3.51e-04 grad_norm=1.5447 tok/s=293,427 raw_tok/s=294,240 step_tokens=130,710 waste=0.3% h100_mfu=74.46% vram=2,690/64,128MB peak=61,434/64,128MB
[step 108/8250] loss=2.6522 lr=3.54e-05 muon_lr=3.54e-04 grad_norm=1.4150 tok/s=299,309 raw_tok/s=300,063 step_tokens=130,743 waste=0.3% h100_mfu=75.94% vram=2,690/64,128MB peak=61,434/64,128MB
[step 109/8250] loss=2.6509 lr=3.58e-05 muon_lr=3.58e-04 grad_norm=1.5829 tok/s=296,684 raw_tok/s=297,242 step_tokens=130,826 waste=0.2% h100_mfu=75.22% vram=2,690/64,128MB peak=61,434/64,128MB
[step 110/8250] loss=2.6612 lr=3.61e-05 muon_lr=3.61e-04 grad_norm=1.3919 tok/s=298,791 raw_tok/s=299,635 step_tokens=130,703 waste=0.3% h100_mfu=75.83% vram=2,690/64,128MB peak=61,434/64,128MB
[step 111/8250] loss=2.6507 lr=3.64e-05 muon_lr=3.64e-04 grad_norm=1.2040 tok/s=297,977 raw_tok/s=298,259 step_tokens=130,948 waste=0.1% h100_mfu=75.48% vram=2,690/64,128MB peak=61,434/64,128MB
[step 112/8250] loss=2.6394 lr=3.68e-05 muon_lr=3.68e-04 grad_norm=1.3542 tok/s=298,923 raw_tok/s=299,620 step_tokens=130,767 waste=0.2% h100_mfu=75.82% vram=2,690/64,128MB peak=61,434/64,128MB
[step 113/8250] loss=2.6490 lr=3.71e-05 muon_lr=3.71e-04 grad_norm=1.2728 tok/s=293,023 raw_tok/s=294,120 step_tokens=130,583 waste=0.4% h100_mfu=74.43% vram=2,690/64,128MB peak=61,434/64,128MB
[step 114/8250] loss=2.6572 lr=3.74e-05 muon_lr=3.74e-04 grad_norm=1.2280 tok/s=297,945 raw_tok/s=298,287 step_tokens=130,922 waste=0.1% h100_mfu=75.49% vram=2,690/64,128MB peak=61,434/64,128MB
[step 115/8250] loss=2.6570 lr=3.77e-05 muon_lr=3.77e-04 grad_norm=1.2774 tok/s=298,181 raw_tok/s=299,167 step_tokens=130,640 waste=0.3% h100_mfu=75.71% vram=2,690/64,128MB peak=61,434/64,128MB
[step 116/8250] loss=2.6622 lr=3.81e-05 muon_lr=3.81e-04 grad_norm=1.2608 tok/s=299,699 raw_tok/s=301,028 step_tokens=130,493 waste=0.4% h100_mfu=76.18% vram=2,690/64,128MB peak=61,434/64,128MB
[step 117/8250] loss=2.6703 lr=3.84e-05 muon_lr=3.84e-04 grad_norm=1.3399 tok/s=295,896 raw_tok/s=296,649 step_tokens=130,739 waste=0.3% h100_mfu=75.07% vram=2,690/64,128MB peak=61,434/64,128MB
W0220 02:08:28.386000 21335 torch/distributed/run.py:852] 
W0220 02:08:28.386000 21335 torch/distributed/run.py:852] *****************************************
W0220 02:08:28.386000 21335 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:08:28.386000 21335 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11008.67it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 2573.98it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 54471.48it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 53773.13it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020208
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020208-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run b0lprttb
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_020837-b0lprttb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020208-2
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/b0lprttb
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
W0220 02:09:54.692000 21806 torch/distributed/run.py:852] 
W0220 02:09:54.692000 21806 torch/distributed/run.py:852] *****************************************
W0220 02:09:54.692000 21806 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:09:54.692000 21806 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12069.94it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 62601.55it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 50840.05it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 51622.20it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=65,536, grad_accum_steps=2, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020210
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=65,536, grad_accum_steps=2, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020210-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=32,768 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 1561], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=32,768 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 1561], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 1651 (from 1900) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 1651 (from 1900) to prevent distributed deadlock with variable packing
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=2, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 7ij88mph
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_021004-7ij88mph
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020210-2
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/7ij88mph
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=2, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
Traceback (most recent call last):
  File "/workspace/.venv/bin/nanoplm", line 10, in <module>
    sys.exit(cli())
             ^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1485, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1406, in main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1873, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1873, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1269, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 824, in invoke
    return callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/nanoplm/src/nanoplm/cli/pretrain.py", line 649, in from_yaml
    run_te_pretraining(
  File "/workspace/nanoplm/src/nanoplm/pretraining/te_pipeline.py", line 676, in run_te_pretraining
    out = model(**fwd_kwargs)
          ^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1882, in _call_impl
    return inner()
           ^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1830, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/nanoplm/src/nanoplm/pretraining/models/modern_bert/modelling_te.py", line 333, in forward
    x = self.model(flat_ids, cu_seqlens=cu_seqlens, max_seqlen=int(max_seqlen), is_first_microbatch=is_first_microbatch)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/nanoplm/src/nanoplm/pretraining/models/modern_bert/modelling_te.py", line 225, in forward
    x = layer(x, rotary_pos_emb=rope, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen, is_first_microbatch=is_first_microbatch)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1882, in _call_impl
    return inner()
           ^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1830, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/nanoplm/src/nanoplm/pretraining/models/modern_bert/modelling_te.py", line 148, in forward
    mlp_out = self.mlp(x, is_first_microbatch=is_first_microbatch)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 1181, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/module/layernorm_mlp.py", line 2153, in forward
    out = fwd_fn(
          ^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/autograd/function.py", line 583, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/module/layernorm_mlp.py", line 885, in forward
    return _LayerNormMLP._forward(
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/module/layernorm_mlp.py", line 546, in _forward
    fc1_outputs = general_gemm(
                  ^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/cpp_extensions/gemm.py", line 205, in general_gemm
    out, bias_grad, gelu_input, extra_output = tex.generic_gemm(*args, **kwargs)
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.21 GiB of which 507.19 MiB is free. Process 3831621 has 10.60 GiB memory in use. Process 3831620 has 68.10 GiB memory in use. Of the allocated memory 66.48 GiB is allocated by PyTorch, and 211.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/.venv/bin/nanoplm", line 10, in <module>
[rank0]:     sys.exit(cli())
[rank0]:              ^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1485, in __call__
[rank0]:     return self.main(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1406, in main
[rank0]:     rv = self.invoke(ctx)
[rank0]:          ^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1873, in invoke
[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1873, in invoke
[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))
[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 1269, in invoke
[rank0]:     return ctx.invoke(self.callback, **ctx.params)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/click/core.py", line 824, in invoke
[rank0]:     return callback(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/nanoplm/src/nanoplm/cli/pretrain.py", line 649, in from_yaml
[rank0]:     run_te_pretraining(
[rank0]:   File "/workspace/nanoplm/src/nanoplm/pretraining/te_pipeline.py", line 676, in run_te_pretraining
[rank0]:     out = model(**fwd_kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1882, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1830, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/nanoplm/src/nanoplm/pretraining/models/modern_bert/modelling_te.py", line 333, in forward
[rank0]:     x = self.model(flat_ids, cu_seqlens=cu_seqlens, max_seqlen=int(max_seqlen), is_first_microbatch=is_first_microbatch)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/nanoplm/src/nanoplm/pretraining/models/modern_bert/modelling_te.py", line 225, in forward
[rank0]:     x = layer(x, rotary_pos_emb=rope, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen, is_first_microbatch=is_first_microbatch)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1882, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1830, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/nanoplm/src/nanoplm/pretraining/models/modern_bert/modelling_te.py", line 148, in forward
[rank0]:     mlp_out = self.mlp(x, is_first_microbatch=is_first_microbatch)
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 1181, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/module/layernorm_mlp.py", line 2153, in forward
[rank0]:     out = fwd_fn(
[rank0]:           ^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/torch/autograd/function.py", line 583, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/module/layernorm_mlp.py", line 885, in forward
[rank0]:     return _LayerNormMLP._forward(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/module/layernorm_mlp.py", line 546, in _forward
[rank0]:     fc1_outputs = general_gemm(
[rank0]:                   ^^^^^^^^^^^^^
[rank0]:   File "/workspace/.venv/lib/python3.11/site-packages/transformer_engine/pytorch/cpp_extensions/gemm.py", line 205, in general_gemm
[rank0]:     out, bias_grad, gelu_input, extra_output = tex.generic_gemm(*args, **kwargs)
[rank0]:                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 79.21 GiB of which 507.19 MiB is free. Process 3831621 has 10.60 GiB memory in use. Process 3831620 has 68.10 GiB memory in use. Of the allocated memory 66.48 GiB is allocated by PyTorch, and 211.89 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mrun-20020210-2[0m at: [34mhttps://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/7ij88mph[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260220_021004-7ij88mph/logs[0m
W0220 02:10:13.160000 21806 torch/distributed/elastic/multiprocessing/api.py:1010] Sending process 21896 closing signal SIGTERM
E0220 02:10:13.286000 21806 torch/distributed/elastic/multiprocessing/api.py:984] failed (exitcode: 1) local_rank: 0 (pid: 21895) of binary: /workspace/.venv/bin/python3
Traceback (most recent call last):
  File "/workspace/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 362, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 991, in main
    run(args)
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/run.py", line 982, in run
    elastic_launch(
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 170, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/.venv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 317, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/workspace/.venv/bin/nanoplm FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2026-02-20_02:10:13
  host      : f8173ef52a94
  rank      : 1 (local_rank: 1)
  exitcode  : -15 (pid: 21896)
  error_file: <N/A>
  traceback : Signal 15 (SIGTERM) received by PID 21896
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-20_02:10:13
  host      : f8173ef52a94
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 21895)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0220 02:10:21.050000 22194 torch/distributed/run.py:852] 
W0220 02:10:21.050000 22194 torch/distributed/run.py:852] *****************************************
W0220 02:10:21.050000 22194 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:10:21.050000 22194 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 70492.50it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 69615.00it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 41527.76it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 59074.70it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=32,768, grad_accum_steps=4, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020210-3
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=32,768, grad_accum_steps=4, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020210-4
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=16,384 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 781], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=16,384 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 781], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 3303 (from 3799) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 3303 (from 3799) to prevent distributed deadlock with variable packing
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=4, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run he2w7kpy
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_021031-he2w7kpy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020210-3
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/he2w7kpy
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=4, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8260] loss=4.3134 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=74.2020 tok/s=2,961 raw_tok/s=2,992 step_tokens=129,739 waste=1.0% h100_mfu=2.96% vram=15,543/61,736MB peak=54,013/61,736MB
[step 2/8260] loss=4.3343 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=93.6907 tok/s=95,074 raw_tok/s=96,027 step_tokens=129,771 waste=1.0% h100_mfu=94.89% vram=15,543/66,346MB peak=59,167/66,346MB
[step 3/8260] loss=4.3520 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=94.1119 tok/s=94,512 raw_tok/s=95,634 step_tokens=129,534 waste=1.2% h100_mfu=94.50% vram=15,543/66,346MB peak=59,167/66,346MB
[step 4/8260] loss=4.3083 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=93.2583 tok/s=87,436 raw_tok/s=88,365 step_tokens=129,694 waste=1.1% h100_mfu=87.32% vram=15,543/66,346MB peak=59,167/66,346MB
[step 5/8260] loss=4.2791 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=91.4471 tok/s=85,824 raw_tok/s=86,580 step_tokens=129,928 waste=0.9% h100_mfu=85.55% vram=15,543/66,346MB peak=59,167/66,346MB
[step 6/8260] loss=4.2364 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=89.0828 tok/s=86,130 raw_tok/s=86,654 step_tokens=130,280 waste=0.6% h100_mfu=85.62% vram=15,543/66,346MB peak=59,167/66,346MB
[step 7/8260] loss=4.1393 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=84.2256 tok/s=85,212 raw_tok/s=86,513 step_tokens=129,101 waste=1.5% h100_mfu=85.49% vram=15,543/66,346MB peak=59,167/66,346MB
[step 8/8260] loss=4.0079 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=76.9760 tok/s=85,485 raw_tok/s=86,405 step_tokens=129,676 waste=1.1% h100_mfu=85.38% vram=15,543/66,346MB peak=59,167/66,346MB
[step 9/8260] loss=3.8510 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=68.4375 tok/s=85,291 raw_tok/s=86,181 step_tokens=129,719 waste=1.0% h100_mfu=85.16% vram=15,543/66,346MB peak=59,167/66,346MB
[step 10/8260] loss=3.6388 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=59.2212 tok/s=85,507 raw_tok/s=86,591 step_tokens=129,431 waste=1.3% h100_mfu=85.56% vram=15,543/66,346MB peak=59,167/66,346MB
[step 11/8260] loss=3.4605 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=49.8328 tok/s=85,467 raw_tok/s=86,054 step_tokens=130,178 waste=0.7% h100_mfu=85.03% vram=15,543/66,346MB peak=59,167/66,346MB
[step 12/8260] loss=3.2990 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=44.0713 tok/s=85,771 raw_tok/s=86,336 step_tokens=130,214 waste=0.7% h100_mfu=85.31% vram=15,543/66,346MB peak=59,167/66,346MB
[step 13/8260] loss=3.1829 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=37.7461 tok/s=85,610 raw_tok/s=86,073 step_tokens=130,367 waste=0.5% h100_mfu=85.05% vram=15,543/66,346MB peak=59,167/66,346MB
[step 14/8260] loss=3.1103 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=34.6650 tok/s=85,292 raw_tok/s=86,129 step_tokens=129,798 waste=1.0% h100_mfu=85.11% vram=15,543/66,346MB peak=59,167/66,346MB
[step 15/8260] loss=3.0728 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=32.2735 tok/s=85,261 raw_tok/s=86,413 step_tokens=129,325 waste=1.3% h100_mfu=85.39% vram=15,543/66,346MB peak=59,167/66,346MB
[step 16/8260] loss=3.0586 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=29.0746 tok/s=85,904 raw_tok/s=86,500 step_tokens=130,169 waste=0.7% h100_mfu=85.47% vram=15,543/66,346MB peak=59,167/66,346MB
[step 17/8260] loss=3.0696 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=28.6367 tok/s=85,955 raw_tok/s=86,605 step_tokens=130,089 waste=0.7% h100_mfu=85.58% vram=15,543/66,346MB peak=59,167/66,346MB
[step 18/8260] loss=3.0915 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=31.3119 tok/s=85,438 raw_tok/s=86,093 step_tokens=130,074 waste=0.8% h100_mfu=85.07% vram=15,543/66,346MB peak=59,167/66,346MB
[step 19/8260] loss=3.1416 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=30.9523 tok/s=86,036 raw_tok/s=86,662 step_tokens=130,125 waste=0.7% h100_mfu=85.63% vram=15,543/66,346MB peak=59,167/66,346MB
[step 20/8260] loss=3.1684 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=26.7318 tok/s=85,675 raw_tok/s=86,337 step_tokens=130,066 waste=0.8% h100_mfu=85.31% vram=15,543/66,346MB peak=59,167/66,346MB
[step 21/8260] loss=3.1705 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=28.0052 tok/s=85,633 raw_tok/s=86,438 step_tokens=129,852 waste=0.9% h100_mfu=85.41% vram=15,543/66,346MB peak=59,167/66,346MB
[step 22/8260] loss=3.2360 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=33.0122 tok/s=86,374 raw_tok/s=86,803 step_tokens=130,423 waste=0.5% h100_mfu=85.77% vram=15,543/66,346MB peak=59,167/66,346MB
W0220 02:11:54.019000 23343 torch/distributed/run.py:852] 
W0220 02:11:54.019000 23343 torch/distributed/run.py:852] *****************************************
W0220 02:11:54.019000 23343 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:11:54.019000 23343 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 63791.70it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 59283.45it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1916.74it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 55007.27it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=32,768, grad_accum_steps=4, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020212
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=32,768, grad_accum_steps=4, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020212-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=16,384 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 781], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=16,384 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 781], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 3303 (from 3799) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 3303 (from 3799) to prevent distributed deadlock with variable packing
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=4, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 2q6on4vc
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_021203-2q6on4vc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020212
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/2q6on4vc
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 835.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=4, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.3, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8260] loss=4.3137 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=74.2230 tok/s=4,806 raw_tok/s=4,855 step_tokens=129,739 waste=1.0% h100_mfu=4.80% vram=15,543/63,668MB peak=54,031/63,668MB
[step 2/8260] loss=4.3395 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=93.6971 tok/s=94,150 raw_tok/s=95,093 step_tokens=129,771 waste=1.0% h100_mfu=93.96% vram=15,543/67,318MB peak=59,185/67,318MB
[step 3/8260] loss=4.3511 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=94.1332 tok/s=93,809 raw_tok/s=94,923 step_tokens=129,534 waste=1.2% h100_mfu=93.80% vram=15,543/67,318MB peak=59,185/67,318MB
[step 4/8260] loss=4.3010 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=93.1655 tok/s=86,186 raw_tok/s=87,102 step_tokens=129,694 waste=1.1% h100_mfu=86.07% vram=15,543/67,318MB peak=59,185/67,318MB
[step 5/8260] loss=4.2820 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=91.2870 tok/s=85,060 raw_tok/s=85,809 step_tokens=129,928 waste=0.9% h100_mfu=84.79% vram=15,543/67,318MB peak=59,185/67,318MB
[step 6/8260] loss=4.2366 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=89.0014 tok/s=85,834 raw_tok/s=86,356 step_tokens=130,280 waste=0.6% h100_mfu=85.33% vram=15,543/67,318MB peak=59,185/67,318MB
[step 7/8260] loss=4.1378 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=84.1195 tok/s=84,404 raw_tok/s=85,693 step_tokens=129,101 waste=1.5% h100_mfu=84.67% vram=15,543/67,318MB peak=59,185/67,318MB
[step 8/8260] loss=4.0107 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=76.9193 tok/s=84,992 raw_tok/s=85,907 step_tokens=129,676 waste=1.1% h100_mfu=84.89% vram=15,543/67,318MB peak=59,185/67,318MB
[step 9/8260] loss=3.8523 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=68.3097 tok/s=84,803 raw_tok/s=85,687 step_tokens=129,719 waste=1.0% h100_mfu=84.67% vram=15,543/67,318MB peak=59,185/67,318MB
[step 10/8260] loss=3.6382 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=59.1582 tok/s=84,420 raw_tok/s=85,491 step_tokens=129,431 waste=1.3% h100_mfu=84.48% vram=15,543/67,318MB peak=59,185/67,318MB
[step 11/8260] loss=3.4588 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=49.7662 tok/s=84,793 raw_tok/s=85,375 step_tokens=130,178 waste=0.7% h100_mfu=84.36% vram=15,543/67,318MB peak=59,185/67,318MB
[step 12/8260] loss=3.2990 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=44.1068 tok/s=84,660 raw_tok/s=85,217 step_tokens=130,214 waste=0.7% h100_mfu=84.21% vram=15,543/67,318MB peak=59,185/67,318MB
[step 13/8260] loss=3.1786 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=37.7293 tok/s=84,903 raw_tok/s=85,362 step_tokens=130,367 waste=0.5% h100_mfu=84.35% vram=15,543/67,318MB peak=59,185/67,318MB
[step 14/8260] loss=3.1080 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=34.6281 tok/s=84,718 raw_tok/s=85,550 step_tokens=129,798 waste=1.0% h100_mfu=84.53% vram=15,543/67,318MB peak=59,185/67,318MB
[step 15/8260] loss=3.0627 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=32.2438 tok/s=84,316 raw_tok/s=85,455 step_tokens=129,325 waste=1.3% h100_mfu=84.44% vram=15,543/67,318MB peak=59,185/67,318MB
W0220 02:12:59.053000 24219 torch/distributed/run.py:852] 
W0220 02:12:59.053000 24219 torch/distributed/run.py:852] *****************************************
W0220 02:12:59.053000 24219 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:12:59.053000 24219 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1167.27it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 71392.41it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 13808.41it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 65281.00it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=32,768, grad_accum_steps=4, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020213
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 2701402144
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=32,768, grad_accum_steps=4, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020213-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=16,384 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 781], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=16,384 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 781], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=65 tensors, adamw_params=36 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 3303 (from 3799) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 3303 (from 3799) to prevent distributed deadlock with variable packing
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=4, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run u5l2wdl6
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_021308-u5l2wdl6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020213-2
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/u5l2wdl6
MFU estimation: 16,509,566,976 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8260, warmup_steps=302, grad_accum=4, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8260] loss=4.3162 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=74.2236 tok/s=5,281 raw_tok/s=5,335 step_tokens=129,739 waste=1.0% h100_mfu=4.45% vram=15,543/61,736MB peak=54,013/61,736MB
[step 2/8260] loss=4.3364 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=93.7064 tok/s=95,054 raw_tok/s=96,007 step_tokens=129,771 waste=1.0% h100_mfu=80.10% vram=15,543/66,346MB peak=59,167/66,346MB
[step 3/8260] loss=4.3525 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=94.1442 tok/s=94,528 raw_tok/s=95,651 step_tokens=129,534 waste=1.2% h100_mfu=79.80% vram=15,543/66,346MB peak=59,167/66,346MB
[step 4/8260] loss=4.3052 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=93.1476 tok/s=87,507 raw_tok/s=88,437 step_tokens=129,694 waste=1.1% h100_mfu=73.79% vram=15,543/66,346MB peak=59,167/66,346MB
[step 5/8260] loss=4.2789 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=91.3153 tok/s=85,535 raw_tok/s=86,288 step_tokens=129,928 waste=0.9% h100_mfu=71.99% vram=15,543/66,346MB peak=59,167/66,346MB
[step 6/8260] loss=4.2381 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=88.9766 tok/s=86,458 raw_tok/s=86,984 step_tokens=130,280 waste=0.6% h100_mfu=72.57% vram=15,543/66,346MB peak=59,167/66,346MB
W0220 02:13:47.863000 24997 torch/distributed/run.py:852] 
W0220 02:13:47.863000 24997 torch/distributed/run.py:852] *****************************************
W0220 02:13:47.863000 24997 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:13:47.863000 24997 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 37282.70it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1112.55it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 55738.26it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3436.55it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 71413280
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=262,144, grad_accum_steps=1, effective_global_batch_size=262,144 tokens
Created directory: output/pretraining_checkpoints/run-20020213-3
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 71413280
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=262,144, grad_accum_steps=1, effective_global_batch_size=262,144 tokens
Created directory: output/pretraining_checkpoints/run-20020213-4
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=131,072 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 4096, 6242], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=28 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=131,072 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 4096, 6242], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=28 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 412 (from 475) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 412 (from 475) to prevent distributed deadlock with variable packing
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=4120, warmup_steps=302, grad_accum=1, achieved_global_batch_size=262,144 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 2prs18d2
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_021356-2prs18d2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020213-4
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/2prs18d2
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=4120, warmup_steps=302, grad_accum=1, achieved_global_batch_size=262,144 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/4120] loss=3.7931 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=8.0075 tok/s=7,001 raw_tok/s=7,010 step_tokens=261,829 waste=0.1% h100_mfu=0.17% vram=401/33,920MB peak=32,675/33,920MB
[step 2/4120] loss=3.7865 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=12.0994 tok/s=1,725,113 raw_tok/s=1,727,650 step_tokens=261,759 waste=0.1% h100_mfu=42.03% vram=401/33,922MB peak=32,813/33,922MB
[step 3/4120] loss=3.7846 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=12.1755 tok/s=1,629,774 raw_tok/s=1,631,585 step_tokens=261,853 waste=0.1% h100_mfu=39.70% vram=401/33,922MB peak=32,814/33,922MB
[step 4/4120] loss=3.7895 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=12.0278 tok/s=1,621,470 raw_tok/s=1,623,452 step_tokens=261,824 waste=0.1% h100_mfu=39.50% vram=401/33,922MB peak=32,815/33,922MB
[step 5/4120] loss=3.7999 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=12.1497 tok/s=1,683,050 raw_tok/s=1,685,101 step_tokens=261,825 waste=0.1% h100_mfu=41.00% vram=401/33,922MB peak=32,814/33,922MB
[step 6/4120] loss=3.7876 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=12.0301 tok/s=1,550,153 raw_tok/s=1,551,484 step_tokens=261,919 waste=0.1% h100_mfu=37.75% vram=401/33,922MB peak=32,814/33,922MB
[step 7/4120] loss=3.7859 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=12.1192 tok/s=1,649,580 raw_tok/s=1,650,864 step_tokens=261,940 waste=0.1% h100_mfu=40.17% vram=401/33,922MB peak=32,814/33,922MB
[step 8/4120] loss=3.7798 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=11.9925 tok/s=1,557,648 raw_tok/s=1,560,058 step_tokens=261,739 waste=0.2% h100_mfu=37.96% vram=401/33,922MB peak=32,815/33,922MB
[step 9/4120] loss=3.7751 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=12.0872 tok/s=1,591,982 raw_tok/s=1,594,098 step_tokens=261,796 waste=0.1% h100_mfu=38.78% vram=401/33,922MB peak=32,814/33,922MB
[step 10/4120] loss=3.7592 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=11.8654 tok/s=1,577,850 raw_tok/s=1,578,627 step_tokens=262,015 waste=0.0% h100_mfu=38.41% vram=401/33,922MB peak=32,814/33,922MB
[step 11/4120] loss=3.7484 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=11.8615 tok/s=1,588,572 raw_tok/s=1,592,204 step_tokens=261,546 waste=0.2% h100_mfu=38.74% vram=401/33,922MB peak=32,814/33,922MB
[step 12/4120] loss=3.7352 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=11.8188 tok/s=1,592,335 raw_tok/s=1,593,435 step_tokens=261,963 waste=0.1% h100_mfu=38.77% vram=401/33,922MB peak=32,815/33,922MB
[step 13/4120] loss=3.7273 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=11.7339 tok/s=1,517,398 raw_tok/s=1,520,024 step_tokens=261,691 waste=0.2% h100_mfu=36.98% vram=401/33,922MB peak=32,814/33,922MB
[step 14/4120] loss=3.7025 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=11.5885 tok/s=1,591,285 raw_tok/s=1,593,151 step_tokens=261,837 waste=0.1% h100_mfu=38.76% vram=401/33,922MB peak=32,814/33,922MB
[step 15/4120] loss=3.6816 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=11.4358 tok/s=1,591,392 raw_tok/s=1,594,257 step_tokens=261,673 waste=0.2% h100_mfu=38.79% vram=401/33,922MB peak=32,814/33,922MB
[step 16/4120] loss=3.6495 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=11.2082 tok/s=1,673,761 raw_tok/s=1,677,710 step_tokens=261,527 waste=0.2% h100_mfu=40.82% vram=401/33,922MB peak=32,815/33,922MB
[step 17/4120] loss=3.6255 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=11.0612 tok/s=1,581,985 raw_tok/s=1,582,981 step_tokens=261,979 waste=0.1% h100_mfu=38.51% vram=401/33,922MB peak=32,814/33,922MB
[step 18/4120] loss=3.6008 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=10.9381 tok/s=1,588,228 raw_tok/s=1,589,672 step_tokens=261,906 waste=0.1% h100_mfu=38.68% vram=401/33,922MB peak=32,814/33,922MB
[step 19/4120] loss=3.5663 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=10.7287 tok/s=1,582,413 raw_tok/s=1,583,627 step_tokens=261,943 waste=0.1% h100_mfu=38.53% vram=401/33,922MB peak=32,814/33,922MB
[step 20/4120] loss=3.5407 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=10.5433 tok/s=1,638,547 raw_tok/s=1,639,679 step_tokens=261,963 waste=0.1% h100_mfu=39.89% vram=401/33,922MB peak=32,815/33,922MB
[step 21/4120] loss=3.5083 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=10.2653 tok/s=1,591,635 raw_tok/s=1,594,183 step_tokens=261,725 waste=0.2% h100_mfu=38.79% vram=401/33,922MB peak=32,814/33,922MB
[step 22/4120] loss=3.4756 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=10.1822 tok/s=1,554,129 raw_tok/s=1,555,880 step_tokens=261,849 waste=0.1% h100_mfu=37.85% vram=401/33,922MB peak=32,814/33,922MB
[step 23/4120] loss=3.4435 lr=7.28e-06 muon_lr=7.28e-05 grad_norm=9.8579 tok/s=1,533,719 raw_tok/s=1,534,638 step_tokens=261,987 waste=0.1% h100_mfu=37.34% vram=401/33,922MB peak=32,814/33,922MB
[step 24/4120] loss=3.4158 lr=7.62e-06 muon_lr=7.62e-05 grad_norm=9.7421 tok/s=1,675,719 raw_tok/s=1,678,229 step_tokens=261,752 waste=0.1% h100_mfu=40.83% vram=401/33,922MB peak=32,815/33,922MB
[step 25/4120] loss=3.3826 lr=7.95e-06 muon_lr=7.95e-05 grad_norm=9.4817 tok/s=1,593,056 raw_tok/s=1,593,926 step_tokens=262,001 waste=0.1% h100_mfu=38.78% vram=401/33,922MB peak=32,814/33,922MB
[step 26/4120] loss=3.3476 lr=8.28e-06 muon_lr=8.28e-05 grad_norm=9.1826 tok/s=1,541,451 raw_tok/s=1,542,521 step_tokens=261,962 waste=0.1% h100_mfu=37.53% vram=401/33,922MB peak=32,814/33,922MB
[step 27/4120] loss=3.3038 lr=8.61e-06 muon_lr=8.61e-05 grad_norm=8.9751 tok/s=1,677,130 raw_tok/s=1,678,879 step_tokens=261,871 waste=0.1% h100_mfu=40.85% vram=401/33,922MB peak=32,814/33,922MB
[step 28/4120] loss=3.2797 lr=8.94e-06 muon_lr=8.94e-05 grad_norm=8.5890 tok/s=1,581,047 raw_tok/s=1,583,076 step_tokens=261,808 waste=0.1% h100_mfu=38.52% vram=401/33,922MB peak=32,815/33,922MB
[step 29/4120] loss=3.2457 lr=9.27e-06 muon_lr=9.27e-05 grad_norm=8.3402 tok/s=1,589,953 raw_tok/s=1,592,718 step_tokens=261,689 waste=0.2% h100_mfu=38.75% vram=401/33,922MB peak=32,814/33,922MB
[step 30/4120] loss=3.2051 lr=9.60e-06 muon_lr=9.60e-05 grad_norm=8.0580 tok/s=1,585,763 raw_tok/s=1,587,441 step_tokens=261,867 waste=0.1% h100_mfu=38.62% vram=401/33,922MB peak=32,814/33,922MB
[step 31/4120] loss=3.1697 lr=9.93e-06 muon_lr=9.93e-05 grad_norm=7.6858 tok/s=1,593,519 raw_tok/s=1,593,677 step_tokens=262,118 waste=0.0% h100_mfu=38.77% vram=401/33,922MB peak=32,814/33,922MB
[step 32/4120] loss=3.1392 lr=1.03e-05 muon_lr=1.03e-04 grad_norm=7.3663 tok/s=1,568,149 raw_tok/s=1,569,328 step_tokens=261,947 waste=0.1% h100_mfu=38.18% vram=401/33,922MB peak=32,815/33,922MB
[step 33/4120] loss=3.1073 lr=1.06e-05 muon_lr=1.06e-04 grad_norm=6.9658 tok/s=1,644,495 raw_tok/s=1,647,537 step_tokens=261,660 waste=0.2% h100_mfu=40.08% vram=401/33,922MB peak=32,814/33,922MB
[step 34/4120] loss=3.0746 lr=1.09e-05 muon_lr=1.09e-04 grad_norm=6.6149 tok/s=1,588,914 raw_tok/s=1,589,848 step_tokens=261,990 waste=0.1% h100_mfu=38.68% vram=401/33,922MB peak=32,814/33,922MB
[step 35/4120] loss=3.0529 lr=1.13e-05 muon_lr=1.13e-04 grad_norm=6.2422 tok/s=1,628,205 raw_tok/s=1,629,137 step_tokens=261,994 waste=0.1% h100_mfu=39.64% vram=401/33,922MB peak=32,814/33,922MB
[step 36/4120] loss=3.0241 lr=1.16e-05 muon_lr=1.16e-04 grad_norm=5.7850 tok/s=1,674,635 raw_tok/s=1,677,251 step_tokens=261,735 waste=0.2% h100_mfu=40.81% vram=401/33,922MB peak=32,815/33,922MB
[step 37/4120] loss=2.9894 lr=1.19e-05 muon_lr=1.19e-04 grad_norm=5.4344 tok/s=1,658,226 raw_tok/s=1,658,858 step_tokens=262,044 waste=0.0% h100_mfu=40.36% vram=401/33,922MB peak=32,814/33,922MB
[step 38/4120] loss=2.9700 lr=1.23e-05 muon_lr=1.23e-04 grad_norm=4.9718 tok/s=1,581,037 raw_tok/s=1,581,170 step_tokens=262,122 waste=0.0% h100_mfu=38.47% vram=401/33,922MB peak=32,814/33,922MB
[step 39/4120] loss=2.9566 lr=1.26e-05 muon_lr=1.26e-04 grad_norm=4.5869 tok/s=1,585,239 raw_tok/s=1,587,534 step_tokens=261,765 waste=0.1% h100_mfu=38.62% vram=401/33,922MB peak=32,814/33,922MB
[step 40/4120] loss=2.9315 lr=1.29e-05 muon_lr=1.29e-04 grad_norm=4.2191 tok/s=1,670,219 raw_tok/s=1,672,733 step_tokens=261,750 waste=0.2% h100_mfu=40.70% vram=401/33,922MB peak=32,815/33,922MB
[step 41/4120] loss=2.9124 lr=1.32e-05 muon_lr=1.32e-04 grad_norm=3.9718 tok/s=1,592,332 raw_tok/s=1,593,486 step_tokens=261,954 waste=0.1% h100_mfu=38.77% vram=401/33,922MB peak=32,814/33,922MB
[step 42/4120] loss=2.9069 lr=1.36e-05 muon_lr=1.36e-04 grad_norm=3.7677 tok/s=1,674,123 raw_tok/s=1,676,579 step_tokens=261,760 waste=0.1% h100_mfu=40.79% vram=401/33,922MB peak=32,814/33,922MB
[step 43/4120] loss=2.8977 lr=1.39e-05 muon_lr=1.39e-04 grad_norm=3.6575 tok/s=1,675,443 raw_tok/s=1,677,382 step_tokens=261,841 waste=0.1% h100_mfu=40.81% vram=401/33,922MB peak=32,814/33,922MB
[step 44/4120] loss=2.8810 lr=1.42e-05 muon_lr=1.42e-04 grad_norm=3.5153 tok/s=1,662,277 raw_tok/s=1,663,616 step_tokens=261,933 waste=0.1% h100_mfu=40.48% vram=401/33,922MB peak=32,815/33,922MB
[step 45/4120] loss=2.8684 lr=1.46e-05 muon_lr=1.46e-04 grad_norm=3.4415 tok/s=1,658,948 raw_tok/s=1,659,631 step_tokens=262,036 waste=0.0% h100_mfu=40.38% vram=401/33,922MB peak=32,814/33,922MB
[step 46/4120] loss=2.8597 lr=1.49e-05 muon_lr=1.49e-04 grad_norm=3.4097 tok/s=1,642,557 raw_tok/s=1,646,836 step_tokens=261,463 waste=0.3% h100_mfu=40.07% vram=401/33,922MB peak=32,814/33,922MB
[step 47/4120] loss=2.8612 lr=1.52e-05 muon_lr=1.52e-04 grad_norm=3.4208 tok/s=1,577,517 raw_tok/s=1,580,206 step_tokens=261,698 waste=0.2% h100_mfu=38.45% vram=401/33,922MB peak=32,814/33,922MB
[step 48/4120] loss=2.8454 lr=1.56e-05 muon_lr=1.56e-04 grad_norm=3.2538 tok/s=1,671,658 raw_tok/s=1,675,903 step_tokens=261,480 waste=0.3% h100_mfu=40.77% vram=401/33,922MB peak=32,815/33,922MB
[step 49/4120] loss=2.8444 lr=1.59e-05 muon_lr=1.59e-04 grad_norm=3.1322 tok/s=1,562,708 raw_tok/s=1,564,092 step_tokens=261,912 waste=0.1% h100_mfu=38.05% vram=401/33,922MB peak=32,814/33,922MB
[step 50/4120] loss=2.8308 lr=1.62e-05 muon_lr=1.62e-04 grad_norm=2.8378 tok/s=1,564,828 raw_tok/s=1,567,087 step_tokens=261,766 waste=0.1% h100_mfu=38.13% vram=401/33,922MB peak=32,814/33,922MB
[step 51/4120] loss=2.8223 lr=1.66e-05 muon_lr=1.66e-04 grad_norm=2.7293 tok/s=1,588,342 raw_tok/s=1,589,943 step_tokens=261,880 waste=0.1% h100_mfu=38.68% vram=401/33,922MB peak=32,814/33,922MB
[step 52/4120] loss=2.8148 lr=1.69e-05 muon_lr=1.69e-04 grad_norm=2.3857 tok/s=1,677,031 raw_tok/s=1,677,370 step_tokens=262,091 waste=0.0% h100_mfu=40.81% vram=401/33,922MB peak=32,815/33,922MB
[step 53/4120] loss=2.8073 lr=1.72e-05 muon_lr=1.72e-04 grad_norm=2.1532 tok/s=1,672,739 raw_tok/s=1,673,895 step_tokens=261,963 waste=0.1% h100_mfu=40.73% vram=401/33,922MB peak=32,814/33,922MB
[step 54/4120] loss=2.7975 lr=1.75e-05 muon_lr=1.75e-04 grad_norm=2.0645 tok/s=1,672,321 raw_tok/s=1,674,812 step_tokens=261,754 waste=0.1% h100_mfu=40.75% vram=401/33,922MB peak=32,814/33,922MB
[step 55/4120] loss=2.7915 lr=1.79e-05 muon_lr=1.79e-04 grad_norm=1.9085 tok/s=1,645,526 raw_tok/s=1,646,657 step_tokens=261,964 waste=0.1% h100_mfu=40.06% vram=401/33,922MB peak=32,814/33,922MB
[step 56/4120] loss=2.7959 lr=1.82e-05 muon_lr=1.82e-04 grad_norm=1.9291 tok/s=1,647,003 raw_tok/s=1,649,299 step_tokens=261,779 waste=0.1% h100_mfu=40.13% vram=401/33,922MB peak=32,815/33,922MB
[step 57/4120] loss=2.7850 lr=1.85e-05 muon_lr=1.85e-04 grad_norm=1.8716 tok/s=1,581,169 raw_tok/s=1,582,491 step_tokens=261,925 waste=0.1% h100_mfu=38.50% vram=401/33,922MB peak=32,814/33,922MB
[step 58/4120] loss=2.7818 lr=1.89e-05 muon_lr=1.89e-04 grad_norm=1.8442 tok/s=1,513,750 raw_tok/s=1,516,411 step_tokens=261,684 waste=0.2% h100_mfu=36.89% vram=401/33,922MB peak=32,814/33,922MB
[step 59/4120] loss=2.7703 lr=1.92e-05 muon_lr=1.92e-04 grad_norm=1.7885 tok/s=1,608,035 raw_tok/s=1,611,632 step_tokens=261,559 waste=0.2% h100_mfu=39.21% vram=401/33,922MB peak=32,814/33,922MB
[step 60/4120] loss=2.7667 lr=1.95e-05 muon_lr=1.95e-04 grad_norm=1.7039 tok/s=1,589,828 raw_tok/s=1,590,799 step_tokens=261,984 waste=0.1% h100_mfu=38.70% vram=401/33,922MB peak=32,815/33,922MB
[step 61/4120] loss=2.7508 lr=1.99e-05 muon_lr=1.99e-04 grad_norm=1.4411 tok/s=1,663,673 raw_tok/s=1,665,706 step_tokens=261,824 waste=0.1% h100_mfu=40.53% vram=401/33,922MB peak=32,814/33,922MB
[step 62/4120] loss=2.7473 lr=2.02e-05 muon_lr=2.02e-04 grad_norm=1.3272 tok/s=1,584,818 raw_tok/s=1,586,409 step_tokens=261,881 waste=0.1% h100_mfu=38.60% vram=401/33,922MB peak=32,814/33,922MB
[step 63/4120] loss=2.7395 lr=2.05e-05 muon_lr=2.05e-04 grad_norm=1.2178 tok/s=1,675,077 raw_tok/s=1,675,972 step_tokens=262,004 waste=0.1% h100_mfu=40.78% vram=401/33,922MB peak=32,814/33,922MB
[step 64/4120] loss=2.7337 lr=2.09e-05 muon_lr=2.09e-04 grad_norm=1.0356 tok/s=1,590,769 raw_tok/s=1,592,470 step_tokens=261,864 waste=0.1% h100_mfu=38.74% vram=401/33,922MB peak=32,815/33,922MB
[step 65/4120] loss=2.7428 lr=2.12e-05 muon_lr=2.12e-04 grad_norm=0.9939 tok/s=1,581,176 raw_tok/s=1,584,380 step_tokens=261,614 waste=0.2% h100_mfu=38.55% vram=401/33,922MB peak=32,814/33,922MB
[step 66/4120] loss=2.7314 lr=2.15e-05 muon_lr=2.15e-04 grad_norm=1.0359 tok/s=1,674,155 raw_tok/s=1,675,664 step_tokens=261,908 waste=0.1% h100_mfu=40.77% vram=401/33,922MB peak=32,814/33,922MB
[step 67/4120] loss=2.7311 lr=2.19e-05 muon_lr=2.19e-04 grad_norm=1.2019 tok/s=1,670,229 raw_tok/s=1,671,868 step_tokens=261,887 waste=0.1% h100_mfu=40.68% vram=401/33,922MB peak=32,814/33,922MB
[step 68/4120] loss=2.7261 lr=2.22e-05 muon_lr=2.22e-04 grad_norm=1.0748 tok/s=1,519,555 raw_tok/s=1,519,990 step_tokens=262,069 waste=0.0% h100_mfu=36.98% vram=401/33,922MB peak=32,815/33,922MB
[step 69/4120] loss=2.7189 lr=2.25e-05 muon_lr=2.25e-04 grad_norm=1.0102 tok/s=1,572,781 raw_tok/s=1,575,179 step_tokens=261,745 waste=0.2% h100_mfu=38.32% vram=401/33,922MB peak=32,814/33,922MB
[step 70/4120] loss=2.7290 lr=2.28e-05 muon_lr=2.28e-04 grad_norm=0.8805 tok/s=1,585,388 raw_tok/s=1,588,327 step_tokens=261,659 waste=0.2% h100_mfu=38.64% vram=401/33,922MB peak=32,814/33,922MB
[step 71/4120] loss=2.7156 lr=2.32e-05 muon_lr=2.32e-04 grad_norm=0.7796 tok/s=1,657,355 raw_tok/s=1,659,285 step_tokens=261,839 waste=0.1% h100_mfu=40.37% vram=401/33,922MB peak=32,814/33,922MB
[step 72/4120] loss=2.7067 lr=2.35e-05 muon_lr=2.35e-04 grad_norm=0.6459 tok/s=1,631,233 raw_tok/s=1,633,046 step_tokens=261,853 waste=0.1% h100_mfu=39.73% vram=401/33,922MB peak=32,815/33,922MB
[step 73/4120] loss=2.7144 lr=2.38e-05 muon_lr=2.38e-04 grad_norm=0.6413 tok/s=1,657,839 raw_tok/s=1,659,225 step_tokens=261,925 waste=0.1% h100_mfu=40.37% vram=401/33,922MB peak=32,814/33,922MB
[step 74/4120] loss=2.7161 lr=2.42e-05 muon_lr=2.42e-04 grad_norm=0.6179 tok/s=1,606,620 raw_tok/s=1,608,006 step_tokens=261,918 waste=0.1% h100_mfu=39.12% vram=401/33,922MB peak=32,814/33,922MB
[step 75/4120] loss=2.7087 lr=2.45e-05 muon_lr=2.45e-04 grad_norm=0.6630 tok/s=1,519,489 raw_tok/s=1,521,428 step_tokens=261,810 waste=0.1% h100_mfu=37.02% vram=401/33,922MB peak=32,814/33,922MB
[step 76/4120] loss=2.7154 lr=2.48e-05 muon_lr=2.48e-04 grad_norm=0.5895 tok/s=1,660,606 raw_tok/s=1,664,676 step_tokens=261,503 waste=0.2% h100_mfu=40.50% vram=401/33,922MB peak=32,815/33,922MB
[step 77/4120] loss=2.7075 lr=2.52e-05 muon_lr=2.52e-04 grad_norm=0.6318 tok/s=1,550,407 raw_tok/s=1,551,366 step_tokens=261,982 waste=0.1% h100_mfu=37.74% vram=401/33,922MB peak=32,814/33,922MB
[step 78/4120] loss=2.6934 lr=2.55e-05 muon_lr=2.55e-04 grad_norm=0.6413 tok/s=1,652,539 raw_tok/s=1,657,015 step_tokens=261,436 waste=0.3% h100_mfu=40.32% vram=401/33,922MB peak=32,814/33,922MB
[step 79/4120] loss=2.6946 lr=2.58e-05 muon_lr=2.58e-04 grad_norm=0.6230 tok/s=1,554,633 raw_tok/s=1,556,777 step_tokens=261,783 waste=0.1% h100_mfu=37.88% vram=401/33,922MB peak=32,814/33,922MB
[step 80/4120] loss=2.7071 lr=2.62e-05 muon_lr=2.62e-04 grad_norm=0.4768 tok/s=1,560,079 raw_tok/s=1,561,169 step_tokens=261,961 waste=0.1% h100_mfu=37.98% vram=401/33,922MB peak=32,815/33,922MB
[step 81/4120] loss=2.7017 lr=2.65e-05 muon_lr=2.65e-04 grad_norm=0.4656 tok/s=1,569,455 raw_tok/s=1,572,616 step_tokens=261,617 waste=0.2% h100_mfu=38.26% vram=401/33,922MB peak=32,814/33,922MB
[step 82/4120] loss=2.6952 lr=2.68e-05 muon_lr=2.68e-04 grad_norm=0.4517 tok/s=1,458,761 raw_tok/s=1,459,295 step_tokens=262,048 waste=0.0% h100_mfu=35.50% vram=401/33,922MB peak=32,814/33,922MB
[step 83/4120] loss=2.6944 lr=2.72e-05 muon_lr=2.72e-04 grad_norm=0.4218 tok/s=1,587,898 raw_tok/s=1,590,500 step_tokens=261,715 waste=0.2% h100_mfu=38.70% vram=401/33,922MB peak=32,814/33,922MB
[step 84/4120] loss=2.6908 lr=2.75e-05 muon_lr=2.75e-04 grad_norm=0.3761 tok/s=1,583,690 raw_tok/s=1,587,408 step_tokens=261,530 waste=0.2% h100_mfu=38.62% vram=401/33,922MB peak=32,815/33,922MB
[step 85/4120] loss=2.6914 lr=2.78e-05 muon_lr=2.78e-04 grad_norm=0.3538 tok/s=1,538,290 raw_tok/s=1,540,865 step_tokens=261,706 waste=0.2% h100_mfu=37.49% vram=401/33,922MB peak=32,814/33,922MB
[step 86/4120] loss=2.6829 lr=2.81e-05 muon_lr=2.81e-04 grad_norm=0.4036 tok/s=1,533,682 raw_tok/s=1,535,762 step_tokens=261,789 waste=0.1% h100_mfu=37.37% vram=401/33,922MB peak=32,814/33,922MB
[step 87/4120] loss=2.6902 lr=2.85e-05 muon_lr=2.85e-04 grad_norm=0.3480 tok/s=1,578,437 raw_tok/s=1,580,130 step_tokens=261,863 waste=0.1% h100_mfu=38.44% vram=401/33,922MB peak=32,814/33,922MB
[step 88/4120] loss=2.6911 lr=2.88e-05 muon_lr=2.88e-04 grad_norm=0.3878 tok/s=1,572,913 raw_tok/s=1,574,890 step_tokens=261,815 waste=0.1% h100_mfu=38.32% vram=401/33,922MB peak=32,815/33,922MB
[step 89/4120] loss=2.6834 lr=2.91e-05 muon_lr=2.91e-04 grad_norm=0.3779 tok/s=1,525,755 raw_tok/s=1,528,145 step_tokens=261,734 waste=0.2% h100_mfu=37.18% vram=401/33,922MB peak=32,814/33,922MB
[step 90/4120] loss=2.6835 lr=2.95e-05 muon_lr=2.95e-04 grad_norm=0.4151 tok/s=1,665,901 raw_tok/s=1,666,975 step_tokens=261,975 waste=0.1% h100_mfu=40.56% vram=401/33,922MB peak=32,814/33,922MB
[step 91/4120] loss=2.6879 lr=2.98e-05 muon_lr=2.98e-04 grad_norm=0.4389 tok/s=1,586,924 raw_tok/s=1,587,760 step_tokens=262,006 waste=0.1% h100_mfu=38.63% vram=401/33,922MB peak=32,814/33,922MB
[step 92/4120] loss=2.6790 lr=3.01e-05 muon_lr=3.01e-04 grad_norm=0.4932 tok/s=1,569,741 raw_tok/s=1,573,450 step_tokens=261,526 waste=0.2% h100_mfu=38.28% vram=401/33,922MB peak=32,815/33,922MB
[step 93/4120] loss=2.6804 lr=3.05e-05 muon_lr=3.05e-04 grad_norm=0.4609 tok/s=1,587,662 raw_tok/s=1,589,639 step_tokens=261,818 waste=0.1% h100_mfu=38.68% vram=401/33,922MB peak=32,814/33,922MB
[step 94/4120] loss=2.6812 lr=3.08e-05 muon_lr=3.08e-04 grad_norm=0.4908 tok/s=1,660,273 raw_tok/s=1,661,541 step_tokens=261,944 waste=0.1% h100_mfu=40.43% vram=401/33,922MB peak=32,814/33,922MB
[step 95/4120] loss=2.6790 lr=3.11e-05 muon_lr=3.11e-04 grad_norm=0.5527 tok/s=1,659,337 raw_tok/s=1,660,883 step_tokens=261,900 waste=0.1% h100_mfu=40.41% vram=401/33,922MB peak=32,814/33,922MB
[step 96/4120] loss=2.6776 lr=3.15e-05 muon_lr=3.15e-04 grad_norm=0.5696 tok/s=1,658,149 raw_tok/s=1,660,328 step_tokens=261,800 waste=0.1% h100_mfu=40.40% vram=401/33,922MB peak=32,815/33,922MB
[step 97/4120] loss=2.6780 lr=3.18e-05 muon_lr=3.18e-04 grad_norm=0.5580 tok/s=1,581,493 raw_tok/s=1,582,320 step_tokens=262,007 waste=0.1% h100_mfu=38.50% vram=401/33,922MB peak=32,814/33,922MB
[step 98/4120] loss=2.6739 lr=3.21e-05 muon_lr=3.21e-04 grad_norm=0.6036 tok/s=1,652,739 raw_tok/s=1,654,525 step_tokens=261,861 waste=0.1% h100_mfu=40.25% vram=401/33,922MB peak=32,814/33,922MB
[step 99/4120] loss=2.6765 lr=3.25e-05 muon_lr=3.25e-04 grad_norm=0.6189 tok/s=1,581,670 raw_tok/s=1,583,875 step_tokens=261,779 waste=0.1% h100_mfu=38.54% vram=401/33,922MB peak=32,814/33,922MB
[step 100/4120] loss=2.6790 lr=3.28e-05 muon_lr=3.28e-04 grad_norm=0.6907 tok/s=1,583,186 raw_tok/s=1,584,510 step_tokens=261,925 waste=0.1% h100_mfu=38.55% vram=401/33,922MB peak=32,815/33,922MB
[step 101/4120] loss=2.6781 lr=3.31e-05 muon_lr=3.31e-04 grad_norm=0.7082 tok/s=1,582,922 raw_tok/s=1,584,699 step_tokens=261,850 waste=0.1% h100_mfu=38.56% vram=401/33,922MB peak=32,814/33,922MB
[step 102/4120] loss=2.6809 lr=3.34e-05 muon_lr=3.34e-04 grad_norm=0.6785 tok/s=1,537,245 raw_tok/s=1,539,171 step_tokens=261,816 waste=0.1% h100_mfu=37.45% vram=401/33,922MB peak=32,814/33,922MB
[step 103/4120] loss=2.6801 lr=3.38e-05 muon_lr=3.38e-04 grad_norm=0.5724 tok/s=1,589,637 raw_tok/s=1,590,851 step_tokens=261,944 waste=0.1% h100_mfu=38.71% vram=401/33,922MB peak=32,814/33,922MB
[step 104/4120] loss=2.6827 lr=3.41e-05 muon_lr=3.41e-04 grad_norm=0.5727 tok/s=1,585,530 raw_tok/s=1,588,135 step_tokens=261,714 waste=0.2% h100_mfu=38.64% vram=401/33,922MB peak=32,815/33,922MB
[step 105/4120] loss=2.6844 lr=3.44e-05 muon_lr=3.44e-04 grad_norm=0.7215 tok/s=1,539,799 raw_tok/s=1,541,457 step_tokens=261,862 waste=0.1% h100_mfu=37.50% vram=401/33,922MB peak=32,814/33,922MB
[step 106/4120] loss=2.6779 lr=3.48e-05 muon_lr=3.48e-04 grad_norm=0.8060 tok/s=1,595,085 raw_tok/s=1,598,060 step_tokens=261,656 waste=0.2% h100_mfu=38.88% vram=401/33,922MB peak=32,814/33,922MB
[step 107/4120] loss=2.6790 lr=3.51e-05 muon_lr=3.51e-04 grad_norm=0.6285 tok/s=1,671,181 raw_tok/s=1,672,948 step_tokens=261,867 waste=0.1% h100_mfu=40.70% vram=401/33,922MB peak=32,814/33,922MB
[step 108/4120] loss=2.6707 lr=3.54e-05 muon_lr=3.54e-04 grad_norm=0.5437 tok/s=1,667,861 raw_tok/s=1,669,478 step_tokens=261,890 waste=0.1% h100_mfu=40.62% vram=401/33,922MB peak=32,815/33,922MB
[step 109/4120] loss=2.6657 lr=3.58e-05 muon_lr=3.58e-04 grad_norm=0.5673 tok/s=1,507,873 raw_tok/s=1,508,696 step_tokens=262,001 waste=0.1% h100_mfu=36.71% vram=401/33,922MB peak=32,814/33,922MB
[step 110/4120] loss=2.6717 lr=3.61e-05 muon_lr=3.61e-04 grad_norm=0.8341 tok/s=1,582,907 raw_tok/s=1,587,492 step_tokens=261,387 waste=0.3% h100_mfu=38.62% vram=401/33,922MB peak=32,814/33,922MB
[step 111/4120] loss=2.6783 lr=3.64e-05 muon_lr=3.64e-04 grad_norm=0.8224 tok/s=1,673,621 raw_tok/s=1,675,232 step_tokens=261,892 waste=0.1% h100_mfu=40.76% vram=401/33,922MB peak=32,814/33,922MB
[step 112/4120] loss=2.6681 lr=3.68e-05 muon_lr=3.68e-04 grad_norm=0.9205 tok/s=1,641,412 raw_tok/s=1,645,567 step_tokens=261,482 waste=0.3% h100_mfu=40.04% vram=401/33,922MB peak=32,815/33,922MB
[step 113/4120] loss=2.6753 lr=3.71e-05 muon_lr=3.71e-04 grad_norm=0.9915 tok/s=1,574,113 raw_tok/s=1,574,551 step_tokens=262,071 waste=0.0% h100_mfu=38.31% vram=401/33,922MB peak=32,814/33,922MB
[step 114/4120] loss=2.6718 lr=3.74e-05 muon_lr=3.74e-04 grad_norm=0.7511 tok/s=1,653,476 raw_tok/s=1,654,555 step_tokens=261,973 waste=0.1% h100_mfu=40.26% vram=401/33,922MB peak=32,814/33,922MB
[step 115/4120] loss=2.6773 lr=3.77e-05 muon_lr=3.77e-04 grad_norm=0.8884 tok/s=1,657,728 raw_tok/s=1,660,020 step_tokens=261,782 waste=0.1% h100_mfu=40.39% vram=401/33,922MB peak=32,814/33,922MB
[step 116/4120] loss=2.6626 lr=3.81e-05 muon_lr=3.81e-04 grad_norm=1.1590 tok/s=1,544,124 raw_tok/s=1,546,076 step_tokens=261,813 waste=0.1% h100_mfu=37.62% vram=401/33,922MB peak=32,815/33,922MB
[step 117/4120] loss=2.6671 lr=3.84e-05 muon_lr=3.84e-04 grad_norm=0.8162 tok/s=1,663,125 raw_tok/s=1,664,935 step_tokens=261,859 waste=0.1% h100_mfu=40.51% vram=401/33,922MB peak=32,814/33,922MB
[step 118/4120] loss=2.6665 lr=3.87e-05 muon_lr=3.87e-04 grad_norm=0.7471 tok/s=1,581,345 raw_tok/s=1,583,683 step_tokens=261,757 waste=0.1% h100_mfu=38.53% vram=401/33,922MB peak=32,814/33,922MB
[step 119/4120] loss=2.6627 lr=3.91e-05 muon_lr=3.91e-04 grad_norm=0.9830 tok/s=1,551,976 raw_tok/s=1,553,345 step_tokens=261,913 waste=0.1% h100_mfu=37.79% vram=401/33,922MB peak=32,814/33,922MB
[step 120/4120] loss=2.6687 lr=3.94e-05 muon_lr=3.94e-04 grad_norm=0.8924 tok/s=1,666,510 raw_tok/s=1,669,548 step_tokens=261,667 waste=0.2% h100_mfu=40.62% vram=401/33,922MB peak=32,815/33,922MB
[step 121/4120] loss=2.6695 lr=3.97e-05 muon_lr=3.97e-04 grad_norm=0.8783 tok/s=1,546,823 raw_tok/s=1,550,389 step_tokens=261,541 waste=0.2% h100_mfu=37.72% vram=401/33,922MB peak=32,814/33,922MB
[step 122/4120] loss=2.6697 lr=4.01e-05 muon_lr=4.01e-04 grad_norm=0.8796 tok/s=1,583,750 raw_tok/s=1,586,359 step_tokens=261,713 waste=0.2% h100_mfu=38.60% vram=401/33,922MB peak=32,814/33,922MB
[step 123/4120] loss=2.6626 lr=4.04e-05 muon_lr=4.04e-04 grad_norm=0.8527 tok/s=1,587,468 raw_tok/s=1,589,063 step_tokens=261,881 waste=0.1% h100_mfu=38.66% vram=401/33,922MB peak=32,814/33,922MB
[step 124/4120] loss=2.6666 lr=4.07e-05 muon_lr=4.07e-04 grad_norm=0.7429 tok/s=1,559,904 raw_tok/s=1,562,061 step_tokens=261,782 waste=0.1% h100_mfu=38.01% vram=401/33,922MB peak=32,815/33,922MB
[step 125/4120] loss=2.6615 lr=4.11e-05 muon_lr=4.11e-04 grad_norm=0.8069 tok/s=1,575,392 raw_tok/s=1,577,209 step_tokens=261,842 waste=0.1% h100_mfu=38.37% vram=401/33,922MB peak=32,814/33,922MB
[step 126/4120] loss=2.6697 lr=4.14e-05 muon_lr=4.14e-04 grad_norm=0.9984 tok/s=1,586,203 raw_tok/s=1,588,087 step_tokens=261,833 waste=0.1% h100_mfu=38.64% vram=401/33,922MB peak=32,814/33,922MB
[step 127/4120] loss=2.6632 lr=4.17e-05 muon_lr=4.17e-04 grad_norm=0.8734 tok/s=1,570,906 raw_tok/s=1,574,094 step_tokens=261,613 waste=0.2% h100_mfu=38.30% vram=401/33,922MB peak=32,814/33,922MB
[step 128/4120] loss=2.6588 lr=4.21e-05 muon_lr=4.21e-04 grad_norm=0.9937 tok/s=1,588,046 raw_tok/s=1,590,048 step_tokens=261,814 waste=0.1% h100_mfu=38.69% vram=401/33,922MB peak=32,815/33,922MB
[step 129/4120] loss=2.6649 lr=4.24e-05 muon_lr=4.24e-04 grad_norm=0.9320 tok/s=1,662,759 raw_tok/s=1,666,644 step_tokens=261,533 waste=0.2% h100_mfu=40.55% vram=401/33,922MB peak=32,814/33,922MB
[step 130/4120] loss=2.6608 lr=4.27e-05 muon_lr=4.27e-04 grad_norm=0.8487 tok/s=1,580,879 raw_tok/s=1,584,088 step_tokens=261,613 waste=0.2% h100_mfu=38.54% vram=401/33,922MB peak=32,814/33,922MB
[step 131/4120] loss=2.6624 lr=4.30e-05 muon_lr=4.30e-04 grad_norm=0.8804 tok/s=1,613,826 raw_tok/s=1,615,379 step_tokens=261,892 waste=0.1% h100_mfu=39.30% vram=401/33,922MB peak=32,814/33,922MB
[step 132/4120] loss=2.6661 lr=4.34e-05 muon_lr=4.34e-04 grad_norm=0.7830 tok/s=1,581,745 raw_tok/s=1,582,941 step_tokens=261,946 waste=0.1% h100_mfu=38.51% vram=401/33,922MB peak=32,815/33,922MB
[step 133/4120] loss=2.6616 lr=4.37e-05 muon_lr=4.37e-04 grad_norm=0.7931 tok/s=1,590,838 raw_tok/s=1,591,883 step_tokens=261,972 waste=0.1% h100_mfu=38.73% vram=401/33,922MB peak=32,814/33,922MB
[step 134/4120] loss=2.6592 lr=4.40e-05 muon_lr=4.40e-04 grad_norm=0.9072 tok/s=1,670,185 raw_tok/s=1,672,264 step_tokens=261,818 waste=0.1% h100_mfu=40.69% vram=401/33,922MB peak=32,814/33,922MB
[step 135/4120] loss=2.6616 lr=4.44e-05 muon_lr=4.44e-04 grad_norm=0.9124 tok/s=1,669,536 raw_tok/s=1,671,449 step_tokens=261,844 waste=0.1% h100_mfu=40.67% vram=401/33,922MB peak=32,814/33,922MB
[step 136/4120] loss=2.6597 lr=4.47e-05 muon_lr=4.47e-04 grad_norm=0.7743 tok/s=1,670,861 raw_tok/s=1,671,511 step_tokens=262,042 waste=0.0% h100_mfu=40.67% vram=401/33,922MB peak=32,815/33,922MB
[step 137/4120] loss=2.6629 lr=4.50e-05 muon_lr=4.50e-04 grad_norm=0.8792 tok/s=1,569,545 raw_tok/s=1,572,845 step_tokens=261,594 waste=0.2% h100_mfu=38.27% vram=401/33,922MB peak=32,814/33,922MB
[step 138/4120] loss=2.6598 lr=4.54e-05 muon_lr=4.54e-04 grad_norm=0.8889 tok/s=1,588,349 raw_tok/s=1,590,005 step_tokens=261,871 waste=0.1% h100_mfu=38.68% vram=401/33,922MB peak=32,814/33,922MB
[step 139/4120] loss=2.6593 lr=4.57e-05 muon_lr=4.57e-04 grad_norm=0.9554 tok/s=1,664,008 raw_tok/s=1,666,367 step_tokens=261,773 waste=0.1% h100_mfu=40.54% vram=401/33,922MB peak=32,814/33,922MB
[step 140/4120] loss=2.6683 lr=4.60e-05 muon_lr=4.60e-04 grad_norm=1.0121 tok/s=1,646,930 raw_tok/s=1,650,153 step_tokens=261,632 waste=0.2% h100_mfu=40.15% vram=401/33,922MB peak=32,815/33,922MB
[step 141/4120] loss=2.6638 lr=4.64e-05 muon_lr=4.64e-04 grad_norm=0.9358 tok/s=1,582,207 raw_tok/s=1,584,219 step_tokens=261,811 waste=0.1% h100_mfu=38.54% vram=401/33,922MB peak=32,814/33,922MB
[step 142/4120] loss=2.6538 lr=4.67e-05 muon_lr=4.67e-04 grad_norm=0.8712 tok/s=1,581,613 raw_tok/s=1,583,189 step_tokens=261,883 waste=0.1% h100_mfu=38.52% vram=401/33,922MB peak=32,814/33,922MB
[step 143/4120] loss=2.6689 lr=4.70e-05 muon_lr=4.70e-04 grad_norm=0.9060 tok/s=1,541,844 raw_tok/s=1,542,603 step_tokens=262,015 waste=0.0% h100_mfu=37.53% vram=401/33,922MB peak=32,814/33,922MB
[step 144/4120] loss=2.6670 lr=4.74e-05 muon_lr=4.74e-04 grad_norm=0.8477 tok/s=1,590,997 raw_tok/s=1,591,549 step_tokens=262,053 waste=0.0% h100_mfu=38.72% vram=401/33,922MB peak=32,815/33,922MB
[step 145/4120] loss=2.6624 lr=4.77e-05 muon_lr=4.77e-04 grad_norm=0.7713 tok/s=1,664,617 raw_tok/s=1,665,328 step_tokens=262,032 waste=0.0% h100_mfu=40.52% vram=401/33,922MB peak=32,814/33,922MB
[step 146/4120] loss=2.6561 lr=4.80e-05 muon_lr=4.80e-04 grad_norm=0.7500 tok/s=1,536,603 raw_tok/s=1,538,728 step_tokens=261,782 waste=0.1% h100_mfu=37.44% vram=401/33,922MB peak=32,814/33,922MB
[step 147/4120] loss=2.6697 lr=4.83e-05 muon_lr=4.83e-04 grad_norm=0.6896 tok/s=1,526,972 raw_tok/s=1,528,080 step_tokens=261,954 waste=0.1% h100_mfu=37.18% vram=401/33,922MB peak=32,814/33,922MB
[step 148/4120] loss=2.6593 lr=4.87e-05 muon_lr=4.87e-04 grad_norm=0.7927 tok/s=1,644,882 raw_tok/s=1,645,239 step_tokens=262,087 waste=0.0% h100_mfu=40.03% vram=401/33,922MB peak=32,815/33,922MB
[step 149/4120] loss=2.6564 lr=4.90e-05 muon_lr=4.90e-04 grad_norm=0.7402 tok/s=1,577,262 raw_tok/s=1,578,791 step_tokens=261,890 waste=0.1% h100_mfu=38.41% vram=401/33,922MB peak=32,814/33,922MB
[step 150/4120] loss=2.6593 lr=4.93e-05 muon_lr=4.93e-04 grad_norm=1.0047 tok/s=1,585,337 raw_tok/s=1,586,262 step_tokens=261,991 waste=0.1% h100_mfu=38.59% vram=401/33,922MB peak=32,814/33,922MB
[step 151/4120] loss=2.6518 lr=4.97e-05 muon_lr=4.97e-04 grad_norm=1.0806 tok/s=1,579,775 raw_tok/s=1,582,298 step_tokens=261,726 waste=0.2% h100_mfu=38.50% vram=401/33,922MB peak=32,814/33,922MB
[step 152/4120] loss=2.6556 lr=5.00e-05 muon_lr=5.00e-04 grad_norm=0.8014 tok/s=1,661,539 raw_tok/s=1,663,557 step_tokens=261,826 waste=0.1% h100_mfu=40.47% vram=401/33,922MB peak=32,815/33,922MB
[step 153/4120] loss=2.6505 lr=5.03e-05 muon_lr=5.03e-04 grad_norm=0.9575 tok/s=1,662,630 raw_tok/s=1,663,734 step_tokens=261,970 waste=0.1% h100_mfu=40.48% vram=401/33,922MB peak=32,814/33,922MB
[step 154/4120] loss=2.6551 lr=5.07e-05 muon_lr=5.07e-04 grad_norm=0.8463 tok/s=1,652,698 raw_tok/s=1,656,508 step_tokens=261,541 waste=0.2% h100_mfu=40.30% vram=401/33,922MB peak=32,814/33,922MB
[step 155/4120] loss=2.6486 lr=5.10e-05 muon_lr=5.10e-04 grad_norm=0.8492 tok/s=1,575,576 raw_tok/s=1,577,442 step_tokens=261,834 waste=0.1% h100_mfu=38.38% vram=401/33,922MB peak=32,814/33,922MB
[step 156/4120] loss=2.6670 lr=5.13e-05 muon_lr=5.13e-04 grad_norm=0.9725 tok/s=1,514,050 raw_tok/s=1,514,194 step_tokens=262,119 waste=0.0% h100_mfu=36.84% vram=401/33,922MB peak=32,815/33,922MB
[step 157/4120] loss=2.6562 lr=5.17e-05 muon_lr=5.17e-04 grad_norm=0.8647 tok/s=1,589,405 raw_tok/s=1,592,211 step_tokens=261,682 waste=0.2% h100_mfu=38.74% vram=401/33,922MB peak=32,814/33,922MB
[step 158/4120] loss=2.6565 lr=5.20e-05 muon_lr=5.20e-04 grad_norm=0.7493 tok/s=1,581,272 raw_tok/s=1,581,586 step_tokens=262,092 waste=0.0% h100_mfu=38.48% vram=401/33,922MB peak=32,814/33,922MB
[step 159/4120] loss=2.6540 lr=5.23e-05 muon_lr=5.23e-04 grad_norm=0.6814 tok/s=1,482,952 raw_tok/s=1,486,024 step_tokens=261,602 waste=0.2% h100_mfu=36.16% vram=401/33,922MB peak=32,814/33,922MB
[step 160/4120] loss=2.6627 lr=5.26e-05 muon_lr=5.26e-04 grad_norm=0.7231 tok/s=1,572,012 raw_tok/s=1,573,483 step_tokens=261,899 waste=0.1% h100_mfu=38.28% vram=401/33,922MB peak=32,815/33,922MB
[step 161/4120] loss=2.6491 lr=5.30e-05 muon_lr=5.30e-04 grad_norm=0.6048 tok/s=1,653,743 raw_tok/s=1,656,067 step_tokens=261,776 waste=0.1% h100_mfu=40.29% vram=401/33,922MB peak=32,814/33,922MB
[step 162/4120] loss=2.6555 lr=5.33e-05 muon_lr=5.33e-04 grad_norm=0.5318 tok/s=1,614,913 raw_tok/s=1,617,208 step_tokens=261,772 waste=0.1% h100_mfu=39.35% vram=401/33,922MB peak=32,814/33,922MB
[step 163/4120] loss=2.6514 lr=5.36e-05 muon_lr=5.36e-04 grad_norm=0.6607 tok/s=1,663,864 raw_tok/s=1,666,133 step_tokens=261,787 waste=0.1% h100_mfu=40.54% vram=401/33,922MB peak=32,814/33,922MB
[step 164/4120] loss=2.6579 lr=5.40e-05 muon_lr=5.40e-04 grad_norm=0.6594 tok/s=1,560,313 raw_tok/s=1,560,837 step_tokens=262,056 waste=0.0% h100_mfu=37.98% vram=401/33,922MB peak=32,815/33,922MB
[step 165/4120] loss=2.6462 lr=5.43e-05 muon_lr=5.43e-04 grad_norm=0.6952 tok/s=1,662,567 raw_tok/s=1,667,625 step_tokens=261,349 waste=0.3% h100_mfu=40.57% vram=401/33,922MB peak=32,814/33,922MB
[step 166/4120] loss=2.6459 lr=5.46e-05 muon_lr=5.46e-04 grad_norm=0.6779 tok/s=1,573,894 raw_tok/s=1,574,447 step_tokens=262,052 waste=0.0% h100_mfu=38.31% vram=401/33,922MB peak=32,814/33,922MB
[step 167/4120] loss=2.6450 lr=5.50e-05 muon_lr=5.50e-04 grad_norm=0.6410 tok/s=1,581,349 raw_tok/s=1,582,351 step_tokens=261,978 waste=0.1% h100_mfu=38.50% vram=401/33,922MB peak=32,814/33,922MB
[step 168/4120] loss=2.6605 lr=5.53e-05 muon_lr=5.53e-04 grad_norm=0.6522 tok/s=1,655,942 raw_tok/s=1,660,141 step_tokens=261,481 waste=0.3% h100_mfu=40.39% vram=401/33,922MB peak=32,815/33,922MB
[step 169/4120] loss=2.6469 lr=5.56e-05 muon_lr=5.56e-04 grad_norm=0.6329 tok/s=1,578,629 raw_tok/s=1,580,534 step_tokens=261,828 waste=0.1% h100_mfu=38.45% vram=401/33,922MB peak=32,814/33,922MB
[step 170/4120] loss=2.6407 lr=5.60e-05 muon_lr=5.60e-04 grad_norm=0.5533 tok/s=1,571,756 raw_tok/s=1,574,326 step_tokens=261,716 waste=0.2% h100_mfu=38.30% vram=401/33,922MB peak=32,814/33,922MB
[step 171/4120] loss=2.6512 lr=5.63e-05 muon_lr=5.63e-04 grad_norm=0.6500 tok/s=1,657,214 raw_tok/s=1,661,366 step_tokens=261,489 waste=0.2% h100_mfu=40.42% vram=401/33,922MB peak=32,814/33,922MB
[step 172/4120] loss=2.6400 lr=5.66e-05 muon_lr=5.66e-04 grad_norm=0.7639 tok/s=1,560,112 raw_tok/s=1,561,112 step_tokens=261,976 waste=0.1% h100_mfu=37.98% vram=401/33,922MB peak=32,815/33,922MB
[step 173/4120] loss=2.6410 lr=5.70e-05 muon_lr=5.70e-04 grad_norm=0.6742 tok/s=1,532,012 raw_tok/s=1,532,603 step_tokens=262,043 waste=0.0% h100_mfu=37.29% vram=401/33,922MB peak=32,814/33,922MB
[step 174/4120] loss=2.6473 lr=5.73e-05 muon_lr=5.73e-04 grad_norm=0.6542 tok/s=1,584,556 raw_tok/s=1,586,904 step_tokens=261,756 waste=0.1% h100_mfu=38.61% vram=401/33,922MB peak=32,814/33,922MB
[step 175/4120] loss=2.6456 lr=5.76e-05 muon_lr=5.76e-04 grad_norm=0.6881 tok/s=1,585,706 raw_tok/s=1,586,941 step_tokens=261,940 waste=0.1% h100_mfu=38.61% vram=401/33,922MB peak=32,814/33,922MB
[step 176/4120] loss=2.6467 lr=5.79e-05 muon_lr=5.79e-04 grad_norm=0.6074 tok/s=1,670,739 raw_tok/s=1,672,698 step_tokens=261,837 waste=0.1% h100_mfu=40.70% vram=401/33,922MB peak=32,815/33,922MB
[step 177/4120] loss=2.6387 lr=5.83e-05 muon_lr=5.83e-04 grad_norm=0.7075 tok/s=1,664,897 raw_tok/s=1,666,391 step_tokens=261,909 waste=0.1% h100_mfu=40.54% vram=401/33,922MB peak=32,814/33,922MB
[step 178/4120] loss=2.6424 lr=5.86e-05 muon_lr=5.86e-04 grad_norm=0.7141 tok/s=1,633,063 raw_tok/s=1,635,190 step_tokens=261,803 waste=0.1% h100_mfu=39.78% vram=401/33,922MB peak=32,814/33,922MB
[step 179/4120] loss=2.6444 lr=5.89e-05 muon_lr=5.89e-04 grad_norm=0.7148 tok/s=1,504,211 raw_tok/s=1,505,417 step_tokens=261,934 waste=0.1% h100_mfu=36.63% vram=401/33,922MB peak=32,814/33,922MB
[step 180/4120] loss=2.6329 lr=5.93e-05 muon_lr=5.93e-04 grad_norm=0.5578 tok/s=1,539,698 raw_tok/s=1,543,124 step_tokens=261,562 waste=0.2% h100_mfu=37.54% vram=401/33,922MB peak=32,815/33,922MB
[step 181/4120] loss=2.6415 lr=5.96e-05 muon_lr=5.96e-04 grad_norm=0.5224 tok/s=1,577,244 raw_tok/s=1,579,352 step_tokens=261,794 waste=0.1% h100_mfu=38.43% vram=401/33,922MB peak=32,814/33,922MB
[step 182/4120] loss=2.6479 lr=5.99e-05 muon_lr=5.99e-04 grad_norm=0.6505 tok/s=1,572,461 raw_tok/s=1,574,606 step_tokens=261,787 waste=0.1% h100_mfu=38.31% vram=401/33,922MB peak=32,814/33,922MB
[step 183/4120] loss=2.6402 lr=6.03e-05 muon_lr=6.03e-04 grad_norm=0.6250 tok/s=1,530,043 raw_tok/s=1,530,902 step_tokens=261,997 waste=0.1% h100_mfu=37.25% vram=401/33,922MB peak=32,814/33,922MB
[step 184/4120] loss=2.6385 lr=6.06e-05 muon_lr=6.06e-04 grad_norm=0.6146 tok/s=1,506,730 raw_tok/s=1,507,576 step_tokens=261,997 waste=0.1% h100_mfu=36.68% vram=401/33,922MB peak=32,815/33,922MB
[step 185/4120] loss=2.6444 lr=6.09e-05 muon_lr=6.09e-04 grad_norm=0.6587 tok/s=1,670,324 raw_tok/s=1,672,403 step_tokens=261,818 waste=0.1% h100_mfu=40.69% vram=401/33,922MB peak=32,814/33,922MB
[step 186/4120] loss=2.6364 lr=6.13e-05 muon_lr=6.13e-04 grad_norm=0.7105 tok/s=1,640,318 raw_tok/s=1,641,652 step_tokens=261,931 waste=0.1% h100_mfu=39.94% vram=401/33,922MB peak=32,814/33,922MB
[step 187/4120] loss=2.6323 lr=6.16e-05 muon_lr=6.16e-04 grad_norm=0.6346 tok/s=1,575,904 raw_tok/s=1,577,300 step_tokens=261,912 waste=0.1% h100_mfu=38.38% vram=401/33,922MB peak=32,814/33,922MB
[step 188/4120] loss=2.6317 lr=6.19e-05 muon_lr=6.19e-04 grad_norm=0.6374 tok/s=1,586,527 raw_tok/s=1,588,174 step_tokens=261,872 waste=0.1% h100_mfu=38.64% vram=401/33,922MB peak=32,815/33,922MB
[step 189/4120] loss=2.6363 lr=6.23e-05 muon_lr=6.23e-04 grad_norm=0.5994 tok/s=1,580,980 raw_tok/s=1,582,906 step_tokens=261,825 waste=0.1% h100_mfu=38.51% vram=401/33,922MB peak=32,814/33,922MB
[step 190/4120] loss=2.6419 lr=6.26e-05 muon_lr=6.26e-04 grad_norm=0.6090 tok/s=1,672,637 raw_tok/s=1,673,722 step_tokens=261,974 waste=0.1% h100_mfu=40.72% vram=401/33,922MB peak=32,814/33,922MB
[step 191/4120] loss=2.6449 lr=6.29e-05 muon_lr=6.29e-04 grad_norm=0.5621 tok/s=1,585,439 raw_tok/s=1,587,516 step_tokens=261,801 waste=0.1% h100_mfu=38.62% vram=401/33,922MB peak=32,814/33,922MB
[step 192/4120] loss=2.6309 lr=6.32e-05 muon_lr=6.32e-04 grad_norm=0.4955 tok/s=1,593,125 raw_tok/s=1,593,490 step_tokens=262,084 waste=0.0% h100_mfu=38.77% vram=401/33,922MB peak=32,815/33,922MB
[step 193/4120] loss=2.6406 lr=6.36e-05 muon_lr=6.36e-04 grad_norm=0.4902 tok/s=1,512,850 raw_tok/s=1,514,352 step_tokens=261,884 waste=0.1% h100_mfu=36.84% vram=401/33,922MB peak=32,814/33,922MB
[step 194/4120] loss=2.6334 lr=6.39e-05 muon_lr=6.39e-04 grad_norm=0.5749 tok/s=1,584,502 raw_tok/s=1,586,420 step_tokens=261,827 waste=0.1% h100_mfu=38.60% vram=401/33,922MB peak=32,814/33,922MB
[step 195/4120] loss=2.6344 lr=6.42e-05 muon_lr=6.42e-04 grad_norm=0.4886 tok/s=1,667,966 raw_tok/s=1,668,654 step_tokens=262,036 waste=0.0% h100_mfu=40.60% vram=401/33,922MB peak=32,814/33,922MB
[step 196/4120] loss=2.6370 lr=6.46e-05 muon_lr=6.46e-04 grad_norm=0.5651 tok/s=1,493,268 raw_tok/s=1,496,442 step_tokens=261,588 waste=0.2% h100_mfu=36.41% vram=401/33,922MB peak=32,815/33,922MB
[step 197/4120] loss=2.6274 lr=6.49e-05 muon_lr=6.49e-04 grad_norm=0.5977 tok/s=1,645,623 raw_tok/s=1,648,730 step_tokens=261,650 waste=0.2% h100_mfu=40.11% vram=401/33,922MB peak=32,814/33,922MB
[step 198/4120] loss=2.6336 lr=6.52e-05 muon_lr=6.52e-04 grad_norm=0.6387 tok/s=1,675,112 raw_tok/s=1,676,884 step_tokens=261,867 waste=0.1% h100_mfu=40.80% vram=401/33,922MB peak=32,814/33,922MB
[step 199/4120] loss=2.6427 lr=6.56e-05 muon_lr=6.56e-04 grad_norm=0.6790 tok/s=1,633,899 raw_tok/s=1,636,715 step_tokens=261,693 waste=0.2% h100_mfu=39.82% vram=401/33,922MB peak=32,814/33,922MB
[step 200/4120] loss=2.6312 lr=6.59e-05 muon_lr=6.59e-04 grad_norm=0.6153 tok/s=1,665,324 raw_tok/s=1,667,570 step_tokens=261,791 waste=0.1% h100_mfu=40.57% vram=401/33,922MB peak=32,815/33,922MB
[step 201/4120] loss=2.6313 lr=6.62e-05 muon_lr=6.62e-04 grad_norm=0.5383 tok/s=1,574,522 raw_tok/s=1,575,604 step_tokens=261,964 waste=0.1% h100_mfu=38.33% vram=401/33,922MB peak=32,814/33,922MB
[step 202/4120] loss=2.6319 lr=6.66e-05 muon_lr=6.66e-04 grad_norm=0.5774 tok/s=1,602,488 raw_tok/s=1,605,894 step_tokens=261,588 waste=0.2% h100_mfu=39.07% vram=401/33,922MB peak=32,814/33,922MB
[step 203/4120] loss=2.6347 lr=6.69e-05 muon_lr=6.69e-04 grad_norm=0.6182 tok/s=1,585,316 raw_tok/s=1,586,018 step_tokens=262,028 waste=0.0% h100_mfu=38.59% vram=401/33,922MB peak=32,814/33,922MB
[step 204/4120] loss=2.6361 lr=6.72e-05 muon_lr=6.72e-04 grad_norm=0.5146 tok/s=1,573,819 raw_tok/s=1,575,177 step_tokens=261,918 waste=0.1% h100_mfu=38.32% vram=401/33,922MB peak=32,815/33,922MB
[step 205/4120] loss=2.6352 lr=6.75e-05 muon_lr=6.75e-04 grad_norm=0.5299 tok/s=1,477,416 raw_tok/s=1,480,047 step_tokens=261,678 waste=0.2% h100_mfu=36.01% vram=401/33,922MB peak=32,814/33,922MB
[step 206/4120] loss=2.6402 lr=6.79e-05 muon_lr=6.79e-04 grad_norm=0.6116 tok/s=1,515,796 raw_tok/s=1,518,211 step_tokens=261,727 waste=0.2% h100_mfu=36.94% vram=401/33,922MB peak=32,814/33,922MB
[step 207/4120] loss=2.6305 lr=6.82e-05 muon_lr=6.82e-04 grad_norm=0.5676 tok/s=1,662,282 raw_tok/s=1,664,842 step_tokens=261,741 waste=0.2% h100_mfu=40.51% vram=401/33,922MB peak=32,814/33,922MB
[step 208/4120] loss=2.6263 lr=6.85e-05 muon_lr=6.85e-04 grad_norm=0.5187 tok/s=1,579,942 raw_tok/s=1,582,792 step_tokens=261,672 waste=0.2% h100_mfu=38.51% vram=401/33,922MB peak=32,815/33,922MB
[step 209/4120] loss=2.6403 lr=6.89e-05 muon_lr=6.89e-04 grad_norm=0.4414 tok/s=1,623,629 raw_tok/s=1,624,584 step_tokens=261,990 waste=0.1% h100_mfu=39.53% vram=401/33,922MB peak=32,814/33,922MB
[step 210/4120] loss=2.6376 lr=6.92e-05 muon_lr=6.92e-04 grad_norm=0.5320 tok/s=1,656,690 raw_tok/s=1,658,575 step_tokens=261,846 waste=0.1% h100_mfu=40.35% vram=401/33,922MB peak=32,814/33,922MB
[step 211/4120] loss=2.6341 lr=6.95e-05 muon_lr=6.95e-04 grad_norm=0.5086 tok/s=1,577,449 raw_tok/s=1,578,871 step_tokens=261,908 waste=0.1% h100_mfu=38.41% vram=401/33,922MB peak=32,814/33,922MB
[step 212/4120] loss=2.6341 lr=6.99e-05 muon_lr=6.99e-04 grad_norm=0.5937 tok/s=1,580,586 raw_tok/s=1,581,298 step_tokens=262,026 waste=0.0% h100_mfu=38.47% vram=401/33,922MB peak=32,815/33,922MB
[step 213/4120] loss=2.6268 lr=7.02e-05 muon_lr=7.02e-04 grad_norm=0.5469 tok/s=1,583,169 raw_tok/s=1,585,067 step_tokens=261,830 waste=0.1% h100_mfu=38.56% vram=401/33,922MB peak=32,814/33,922MB
[step 214/4120] loss=2.6247 lr=7.05e-05 muon_lr=7.05e-04 grad_norm=0.4570 tok/s=1,587,625 raw_tok/s=1,588,183 step_tokens=262,052 waste=0.0% h100_mfu=38.64% vram=401/33,922MB peak=32,814/33,922MB
[step 215/4120] loss=2.6334 lr=7.09e-05 muon_lr=7.09e-04 grad_norm=0.5637 tok/s=1,566,920 raw_tok/s=1,568,535 step_tokens=261,874 waste=0.1% h100_mfu=38.16% vram=401/33,922MB peak=32,814/33,922MB
[step 216/4120] loss=2.6284 lr=7.12e-05 muon_lr=7.12e-04 grad_norm=0.6443 tok/s=1,583,532 raw_tok/s=1,586,934 step_tokens=261,582 waste=0.2% h100_mfu=38.61% vram=401/33,922MB peak=32,815/33,922MB
[step 217/4120] loss=2.6230 lr=7.15e-05 muon_lr=7.15e-04 grad_norm=0.5774 tok/s=1,574,309 raw_tok/s=1,576,492 step_tokens=261,781 waste=0.1% h100_mfu=38.36% vram=401/33,922MB peak=32,814/33,922MB
[step 218/4120] loss=2.6335 lr=7.19e-05 muon_lr=7.19e-04 grad_norm=0.5784 tok/s=1,660,582 raw_tok/s=1,662,314 step_tokens=261,871 waste=0.1% h100_mfu=40.44% vram=401/33,922MB peak=32,814/33,922MB
[step 219/4120] loss=2.6249 lr=7.22e-05 muon_lr=7.22e-04 grad_norm=0.5550 tok/s=1,585,549 raw_tok/s=1,587,736 step_tokens=261,783 waste=0.1% h100_mfu=38.63% vram=401/33,922MB peak=32,814/33,922MB
[step 220/4120] loss=2.6204 lr=7.25e-05 muon_lr=7.25e-04 grad_norm=0.5630 tok/s=1,581,263 raw_tok/s=1,583,474 step_tokens=261,778 waste=0.1% h100_mfu=38.53% vram=401/33,922MB peak=32,815/33,922MB
[step 221/4120] loss=2.6219 lr=7.28e-05 muon_lr=7.28e-04 grad_norm=0.5690 tok/s=1,584,206 raw_tok/s=1,584,605 step_tokens=262,078 waste=0.0% h100_mfu=38.55% vram=401/33,922MB peak=32,814/33,922MB
[step 222/4120] loss=2.6252 lr=7.32e-05 muon_lr=7.32e-04 grad_norm=0.5199 tok/s=1,642,823 raw_tok/s=1,644,548 step_tokens=261,869 waste=0.1% h100_mfu=40.01% vram=401/33,922MB peak=32,814/33,922MB
[step 223/4120] loss=2.6247 lr=7.35e-05 muon_lr=7.35e-04 grad_norm=0.5404 tok/s=1,589,300 raw_tok/s=1,591,005 step_tokens=261,863 waste=0.1% h100_mfu=38.71% vram=401/33,922MB peak=32,814/33,922MB
[step 224/4120] loss=2.6200 lr=7.38e-05 muon_lr=7.38e-04 grad_norm=0.4622 tok/s=1,586,252 raw_tok/s=1,587,815 step_tokens=261,886 waste=0.1% h100_mfu=38.63% vram=401/33,922MB peak=32,815/33,922MB
[step 225/4120] loss=2.6372 lr=7.42e-05 muon_lr=7.42e-04 grad_norm=0.5106 tok/s=1,616,105 raw_tok/s=1,618,970 step_tokens=261,680 waste=0.2% h100_mfu=39.39% vram=401/33,922MB peak=32,814/33,922MB
[step 226/4120] loss=2.6233 lr=7.45e-05 muon_lr=7.45e-04 grad_norm=0.5212 tok/s=1,579,731 raw_tok/s=1,581,094 step_tokens=261,918 waste=0.1% h100_mfu=38.47% vram=401/33,922MB peak=32,814/33,922MB
[step 227/4120] loss=2.6197 lr=7.48e-05 muon_lr=7.48e-04 grad_norm=0.5568 tok/s=1,660,431 raw_tok/s=1,661,743 step_tokens=261,937 waste=0.1% h100_mfu=40.43% vram=401/33,922MB peak=32,814/33,922MB
[step 228/4120] loss=2.6234 lr=7.52e-05 muon_lr=7.52e-04 grad_norm=0.5743 tok/s=1,551,963 raw_tok/s=1,552,413 step_tokens=262,068 waste=0.0% h100_mfu=37.77% vram=401/33,922MB peak=32,815/33,922MB
[step 229/4120] loss=2.6415 lr=7.55e-05 muon_lr=7.55e-04 grad_norm=0.5658 tok/s=1,670,676 raw_tok/s=1,672,859 step_tokens=261,802 waste=0.1% h100_mfu=40.70% vram=401/33,922MB peak=32,814/33,922MB
[step 230/4120] loss=2.6264 lr=7.58e-05 muon_lr=7.58e-04 grad_norm=0.4521 tok/s=1,577,899 raw_tok/s=1,579,713 step_tokens=261,843 waste=0.1% h100_mfu=38.43% vram=401/33,922MB peak=32,814/33,922MB
[step 231/4120] loss=2.6184 lr=7.62e-05 muon_lr=7.62e-04 grad_norm=0.4160 tok/s=1,556,025 raw_tok/s=1,557,837 step_tokens=261,839 waste=0.1% h100_mfu=37.90% vram=401/33,922MB peak=32,814/33,922MB
[step 232/4120] loss=2.6271 lr=7.65e-05 muon_lr=7.65e-04 grad_norm=0.5142 tok/s=1,558,685 raw_tok/s=1,560,822 step_tokens=261,785 waste=0.1% h100_mfu=37.97% vram=401/33,922MB peak=32,815/33,922MB
[step 233/4120] loss=2.6242 lr=7.68e-05 muon_lr=7.68e-04 grad_norm=0.5062 tok/s=1,673,335 raw_tok/s=1,676,924 step_tokens=261,583 waste=0.2% h100_mfu=40.80% vram=401/33,922MB peak=32,814/33,922MB
[step 234/4120] loss=2.6256 lr=7.72e-05 muon_lr=7.72e-04 grad_norm=0.4482 tok/s=1,548,239 raw_tok/s=1,550,865 step_tokens=261,700 waste=0.2% h100_mfu=37.73% vram=401/33,922MB peak=32,814/33,922MB
[step 235/4120] loss=2.6325 lr=7.75e-05 muon_lr=7.75e-04 grad_norm=0.4928 tok/s=1,533,757 raw_tok/s=1,536,219 step_tokens=261,724 waste=0.2% h100_mfu=37.38% vram=401/33,922MB peak=32,814/33,922MB
[step 236/4120] loss=2.6289 lr=7.78e-05 muon_lr=7.78e-04 grad_norm=0.5201 tok/s=1,577,902 raw_tok/s=1,581,963 step_tokens=261,471 waste=0.3% h100_mfu=38.49% vram=401/33,922MB peak=32,815/33,922MB
[step 237/4120] loss=2.6246 lr=7.81e-05 muon_lr=7.81e-04 grad_norm=0.6042 tok/s=1,654,382 raw_tok/s=1,658,139 step_tokens=261,550 waste=0.2% h100_mfu=40.34% vram=401/33,922MB peak=32,814/33,922MB
[step 238/4120] loss=2.6169 lr=7.85e-05 muon_lr=7.85e-04 grad_norm=0.6033 tok/s=1,558,566 raw_tok/s=1,558,822 step_tokens=262,101 waste=0.0% h100_mfu=37.93% vram=401/33,922MB peak=32,814/33,922MB
[step 239/4120] loss=2.6233 lr=7.88e-05 muon_lr=7.88e-04 grad_norm=0.5787 tok/s=1,648,821 raw_tok/s=1,652,951 step_tokens=261,489 waste=0.2% h100_mfu=40.22% vram=401/33,922MB peak=32,814/33,922MB
[step 240/4120] loss=2.6194 lr=7.91e-05 muon_lr=7.91e-04 grad_norm=0.6490 tok/s=1,582,650 raw_tok/s=1,584,100 step_tokens=261,904 waste=0.1% h100_mfu=38.54% vram=401/33,922MB peak=32,815/33,922MB
[step 241/4120] loss=2.6218 lr=7.95e-05 muon_lr=7.95e-04 grad_norm=0.5448 tok/s=1,589,565 raw_tok/s=1,590,214 step_tokens=262,037 waste=0.0% h100_mfu=38.69% vram=401/33,922MB peak=32,814/33,922MB
[step 242/4120] loss=2.6346 lr=7.98e-05 muon_lr=7.98e-04 grad_norm=0.5635 tok/s=1,667,158 raw_tok/s=1,668,935 step_tokens=261,865 waste=0.1% h100_mfu=40.61% vram=401/33,922MB peak=32,814/33,922MB
[step 243/4120] loss=2.6269 lr=8.01e-05 muon_lr=8.01e-04 grad_norm=0.5691 tok/s=1,577,421 raw_tok/s=1,580,435 step_tokens=261,644 waste=0.2% h100_mfu=38.45% vram=401/33,922MB peak=32,814/33,922MB
[step 244/4120] loss=2.6277 lr=8.05e-05 muon_lr=8.05e-04 grad_norm=0.5391 tok/s=1,613,096 raw_tok/s=1,614,691 step_tokens=261,885 waste=0.1% h100_mfu=39.29% vram=401/33,922MB peak=32,815/33,922MB
[step 245/4120] loss=2.6123 lr=8.08e-05 muon_lr=8.08e-04 grad_norm=0.5313 tok/s=1,588,845 raw_tok/s=1,590,902 step_tokens=261,805 waste=0.1% h100_mfu=38.71% vram=401/33,922MB peak=32,814/33,922MB
[step 246/4120] loss=2.6205 lr=8.11e-05 muon_lr=8.11e-04 grad_norm=0.4907 tok/s=1,589,664 raw_tok/s=1,591,321 step_tokens=261,871 waste=0.1% h100_mfu=38.72% vram=401/33,922MB peak=32,814/33,922MB
[step 247/4120] loss=2.6274 lr=8.15e-05 muon_lr=8.15e-04 grad_norm=0.5166 tok/s=1,586,155 raw_tok/s=1,588,354 step_tokens=261,781 waste=0.1% h100_mfu=38.64% vram=401/33,922MB peak=32,814/33,922MB
[step 248/4120] loss=2.6124 lr=8.18e-05 muon_lr=8.18e-04 grad_norm=0.4354 tok/s=1,579,477 raw_tok/s=1,582,701 step_tokens=261,610 waste=0.2% h100_mfu=38.51% vram=401/33,922MB peak=32,815/33,922MB
[step 249/4120] loss=2.6106 lr=8.21e-05 muon_lr=8.21e-04 grad_norm=0.5144 tok/s=1,591,680 raw_tok/s=1,592,525 step_tokens=262,005 waste=0.1% h100_mfu=38.75% vram=401/33,922MB peak=32,814/33,922MB
[step 250/4120] loss=2.6211 lr=8.25e-05 muon_lr=8.25e-04 grad_norm=0.5513 tok/s=1,597,258 raw_tok/s=1,598,099 step_tokens=262,006 waste=0.1% h100_mfu=38.88% vram=401/33,922MB peak=32,814/33,922MB
W0220 02:15:26.355000 26153 torch/distributed/run.py:852] 
W0220 02:15:26.355000 26153 torch/distributed/run.py:852] *****************************************
W0220 02:15:26.355000 26153 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:15:26.355000 26153 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 67650.06it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 665.45it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 63791.70it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 34100.03it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 71415608
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020215
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 71415608
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020215-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=52 tensors, resid_scalar_params=1 tensors, x0_scalar_params=1 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=52 tensors, resid_scalar_params=1 tensors, x0_scalar_params=1 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run r6oq88ap
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_021535-r6oq88ap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020215
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/r6oq88ap
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8250] loss=3.8072 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=8.0881 tok/s=4,275 raw_tok/s=4,293 step_tokens=130,519 waste=0.4% h100_mfu=0.10% vram=396/20,846MB peak=20,111/20,846MB
[step 2/8250] loss=3.7876 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=8.9793 tok/s=1,251,219 raw_tok/s=1,253,916 step_tokens=130,790 waste=0.2% h100_mfu=30.51% vram=396/20,848MB peak=20,250/20,848MB
[step 3/8250] loss=3.7789 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=8.8965 tok/s=1,168,728 raw_tok/s=1,171,562 step_tokens=130,755 waste=0.2% h100_mfu=28.50% vram=396/20,848MB peak=20,251/20,848MB
[step 4/8250] loss=3.7954 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=8.9557 tok/s=1,204,978 raw_tok/s=1,206,441 step_tokens=130,913 waste=0.1% h100_mfu=29.35% vram=396/20,848MB peak=20,251/20,848MB
[step 5/8250] loss=3.7940 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=8.9553 tok/s=1,238,343 raw_tok/s=1,242,809 step_tokens=130,601 waste=0.4% h100_mfu=30.24% vram=396/20,848MB peak=20,251/20,848MB
[step 6/8250] loss=3.7833 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=8.8199 tok/s=1,192,889 raw_tok/s=1,196,193 step_tokens=130,710 waste=0.3% h100_mfu=29.10% vram=396/20,848MB peak=20,251/20,848MB
[step 7/8250] loss=3.7918 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=8.9217 tok/s=1,168,926 raw_tok/s=1,170,864 step_tokens=130,855 waste=0.2% h100_mfu=28.49% vram=396/20,848MB peak=20,251/20,848MB
[step 8/8250] loss=3.7944 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=9.0100 tok/s=1,146,761 raw_tok/s=1,149,699 step_tokens=130,737 waste=0.3% h100_mfu=27.97% vram=396/20,848MB peak=20,251/20,848MB
[step 9/8250] loss=3.7906 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=9.1289 tok/s=1,177,587 raw_tok/s=1,179,766 step_tokens=130,830 waste=0.2% h100_mfu=28.70% vram=396/20,848MB peak=20,251/20,848MB
[step 10/8250] loss=3.7697 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=8.8824 tok/s=1,109,190 raw_tok/s=1,111,106 step_tokens=130,846 waste=0.2% h100_mfu=27.03% vram=396/20,848MB peak=20,251/20,848MB
[step 11/8250] loss=3.7673 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=9.0155 tok/s=1,190,371 raw_tok/s=1,193,549 step_tokens=130,723 waste=0.3% h100_mfu=29.04% vram=396/20,848MB peak=20,251/20,848MB
[step 12/8250] loss=3.7580 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=9.0355 tok/s=1,193,373 raw_tok/s=1,194,841 step_tokens=130,911 waste=0.1% h100_mfu=29.07% vram=396/20,848MB peak=20,251/20,848MB
[step 13/8250] loss=3.7534 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=9.3085 tok/s=1,177,100 raw_tok/s=1,178,989 step_tokens=130,862 waste=0.2% h100_mfu=28.68% vram=396/20,848MB peak=20,251/20,848MB
[step 14/8250] loss=3.7367 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=9.4799 tok/s=1,153,207 raw_tok/s=1,158,262 step_tokens=130,500 waste=0.4% h100_mfu=28.18% vram=396/20,848MB peak=20,251/20,848MB
[step 15/8250] loss=3.7133 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=9.6405 tok/s=1,072,252 raw_tok/s=1,073,989 step_tokens=130,860 waste=0.2% h100_mfu=26.13% vram=396/20,848MB peak=20,251/20,848MB
[step 16/8250] loss=3.6778 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=9.8192 tok/s=1,154,507 raw_tok/s=1,155,124 step_tokens=131,002 waste=0.1% h100_mfu=28.10% vram=396/20,848MB peak=20,251/20,848MB
[step 17/8250] loss=3.6497 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=10.3027 tok/s=1,226,269 raw_tok/s=1,228,584 step_tokens=130,825 waste=0.2% h100_mfu=29.89% vram=396/20,848MB peak=20,251/20,848MB
[step 18/8250] loss=3.5882 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=10.6210 tok/s=1,184,221 raw_tok/s=1,187,701 step_tokens=130,688 waste=0.3% h100_mfu=28.90% vram=396/20,848MB peak=20,251/20,848MB
[step 19/8250] loss=3.5140 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=12.0870 tok/s=1,151,581 raw_tok/s=1,158,457 step_tokens=130,294 waste=0.6% h100_mfu=28.19% vram=396/20,848MB peak=20,251/20,848MB
[step 20/8250] loss=3.4041 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=13.5276 tok/s=1,163,220 raw_tok/s=1,166,076 step_tokens=130,751 waste=0.2% h100_mfu=28.37% vram=396/20,848MB peak=20,251/20,848MB
[step 21/8250] loss=3.2362 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=15.5908 tok/s=1,138,342 raw_tok/s=1,140,954 step_tokens=130,772 waste=0.2% h100_mfu=27.76% vram=396/20,848MB peak=20,251/20,848MB
[step 22/8250] loss=2.9874 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=15.6272 tok/s=1,227,054 raw_tok/s=1,228,957 step_tokens=130,869 waste=0.2% h100_mfu=29.90% vram=396/20,848MB peak=20,251/20,848MB
[step 23/8250] loss=3.1011 lr=7.28e-06 muon_lr=7.28e-05 grad_norm=85.8218 tok/s=1,183,185 raw_tok/s=1,188,580 step_tokens=130,477 waste=0.5% h100_mfu=28.92% vram=396/20,848MB peak=20,251/20,848MB
[step 24/8250] loss=3.0824 lr=7.62e-06 muon_lr=7.62e-05 grad_norm=60.6726 tok/s=1,229,012 raw_tok/s=1,231,700 step_tokens=130,786 waste=0.2% h100_mfu=29.97% vram=396/20,848MB peak=20,251/20,848MB
[step 25/8250] loss=2.8821 lr=7.95e-06 muon_lr=7.95e-05 grad_norm=32.1317 tok/s=1,149,725 raw_tok/s=1,153,907 step_tokens=130,597 waste=0.4% h100_mfu=28.07% vram=396/20,848MB peak=20,251/20,848MB
[step 26/8250] loss=2.8752 lr=8.28e-06 muon_lr=8.28e-05 grad_norm=20.2919 tok/s=1,150,470 raw_tok/s=1,153,868 step_tokens=130,686 waste=0.3% h100_mfu=28.07% vram=396/20,848MB peak=20,251/20,848MB
[step 27/8250] loss=2.8567 lr=8.61e-06 muon_lr=8.61e-05 grad_norm=13.7744 tok/s=1,181,900 raw_tok/s=1,188,601 step_tokens=130,333 waste=0.6% h100_mfu=28.92% vram=396/20,848MB peak=20,251/20,848MB
[step 28/8250] loss=2.8476 lr=8.94e-06 muon_lr=8.94e-05 grad_norm=11.7575 tok/s=1,115,850 raw_tok/s=1,119,600 step_tokens=130,633 waste=0.3% h100_mfu=27.24% vram=396/20,848MB peak=20,251/20,848MB
[step 29/8250] loss=2.8395 lr=9.27e-06 muon_lr=9.27e-05 grad_norm=11.1210 tok/s=1,150,646 raw_tok/s=1,152,916 step_tokens=130,814 waste=0.2% h100_mfu=28.05% vram=396/20,848MB peak=20,251/20,848MB
[step 30/8250] loss=2.8344 lr=9.60e-06 muon_lr=9.60e-05 grad_norm=10.5820 tok/s=1,182,342 raw_tok/s=1,186,351 step_tokens=130,629 waste=0.3% h100_mfu=28.86% vram=396/20,848MB peak=20,251/20,848MB
[step 31/8250] loss=2.8183 lr=9.93e-06 muon_lr=9.93e-05 grad_norm=9.2834 tok/s=1,213,419 raw_tok/s=1,216,724 step_tokens=130,716 waste=0.3% h100_mfu=29.60% vram=396/20,848MB peak=20,251/20,848MB
[step 32/8250] loss=2.8225 lr=1.03e-05 muon_lr=1.03e-04 grad_norm=8.3144 tok/s=1,112,047 raw_tok/s=1,114,862 step_tokens=130,741 waste=0.3% h100_mfu=27.12% vram=396/20,848MB peak=20,251/20,848MB
[step 33/8250] loss=2.7829 lr=1.06e-05 muon_lr=1.06e-04 grad_norm=8.0009 tok/s=1,152,012 raw_tok/s=1,154,743 step_tokens=130,762 waste=0.2% h100_mfu=28.10% vram=396/20,848MB peak=20,251/20,848MB
[step 34/8250] loss=2.7717 lr=1.09e-05 muon_lr=1.09e-04 grad_norm=8.0922 tok/s=1,187,221 raw_tok/s=1,188,391 step_tokens=130,943 waste=0.1% h100_mfu=28.91% vram=396/20,848MB peak=20,251/20,848MB
[step 35/8250] loss=2.7861 lr=1.13e-05 muon_lr=1.13e-04 grad_norm=8.4313 tok/s=1,189,144 raw_tok/s=1,192,146 step_tokens=130,742 waste=0.3% h100_mfu=29.01% vram=396/20,848MB peak=20,251/20,848MB
[step 36/8250] loss=2.7769 lr=1.16e-05 muon_lr=1.16e-04 grad_norm=8.1117 tok/s=1,179,550 raw_tok/s=1,183,007 step_tokens=130,689 waste=0.3% h100_mfu=28.78% vram=396/20,848MB peak=20,251/20,848MB
[step 37/8250] loss=2.7667 lr=1.19e-05 muon_lr=1.19e-04 grad_norm=6.7611 tok/s=1,167,654 raw_tok/s=1,168,144 step_tokens=131,017 waste=0.0% h100_mfu=28.42% vram=396/20,848MB peak=20,251/20,848MB
[step 38/8250] loss=2.7401 lr=1.23e-05 muon_lr=1.23e-04 grad_norm=4.8011 tok/s=1,181,660 raw_tok/s=1,188,105 step_tokens=130,361 waste=0.5% h100_mfu=28.91% vram=396/20,848MB peak=20,251/20,848MB
[step 39/8250] loss=2.7403 lr=1.26e-05 muon_lr=1.26e-04 grad_norm=3.6749 tok/s=1,144,303 raw_tok/s=1,148,623 step_tokens=130,579 waste=0.4% h100_mfu=27.95% vram=396/20,848MB peak=20,251/20,848MB
[step 40/8250] loss=2.7248 lr=1.29e-05 muon_lr=1.29e-04 grad_norm=3.2737 tok/s=1,188,674 raw_tok/s=1,189,173 step_tokens=131,017 waste=0.0% h100_mfu=28.93% vram=396/20,848MB peak=20,251/20,848MB
[step 41/8250] loss=2.7311 lr=1.32e-05 muon_lr=1.32e-04 grad_norm=2.7577 tok/s=1,162,082 raw_tok/s=1,163,645 step_tokens=130,896 waste=0.1% h100_mfu=28.31% vram=396/20,848MB peak=20,251/20,848MB
[step 42/8250] loss=2.7160 lr=1.36e-05 muon_lr=1.36e-04 grad_norm=2.3496 tok/s=1,193,926 raw_tok/s=1,195,211 step_tokens=130,931 waste=0.1% h100_mfu=29.08% vram=396/20,848MB peak=20,251/20,848MB
[step 43/8250] loss=2.7150 lr=1.39e-05 muon_lr=1.39e-04 grad_norm=2.2034 tok/s=1,239,040 raw_tok/s=1,240,829 step_tokens=130,883 waste=0.1% h100_mfu=30.19% vram=396/20,848MB peak=20,251/20,848MB
[step 44/8250] loss=2.7208 lr=1.42e-05 muon_lr=1.42e-04 grad_norm=2.9484 tok/s=1,199,268 raw_tok/s=1,201,450 step_tokens=130,834 waste=0.2% h100_mfu=29.23% vram=396/20,848MB peak=20,251/20,848MB
[step 45/8250] loss=2.6991 lr=1.46e-05 muon_lr=1.46e-04 grad_norm=2.5878 tok/s=1,177,310 raw_tok/s=1,182,534 step_tokens=130,493 waste=0.4% h100_mfu=28.77% vram=396/20,848MB peak=20,251/20,848MB
[step 46/8250] loss=2.7034 lr=1.49e-05 muon_lr=1.49e-04 grad_norm=2.4020 tok/s=1,100,208 raw_tok/s=1,104,996 step_tokens=130,504 waste=0.4% h100_mfu=26.88% vram=396/20,848MB peak=20,251/20,848MB
[step 47/8250] loss=2.7146 lr=1.52e-05 muon_lr=1.52e-04 grad_norm=2.0310 tok/s=1,188,991 raw_tok/s=1,191,883 step_tokens=130,754 waste=0.2% h100_mfu=29.00% vram=396/20,848MB peak=20,251/20,848MB
[step 48/8250] loss=2.7075 lr=1.56e-05 muon_lr=1.56e-04 grad_norm=1.5376 tok/s=1,240,444 raw_tok/s=1,243,642 step_tokens=130,735 waste=0.3% h100_mfu=30.26% vram=396/20,848MB peak=20,251/20,848MB
[step 49/8250] loss=2.7038 lr=1.59e-05 muon_lr=1.59e-04 grad_norm=1.6499 tok/s=1,189,457 raw_tok/s=1,190,229 step_tokens=130,987 waste=0.1% h100_mfu=28.96% vram=396/20,848MB peak=20,251/20,848MB
[step 50/8250] loss=2.7011 lr=1.62e-05 muon_lr=1.62e-04 grad_norm=1.6193 tok/s=1,181,838 raw_tok/s=1,183,708 step_tokens=130,865 waste=0.2% h100_mfu=28.80% vram=396/20,848MB peak=20,251/20,848MB
[step 51/8250] loss=2.6936 lr=1.66e-05 muon_lr=1.66e-04 grad_norm=1.7747 tok/s=1,205,003 raw_tok/s=1,206,623 step_tokens=130,896 waste=0.1% h100_mfu=29.36% vram=396/20,848MB peak=20,251/20,848MB
[step 52/8250] loss=2.7034 lr=1.69e-05 muon_lr=1.69e-04 grad_norm=1.5620 tok/s=1,154,100 raw_tok/s=1,155,758 step_tokens=130,884 waste=0.1% h100_mfu=28.12% vram=396/20,848MB peak=20,251/20,848MB
[step 53/8250] loss=2.6932 lr=1.72e-05 muon_lr=1.72e-04 grad_norm=1.1685 tok/s=1,186,816 raw_tok/s=1,190,686 step_tokens=130,646 waste=0.3% h100_mfu=28.97% vram=396/20,848MB peak=20,251/20,848MB
[step 54/8250] loss=2.6968 lr=1.75e-05 muon_lr=1.75e-04 grad_norm=1.2937 tok/s=1,191,793 raw_tok/s=1,192,366 step_tokens=131,009 waste=0.0% h100_mfu=29.01% vram=396/20,848MB peak=20,251/20,848MB
[step 55/8250] loss=2.6956 lr=1.79e-05 muon_lr=1.79e-04 grad_norm=1.0021 tok/s=1,191,659 raw_tok/s=1,193,398 step_tokens=130,881 waste=0.1% h100_mfu=29.04% vram=396/20,848MB peak=20,251/20,848MB
[step 56/8250] loss=2.6899 lr=1.82e-05 muon_lr=1.82e-04 grad_norm=1.0206 tok/s=1,192,871 raw_tok/s=1,194,913 step_tokens=130,848 waste=0.2% h100_mfu=29.07% vram=396/20,848MB peak=20,251/20,848MB
[step 57/8250] loss=2.6933 lr=1.85e-05 muon_lr=1.85e-04 grad_norm=1.3382 tok/s=1,163,197 raw_tok/s=1,166,990 step_tokens=130,646 waste=0.3% h100_mfu=28.39% vram=396/20,848MB peak=20,251/20,848MB
[step 58/8250] loss=2.6848 lr=1.89e-05 muon_lr=1.89e-04 grad_norm=1.1728 tok/s=1,181,923 raw_tok/s=1,183,738 step_tokens=130,871 waste=0.2% h100_mfu=28.80% vram=396/20,848MB peak=20,251/20,848MB
[step 59/8250] loss=2.6946 lr=1.92e-05 muon_lr=1.92e-04 grad_norm=1.0900 tok/s=1,097,381 raw_tok/s=1,100,715 step_tokens=130,675 waste=0.3% h100_mfu=26.78% vram=396/20,848MB peak=20,251/20,848MB
[step 60/8250] loss=2.6673 lr=1.95e-05 muon_lr=1.95e-04 grad_norm=1.2401 tok/s=1,243,348 raw_tok/s=1,244,355 step_tokens=130,966 waste=0.1% h100_mfu=30.28% vram=396/20,848MB peak=20,251/20,848MB
[step 61/8250] loss=2.6968 lr=1.99e-05 muon_lr=1.99e-04 grad_norm=1.2909 tok/s=1,178,010 raw_tok/s=1,182,258 step_tokens=130,601 waste=0.4% h100_mfu=28.76% vram=396/20,848MB peak=20,251/20,848MB
[step 62/8250] loss=2.6893 lr=2.02e-05 muon_lr=2.02e-04 grad_norm=1.4788 tok/s=1,191,135 raw_tok/s=1,193,439 step_tokens=130,819 waste=0.2% h100_mfu=29.04% vram=396/20,848MB peak=20,251/20,848MB
[step 63/8250] loss=2.6990 lr=2.05e-05 muon_lr=2.05e-04 grad_norm=1.5055 tok/s=1,175,241 raw_tok/s=1,177,387 step_tokens=130,833 waste=0.2% h100_mfu=28.65% vram=396/20,848MB peak=20,251/20,848MB
[step 64/8250] loss=2.6966 lr=2.09e-05 muon_lr=2.09e-04 grad_norm=1.4158 tok/s=1,189,491 raw_tok/s=1,192,020 step_tokens=130,794 waste=0.2% h100_mfu=29.00% vram=396/20,848MB peak=20,251/20,848MB
[step 65/8250] loss=2.6741 lr=2.12e-05 muon_lr=2.12e-04 grad_norm=1.4063 tok/s=1,157,670 raw_tok/s=1,163,850 step_tokens=130,376 waste=0.5% h100_mfu=28.32% vram=396/20,848MB peak=20,251/20,848MB
[step 66/8250] loss=2.6728 lr=2.15e-05 muon_lr=2.15e-04 grad_norm=1.5549 tok/s=1,185,444 raw_tok/s=1,190,612 step_tokens=130,503 waste=0.4% h100_mfu=28.97% vram=396/20,848MB peak=20,251/20,848MB
[step 67/8250] loss=2.6822 lr=2.19e-05 muon_lr=2.19e-04 grad_norm=1.3154 tok/s=1,224,966 raw_tok/s=1,227,457 step_tokens=130,806 waste=0.2% h100_mfu=29.86% vram=396/20,848MB peak=20,251/20,848MB
[step 68/8250] loss=2.6766 lr=2.22e-05 muon_lr=2.22e-04 grad_norm=0.9991 tok/s=1,189,417 raw_tok/s=1,189,526 step_tokens=131,060 waste=0.0% h100_mfu=28.94% vram=396/20,848MB peak=20,251/20,848MB
[step 69/8250] loss=2.6718 lr=2.25e-05 muon_lr=2.25e-04 grad_norm=1.0733 tok/s=1,220,113 raw_tok/s=1,224,382 step_tokens=130,615 waste=0.3% h100_mfu=29.79% vram=396/20,848MB peak=20,251/20,848MB
[step 70/8250] loss=2.6849 lr=2.28e-05 muon_lr=2.28e-04 grad_norm=1.2898 tok/s=1,177,496 raw_tok/s=1,180,965 step_tokens=130,687 waste=0.3% h100_mfu=28.73% vram=396/20,848MB peak=20,251/20,848MB
[step 71/8250] loss=2.6732 lr=2.32e-05 muon_lr=2.32e-04 grad_norm=1.3836 tok/s=1,176,536 raw_tok/s=1,182,645 step_tokens=130,395 waste=0.5% h100_mfu=28.77% vram=396/20,848MB peak=20,251/20,848MB
[step 72/8250] loss=2.6647 lr=2.35e-05 muon_lr=2.35e-04 grad_norm=1.2110 tok/s=1,157,369 raw_tok/s=1,162,068 step_tokens=130,542 waste=0.4% h100_mfu=28.27% vram=396/20,848MB peak=20,251/20,848MB
[step 73/8250] loss=2.6874 lr=2.38e-05 muon_lr=2.38e-04 grad_norm=1.0555 tok/s=1,230,590 raw_tok/s=1,235,539 step_tokens=130,547 waste=0.4% h100_mfu=30.06% vram=396/20,848MB peak=20,251/20,848MB
[step 74/8250] loss=2.6733 lr=2.42e-05 muon_lr=2.42e-04 grad_norm=1.1200 tok/s=1,187,121 raw_tok/s=1,188,100 step_tokens=130,964 waste=0.1% h100_mfu=28.91% vram=396/20,848MB peak=20,251/20,848MB
[step 75/8250] loss=2.6645 lr=2.45e-05 muon_lr=2.45e-04 grad_norm=1.1601 tok/s=1,236,594 raw_tok/s=1,242,719 step_tokens=130,426 waste=0.5% h100_mfu=30.24% vram=396/20,848MB peak=20,251/20,848MB
[step 76/8250] loss=2.6841 lr=2.48e-05 muon_lr=2.48e-04 grad_norm=1.3427 tok/s=1,216,695 raw_tok/s=1,221,102 step_tokens=130,599 waste=0.4% h100_mfu=29.71% vram=396/20,848MB peak=20,251/20,848MB
[step 77/8250] loss=2.6728 lr=2.52e-05 muon_lr=2.52e-04 grad_norm=1.0685 tok/s=1,241,207 raw_tok/s=1,242,914 step_tokens=130,892 waste=0.1% h100_mfu=30.24% vram=396/20,848MB peak=20,251/20,848MB
[step 78/8250] loss=2.6755 lr=2.55e-05 muon_lr=2.55e-04 grad_norm=1.1338 tok/s=1,174,633 raw_tok/s=1,177,427 step_tokens=130,761 waste=0.2% h100_mfu=28.65% vram=396/20,848MB peak=20,251/20,848MB
[step 79/8250] loss=2.6882 lr=2.58e-05 muon_lr=2.58e-04 grad_norm=1.2799 tok/s=1,142,419 raw_tok/s=1,144,681 step_tokens=130,813 waste=0.2% h100_mfu=27.85% vram=396/20,848MB peak=20,251/20,848MB
[step 80/8250] loss=2.6880 lr=2.62e-05 muon_lr=2.62e-04 grad_norm=1.2156 tok/s=1,233,216 raw_tok/s=1,237,313 step_tokens=130,638 waste=0.3% h100_mfu=30.10% vram=396/20,848MB peak=20,251/20,848MB
[step 81/8250] loss=2.6654 lr=2.65e-05 muon_lr=2.65e-04 grad_norm=1.3252 tok/s=1,153,954 raw_tok/s=1,155,814 step_tokens=130,861 waste=0.2% h100_mfu=28.12% vram=396/20,848MB peak=20,251/20,848MB
[step 82/8250] loss=2.6692 lr=2.68e-05 muon_lr=2.68e-04 grad_norm=1.1819 tok/s=1,241,334 raw_tok/s=1,242,643 step_tokens=130,934 waste=0.1% h100_mfu=30.23% vram=396/20,848MB peak=20,251/20,848MB
[step 83/8250] loss=2.6751 lr=2.72e-05 muon_lr=2.72e-04 grad_norm=1.2520 tok/s=1,191,298 raw_tok/s=1,192,517 step_tokens=130,938 waste=0.1% h100_mfu=29.01% vram=396/20,848MB peak=20,251/20,848MB
[step 84/8250] loss=2.6680 lr=2.75e-05 muon_lr=2.75e-04 grad_norm=1.3708 tok/s=1,240,170 raw_tok/s=1,242,084 step_tokens=130,870 waste=0.2% h100_mfu=30.22% vram=396/20,848MB peak=20,251/20,848MB
[step 85/8250] loss=2.6730 lr=2.78e-05 muon_lr=2.78e-04 grad_norm=1.5185 tok/s=1,213,344 raw_tok/s=1,215,097 step_tokens=130,883 waste=0.1% h100_mfu=29.56% vram=396/20,848MB peak=20,251/20,848MB
[step 86/8250] loss=2.6716 lr=2.81e-05 muon_lr=2.81e-04 grad_norm=1.3614 tok/s=1,149,512 raw_tok/s=1,152,000 step_tokens=130,789 waste=0.2% h100_mfu=28.03% vram=396/20,848MB peak=20,251/20,848MB
[step 87/8250] loss=2.6572 lr=2.85e-05 muon_lr=2.85e-04 grad_norm=1.2727 tok/s=1,154,424 raw_tok/s=1,156,896 step_tokens=130,792 waste=0.2% h100_mfu=28.15% vram=396/20,848MB peak=20,251/20,848MB
[step 88/8250] loss=2.6613 lr=2.88e-05 muon_lr=2.88e-04 grad_norm=1.0652 tok/s=1,189,397 raw_tok/s=1,190,806 step_tokens=130,917 waste=0.1% h100_mfu=28.97% vram=396/20,848MB peak=20,251/20,848MB
[step 89/8250] loss=2.6691 lr=2.91e-05 muon_lr=2.91e-04 grad_norm=1.0867 tok/s=1,233,857 raw_tok/s=1,239,323 step_tokens=130,494 waste=0.4% h100_mfu=30.15% vram=396/20,848MB peak=20,251/20,848MB
[step 90/8250] loss=2.6723 lr=2.95e-05 muon_lr=2.95e-04 grad_norm=1.1451 tok/s=1,239,637 raw_tok/s=1,242,557 step_tokens=130,764 waste=0.2% h100_mfu=30.23% vram=396/20,848MB peak=20,251/20,848MB
[step 91/8250] loss=2.6637 lr=2.98e-05 muon_lr=2.98e-04 grad_norm=0.9878 tok/s=1,233,853 raw_tok/s=1,238,483 step_tokens=130,582 waste=0.4% h100_mfu=30.13% vram=396/20,848MB peak=20,251/20,848MB
[step 92/8250] loss=2.6565 lr=3.01e-05 muon_lr=3.01e-04 grad_norm=1.3985 tok/s=1,238,592 raw_tok/s=1,240,988 step_tokens=130,819 waste=0.2% h100_mfu=30.19% vram=396/20,848MB peak=20,251/20,848MB
[step 93/8250] loss=2.6679 lr=3.05e-05 muon_lr=3.05e-04 grad_norm=1.3972 tok/s=1,181,478 raw_tok/s=1,184,197 step_tokens=130,771 waste=0.2% h100_mfu=28.81% vram=396/20,848MB peak=20,251/20,848MB
[step 94/8250] loss=2.6546 lr=3.08e-05 muon_lr=3.08e-04 grad_norm=1.2483 tok/s=1,149,440 raw_tok/s=1,152,844 step_tokens=130,685 waste=0.3% h100_mfu=28.05% vram=396/20,848MB peak=20,251/20,848MB
[step 95/8250] loss=2.6794 lr=3.11e-05 muon_lr=3.11e-04 grad_norm=1.2751 tok/s=1,189,391 raw_tok/s=1,192,959 step_tokens=130,680 waste=0.3% h100_mfu=29.02% vram=396/20,848MB peak=20,251/20,848MB
[step 96/8250] loss=2.6640 lr=3.15e-05 muon_lr=3.15e-04 grad_norm=0.9492 tok/s=1,228,027 raw_tok/s=1,230,920 step_tokens=130,764 waste=0.2% h100_mfu=29.95% vram=396/20,848MB peak=20,251/20,848MB
[step 97/8250] loss=2.6658 lr=3.18e-05 muon_lr=3.18e-04 grad_norm=1.0718 tok/s=1,180,399 raw_tok/s=1,181,480 step_tokens=130,952 waste=0.1% h100_mfu=28.75% vram=396/20,848MB peak=20,251/20,848MB
[step 98/8250] loss=2.6593 lr=3.21e-05 muon_lr=3.21e-04 grad_norm=1.1254 tok/s=1,181,728 raw_tok/s=1,188,528 step_tokens=130,322 waste=0.6% h100_mfu=28.92% vram=396/20,848MB peak=20,251/20,848MB
[step 99/8250] loss=2.6632 lr=3.25e-05 muon_lr=3.25e-04 grad_norm=1.0472 tok/s=1,177,396 raw_tok/s=1,178,763 step_tokens=130,920 waste=0.1% h100_mfu=28.68% vram=396/20,848MB peak=20,251/20,848MB
[step 100/8250] loss=2.6637 lr=3.28e-05 muon_lr=3.28e-04 grad_norm=0.7605 tok/s=1,190,538 raw_tok/s=1,192,230 step_tokens=130,886 waste=0.1% h100_mfu=29.01% vram=396/20,848MB peak=20,251/20,848MB
[step 101/8250] loss=2.6643 lr=3.31e-05 muon_lr=3.31e-04 grad_norm=0.7718 tok/s=1,224,906 raw_tok/s=1,229,860 step_tokens=130,544 waste=0.4% h100_mfu=29.92% vram=396/20,848MB peak=20,251/20,848MB
[step 102/8250] loss=2.6725 lr=3.34e-05 muon_lr=3.34e-04 grad_norm=0.8879 tok/s=1,183,439 raw_tok/s=1,184,686 step_tokens=130,934 waste=0.1% h100_mfu=28.82% vram=396/20,848MB peak=20,251/20,848MB
[step 103/8250] loss=2.6532 lr=3.38e-05 muon_lr=3.38e-04 grad_norm=0.6693 tok/s=1,158,804 raw_tok/s=1,160,096 step_tokens=130,926 waste=0.1% h100_mfu=28.23% vram=396/20,848MB peak=20,251/20,848MB
[step 104/8250] loss=2.6717 lr=3.41e-05 muon_lr=3.41e-04 grad_norm=0.7279 tok/s=1,193,200 raw_tok/s=1,195,070 step_tokens=130,867 waste=0.2% h100_mfu=29.08% vram=396/20,848MB peak=20,251/20,848MB
[step 105/8250] loss=2.6801 lr=3.44e-05 muon_lr=3.44e-04 grad_norm=0.9333 tok/s=1,195,280 raw_tok/s=1,196,119 step_tokens=130,980 waste=0.1% h100_mfu=29.10% vram=396/20,848MB peak=20,251/20,848MB
[step 106/8250] loss=2.6590 lr=3.48e-05 muon_lr=3.48e-04 grad_norm=0.8696 tok/s=1,179,637 raw_tok/s=1,182,940 step_tokens=130,706 waste=0.3% h100_mfu=28.78% vram=396/20,848MB peak=20,251/20,848MB
[step 107/8250] loss=2.6636 lr=3.51e-05 muon_lr=3.51e-04 grad_norm=0.9795 tok/s=1,172,964 raw_tok/s=1,176,213 step_tokens=130,710 waste=0.3% h100_mfu=28.62% vram=396/20,848MB peak=20,251/20,848MB
[step 108/8250] loss=2.6639 lr=3.54e-05 muon_lr=3.54e-04 grad_norm=0.8761 tok/s=1,148,631 raw_tok/s=1,151,522 step_tokens=130,743 waste=0.3% h100_mfu=28.02% vram=396/20,848MB peak=20,251/20,848MB
[step 109/8250] loss=2.6623 lr=3.58e-05 muon_lr=3.58e-04 grad_norm=0.6998 tok/s=1,088,824 raw_tok/s=1,090,871 step_tokens=130,826 waste=0.2% h100_mfu=26.54% vram=396/20,848MB peak=20,251/20,848MB
[step 110/8250] loss=2.6723 lr=3.61e-05 muon_lr=3.61e-04 grad_norm=0.8993 tok/s=1,189,075 raw_tok/s=1,192,432 step_tokens=130,703 waste=0.3% h100_mfu=29.01% vram=396/20,848MB peak=20,251/20,848MB
[step 111/8250] loss=2.6591 lr=3.64e-05 muon_lr=3.64e-04 grad_norm=1.0975 tok/s=1,194,522 raw_tok/s=1,195,653 step_tokens=130,948 waste=0.1% h100_mfu=29.09% vram=396/20,848MB peak=20,251/20,848MB
[step 112/8250] loss=2.6510 lr=3.68e-05 muon_lr=3.68e-04 grad_norm=1.3291 tok/s=1,184,411 raw_tok/s=1,187,173 step_tokens=130,767 waste=0.2% h100_mfu=28.88% vram=396/20,848MB peak=20,251/20,848MB
[step 113/8250] loss=2.6636 lr=3.71e-05 muon_lr=3.71e-04 grad_norm=1.2954 tok/s=1,184,907 raw_tok/s=1,189,344 step_tokens=130,583 waste=0.4% h100_mfu=28.94% vram=396/20,848MB peak=20,251/20,848MB
[step 114/8250] loss=2.6733 lr=3.74e-05 muon_lr=3.74e-04 grad_norm=1.2471 tok/s=986,312 raw_tok/s=987,442 step_tokens=130,922 waste=0.1% h100_mfu=24.02% vram=396/20,848MB peak=20,251/20,848MB
[step 115/8250] loss=2.6719 lr=3.77e-05 muon_lr=3.77e-04 grad_norm=1.1015 tok/s=1,212,461 raw_tok/s=1,216,471 step_tokens=130,640 waste=0.3% h100_mfu=29.60% vram=396/20,848MB peak=20,251/20,848MB
[step 116/8250] loss=2.6719 lr=3.81e-05 muon_lr=3.81e-04 grad_norm=1.0294 tok/s=1,227,903 raw_tok/s=1,233,351 step_tokens=130,493 waste=0.4% h100_mfu=30.01% vram=396/20,848MB peak=20,251/20,848MB
[step 117/8250] loss=2.6799 lr=3.84e-05 muon_lr=3.84e-04 grad_norm=1.2590 tok/s=1,193,597 raw_tok/s=1,196,638 step_tokens=130,739 waste=0.3% h100_mfu=29.11% vram=396/20,848MB peak=20,251/20,848MB
[step 118/8250] loss=2.6588 lr=3.87e-05 muon_lr=3.87e-04 grad_norm=1.3069 tok/s=1,191,965 raw_tok/s=1,193,759 step_tokens=130,875 waste=0.2% h100_mfu=29.04% vram=396/20,848MB peak=20,251/20,848MB
[step 119/8250] loss=2.6574 lr=3.91e-05 muon_lr=3.91e-04 grad_norm=1.1283 tok/s=1,208,513 raw_tok/s=1,213,615 step_tokens=130,521 waste=0.4% h100_mfu=29.53% vram=396/20,848MB peak=20,251/20,848MB
[step 120/8250] loss=2.6541 lr=3.94e-05 muon_lr=3.94e-04 grad_norm=0.8926 tok/s=1,181,478 raw_tok/s=1,183,130 step_tokens=130,889 waste=0.1% h100_mfu=28.79% vram=396/20,848MB peak=20,251/20,848MB
[step 121/8250] loss=2.6533 lr=3.97e-05 muon_lr=3.97e-04 grad_norm=0.9369 tok/s=1,190,845 raw_tok/s=1,194,070 step_tokens=130,718 waste=0.3% h100_mfu=29.05% vram=396/20,848MB peak=20,251/20,848MB
[step 122/8250] loss=2.6586 lr=4.01e-05 muon_lr=4.01e-04 grad_norm=1.0439 tok/s=1,060,098 raw_tok/s=1,062,498 step_tokens=130,776 waste=0.2% h100_mfu=25.85% vram=396/20,848MB peak=20,251/20,848MB
[step 123/8250] loss=2.6769 lr=4.04e-05 muon_lr=4.04e-04 grad_norm=0.8151 tok/s=1,187,596 raw_tok/s=1,192,353 step_tokens=130,549 waste=0.4% h100_mfu=29.01% vram=396/20,848MB peak=20,251/20,848MB
[step 124/8250] loss=2.6633 lr=4.07e-05 muon_lr=4.07e-04 grad_norm=0.8074 tok/s=1,193,575 raw_tok/s=1,195,892 step_tokens=130,818 waste=0.2% h100_mfu=29.10% vram=396/20,848MB peak=20,251/20,848MB
[step 125/8250] loss=2.6554 lr=4.11e-05 muon_lr=4.11e-04 grad_norm=0.9214 tok/s=1,206,807 raw_tok/s=1,208,781 step_tokens=130,858 waste=0.2% h100_mfu=29.41% vram=396/20,848MB peak=20,251/20,848MB
[step 126/8250] loss=2.6715 lr=4.14e-05 muon_lr=4.14e-04 grad_norm=0.8285 tok/s=1,155,667 raw_tok/s=1,158,300 step_tokens=130,774 waste=0.2% h100_mfu=28.18% vram=396/20,848MB peak=20,251/20,848MB
[step 127/8250] loss=2.6620 lr=4.17e-05 muon_lr=4.17e-04 grad_norm=0.6832 tok/s=1,225,555 raw_tok/s=1,227,691 step_tokens=130,844 waste=0.2% h100_mfu=29.87% vram=396/20,848MB peak=20,251/20,848MB
[step 128/8250] loss=2.6501 lr=4.21e-05 muon_lr=4.21e-04 grad_norm=0.6889 tok/s=1,105,753 raw_tok/s=1,109,096 step_tokens=130,677 waste=0.3% h100_mfu=26.98% vram=396/20,848MB peak=20,251/20,848MB
[step 129/8250] loss=2.6459 lr=4.24e-05 muon_lr=4.24e-04 grad_norm=0.9265 tok/s=1,215,146 raw_tok/s=1,218,819 step_tokens=130,677 waste=0.3% h100_mfu=29.65% vram=396/20,848MB peak=20,251/20,848MB
[step 130/8250] loss=2.6562 lr=4.27e-05 muon_lr=4.27e-04 grad_norm=0.8938 tok/s=1,170,631 raw_tok/s=1,175,285 step_tokens=130,553 waste=0.4% h100_mfu=28.59% vram=396/20,848MB peak=20,251/20,848MB
[step 131/8250] loss=2.6574 lr=4.30e-05 muon_lr=4.30e-04 grad_norm=0.8427 tok/s=1,206,533 raw_tok/s=1,208,128 step_tokens=130,899 waste=0.1% h100_mfu=29.39% vram=396/20,848MB peak=20,251/20,848MB
[step 132/8250] loss=2.6675 lr=4.34e-05 muon_lr=4.34e-04 grad_norm=0.8799 tok/s=1,188,790 raw_tok/s=1,193,653 step_tokens=130,538 waste=0.4% h100_mfu=29.04% vram=396/20,848MB peak=20,251/20,848MB
[step 133/8250] loss=2.6650 lr=4.37e-05 muon_lr=4.37e-04 grad_norm=1.0086 tok/s=1,115,607 raw_tok/s=1,118,107 step_tokens=130,779 waste=0.2% h100_mfu=27.20% vram=396/20,848MB peak=20,251/20,848MB
[step 134/8250] loss=2.6524 lr=4.40e-05 muon_lr=4.40e-04 grad_norm=0.8161 tok/s=1,122,415 raw_tok/s=1,126,498 step_tokens=130,597 waste=0.4% h100_mfu=27.41% vram=396/20,848MB peak=20,251/20,848MB
[step 135/8250] loss=2.6527 lr=4.44e-05 muon_lr=4.44e-04 grad_norm=0.8844 tok/s=1,211,427 raw_tok/s=1,216,075 step_tokens=130,571 waste=0.4% h100_mfu=29.59% vram=396/20,848MB peak=20,251/20,848MB
[step 136/8250] loss=2.6579 lr=4.47e-05 muon_lr=4.47e-04 grad_norm=0.8123 tok/s=1,188,487 raw_tok/s=1,190,931 step_tokens=130,803 waste=0.2% h100_mfu=28.98% vram=396/20,848MB peak=20,251/20,848MB
[step 137/8250] loss=2.6573 lr=4.50e-05 muon_lr=4.50e-04 grad_norm=0.5683 tok/s=1,157,855 raw_tok/s=1,162,922 step_tokens=130,501 waste=0.4% h100_mfu=28.29% vram=396/20,848MB peak=20,251/20,848MB
[step 138/8250] loss=2.6589 lr=4.54e-05 muon_lr=4.54e-04 grad_norm=0.7926 tok/s=1,162,118 raw_tok/s=1,163,930 step_tokens=130,868 waste=0.2% h100_mfu=28.32% vram=396/20,848MB peak=20,251/20,848MB
[step 139/8250] loss=2.6495 lr=4.57e-05 muon_lr=4.57e-04 grad_norm=0.9537 tok/s=1,118,555 raw_tok/s=1,120,675 step_tokens=130,824 waste=0.2% h100_mfu=27.27% vram=396/20,848MB peak=20,251/20,848MB
[step 140/8250] loss=2.6511 lr=4.60e-05 muon_lr=4.60e-04 grad_norm=0.9334 tok/s=1,168,912 raw_tok/s=1,170,242 step_tokens=130,923 waste=0.1% h100_mfu=28.47% vram=396/20,848MB peak=20,251/20,848MB
[step 141/8250] loss=2.6382 lr=4.64e-05 muon_lr=4.64e-04 grad_norm=0.9415 tok/s=1,164,880 raw_tok/s=1,166,072 step_tokens=130,938 waste=0.1% h100_mfu=28.37% vram=396/20,848MB peak=20,251/20,848MB
[step 142/8250] loss=2.6608 lr=4.67e-05 muon_lr=4.67e-04 grad_norm=0.8263 tok/s=1,175,395 raw_tok/s=1,178,199 step_tokens=130,760 waste=0.2% h100_mfu=28.67% vram=396/20,848MB peak=20,251/20,848MB
[step 143/8250] loss=2.6522 lr=4.70e-05 muon_lr=4.70e-04 grad_norm=0.7379 tok/s=1,175,389 raw_tok/s=1,180,514 step_tokens=130,503 waste=0.4% h100_mfu=28.72% vram=396/20,848MB peak=20,251/20,848MB
[step 144/8250] loss=2.6530 lr=4.74e-05 muon_lr=4.74e-04 grad_norm=0.8805 tok/s=1,171,871 raw_tok/s=1,174,227 step_tokens=130,809 waste=0.2% h100_mfu=28.57% vram=396/20,848MB peak=20,251/20,848MB
[step 145/8250] loss=2.6444 lr=4.77e-05 muon_lr=4.77e-04 grad_norm=0.7255 tok/s=1,135,818 raw_tok/s=1,141,548 step_tokens=130,414 waste=0.5% h100_mfu=27.77% vram=396/20,848MB peak=20,251/20,848MB
[step 146/8250] loss=2.6439 lr=4.80e-05 muon_lr=4.80e-04 grad_norm=0.6933 tok/s=1,175,945 raw_tok/s=1,177,293 step_tokens=130,922 waste=0.1% h100_mfu=28.64% vram=396/20,848MB peak=20,251/20,848MB
[step 147/8250] loss=2.6493 lr=4.83e-05 muon_lr=4.83e-04 grad_norm=0.7634 tok/s=1,183,621 raw_tok/s=1,188,491 step_tokens=130,535 waste=0.4% h100_mfu=28.92% vram=396/20,848MB peak=20,251/20,848MB
[step 148/8250] loss=2.6412 lr=4.87e-05 muon_lr=4.87e-04 grad_norm=0.6563 tok/s=1,233,485 raw_tok/s=1,235,521 step_tokens=130,856 waste=0.2% h100_mfu=30.06% vram=396/20,848MB peak=20,251/20,848MB
[step 149/8250] loss=2.6418 lr=4.90e-05 muon_lr=4.90e-04 grad_norm=0.6126 tok/s=1,201,209 raw_tok/s=1,202,686 step_tokens=130,911 waste=0.1% h100_mfu=29.26% vram=396/20,848MB peak=20,251/20,848MB
[step 150/8250] loss=2.6442 lr=4.93e-05 muon_lr=4.93e-04 grad_norm=0.5980 tok/s=1,157,661 raw_tok/s=1,162,157 step_tokens=130,565 waste=0.4% h100_mfu=28.28% vram=396/20,848MB peak=20,251/20,848MB
[step 151/8250] loss=2.6476 lr=4.97e-05 muon_lr=4.97e-04 grad_norm=0.5889 tok/s=1,146,485 raw_tok/s=1,150,681 step_tokens=130,594 waste=0.4% h100_mfu=28.00% vram=396/20,848MB peak=20,251/20,848MB
[step 152/8250] loss=2.6458 lr=5.00e-05 muon_lr=5.00e-04 grad_norm=0.6047 tok/s=1,164,483 raw_tok/s=1,166,718 step_tokens=130,821 waste=0.2% h100_mfu=28.39% vram=396/20,848MB peak=20,251/20,848MB
[step 153/8250] loss=2.6379 lr=5.03e-05 muon_lr=5.03e-04 grad_norm=0.5988 tok/s=1,172,717 raw_tok/s=1,175,920 step_tokens=130,715 waste=0.3% h100_mfu=28.61% vram=396/20,848MB peak=20,251/20,848MB
[step 154/8250] loss=2.6331 lr=5.07e-05 muon_lr=5.07e-04 grad_norm=0.6206 tok/s=1,118,613 raw_tok/s=1,120,442 step_tokens=130,858 waste=0.2% h100_mfu=27.26% vram=396/20,848MB peak=20,251/20,848MB
[step 155/8250] loss=2.6440 lr=5.10e-05 muon_lr=5.10e-04 grad_norm=0.5047 tok/s=1,156,883 raw_tok/s=1,157,298 step_tokens=131,025 waste=0.0% h100_mfu=28.16% vram=396/20,848MB peak=20,251/20,848MB
[step 156/8250] loss=2.6496 lr=5.13e-05 muon_lr=5.13e-04 grad_norm=0.5796 tok/s=1,145,158 raw_tok/s=1,147,224 step_tokens=130,836 waste=0.2% h100_mfu=27.91% vram=396/20,848MB peak=20,251/20,848MB
[step 157/8250] loss=2.6468 lr=5.17e-05 muon_lr=5.17e-04 grad_norm=0.7893 tok/s=1,166,525 raw_tok/s=1,169,765 step_tokens=130,709 waste=0.3% h100_mfu=28.46% vram=396/20,848MB peak=20,251/20,848MB
[step 158/8250] loss=2.6496 lr=5.20e-05 muon_lr=5.20e-04 grad_norm=0.7746 tok/s=1,233,670 raw_tok/s=1,237,238 step_tokens=130,694 waste=0.3% h100_mfu=30.10% vram=396/20,848MB peak=20,251/20,848MB
[step 159/8250] loss=2.6357 lr=5.23e-05 muon_lr=5.23e-04 grad_norm=0.6840 tok/s=1,092,084 raw_tok/s=1,095,838 step_tokens=130,623 waste=0.3% h100_mfu=26.66% vram=396/20,848MB peak=20,251/20,848MB
[step 160/8250] loss=2.6478 lr=5.26e-05 muon_lr=5.26e-04 grad_norm=0.8268 tok/s=1,174,728 raw_tok/s=1,179,859 step_tokens=130,502 waste=0.4% h100_mfu=28.71% vram=396/20,848MB peak=20,251/20,848MB
[step 161/8250] loss=2.6417 lr=5.30e-05 muon_lr=5.30e-04 grad_norm=0.5681 tok/s=1,157,749 raw_tok/s=1,160,742 step_tokens=130,734 waste=0.3% h100_mfu=28.24% vram=396/20,848MB peak=20,251/20,848MB
[step 162/8250] loss=2.6651 lr=5.33e-05 muon_lr=5.33e-04 grad_norm=0.6536 tok/s=1,130,286 raw_tok/s=1,132,888 step_tokens=130,771 waste=0.2% h100_mfu=27.56% vram=396/20,848MB peak=20,251/20,848MB
[step 163/8250] loss=2.6423 lr=5.36e-05 muon_lr=5.36e-04 grad_norm=0.5496 tok/s=1,174,617 raw_tok/s=1,180,398 step_tokens=130,430 waste=0.5% h100_mfu=28.72% vram=396/20,848MB peak=20,251/20,848MB
[step 164/8250] loss=2.6469 lr=5.40e-05 muon_lr=5.40e-04 grad_norm=0.5960 tok/s=1,187,553 raw_tok/s=1,190,350 step_tokens=130,764 waste=0.2% h100_mfu=28.96% vram=396/20,848MB peak=20,251/20,848MB
[step 165/8250] loss=2.6398 lr=5.43e-05 muon_lr=5.43e-04 grad_norm=0.8598 tok/s=1,149,130 raw_tok/s=1,152,269 step_tokens=130,715 waste=0.3% h100_mfu=28.03% vram=396/20,848MB peak=20,251/20,848MB
[step 166/8250] loss=2.6388 lr=5.46e-05 muon_lr=5.46e-04 grad_norm=0.6246 tok/s=1,185,323 raw_tok/s=1,189,079 step_tokens=130,658 waste=0.3% h100_mfu=28.93% vram=396/20,848MB peak=20,251/20,848MB
[step 167/8250] loss=2.6451 lr=5.50e-05 muon_lr=5.50e-04 grad_norm=0.5441 tok/s=1,238,206 raw_tok/s=1,239,142 step_tokens=130,973 waste=0.1% h100_mfu=30.15% vram=396/20,848MB peak=20,251/20,848MB
[step 168/8250] loss=2.6476 lr=5.53e-05 muon_lr=5.53e-04 grad_norm=0.7289 tok/s=1,187,111 raw_tok/s=1,188,735 step_tokens=130,893 waste=0.1% h100_mfu=28.92% vram=396/20,848MB peak=20,251/20,848MB
[step 169/8250] loss=2.6441 lr=5.56e-05 muon_lr=5.56e-04 grad_norm=0.6025 tok/s=1,190,394 raw_tok/s=1,194,221 step_tokens=130,652 waste=0.3% h100_mfu=29.06% vram=396/20,848MB peak=20,251/20,848MB
[step 170/8250] loss=2.6345 lr=5.60e-05 muon_lr=5.60e-04 grad_norm=0.4815 tok/s=1,172,734 raw_tok/s=1,176,396 step_tokens=130,664 waste=0.3% h100_mfu=28.62% vram=396/20,848MB peak=20,251/20,848MB
[step 171/8250] loss=2.6402 lr=5.63e-05 muon_lr=5.63e-04 grad_norm=0.7564 tok/s=1,073,177 raw_tok/s=1,075,886 step_tokens=130,742 waste=0.3% h100_mfu=26.18% vram=396/20,848MB peak=20,251/20,848MB
[step 172/8250] loss=2.6561 lr=5.66e-05 muon_lr=5.66e-04 grad_norm=0.5954 tok/s=1,218,209 raw_tok/s=1,218,729 step_tokens=131,016 waste=0.0% h100_mfu=29.65% vram=396/20,848MB peak=20,251/20,848MB
[step 173/8250] loss=2.6328 lr=5.70e-05 muon_lr=5.70e-04 grad_norm=0.7090 tok/s=1,236,691 raw_tok/s=1,241,341 step_tokens=130,581 waste=0.4% h100_mfu=30.20% vram=396/20,848MB peak=20,251/20,848MB
[step 174/8250] loss=2.6353 lr=5.73e-05 muon_lr=5.73e-04 grad_norm=0.6929 tok/s=1,189,552 raw_tok/s=1,195,307 step_tokens=130,441 waste=0.5% h100_mfu=29.08% vram=396/20,848MB peak=20,251/20,848MB
[step 175/8250] loss=2.6374 lr=5.76e-05 muon_lr=5.76e-04 grad_norm=0.6255 tok/s=1,167,958 raw_tok/s=1,171,121 step_tokens=130,718 waste=0.3% h100_mfu=28.49% vram=396/20,848MB peak=20,251/20,848MB
[step 176/8250] loss=2.6313 lr=5.79e-05 muon_lr=5.79e-04 grad_norm=0.6521 tok/s=1,071,541 raw_tok/s=1,073,613 step_tokens=130,819 waste=0.2% h100_mfu=26.12% vram=396/20,848MB peak=20,251/20,848MB
[step 177/8250] loss=2.6276 lr=5.83e-05 muon_lr=5.83e-04 grad_norm=0.5435 tok/s=1,186,351 raw_tok/s=1,190,657 step_tokens=130,598 waste=0.4% h100_mfu=28.97% vram=396/20,848MB peak=20,251/20,848MB
[step 178/8250] loss=2.6558 lr=5.86e-05 muon_lr=5.86e-04 grad_norm=0.5571 tok/s=1,161,700 raw_tok/s=1,165,407 step_tokens=130,655 waste=0.3% h100_mfu=28.35% vram=396/20,848MB peak=20,251/20,848MB
[step 179/8250] loss=2.6361 lr=5.89e-05 muon_lr=5.89e-04 grad_norm=0.5761 tok/s=1,192,821 raw_tok/s=1,195,512 step_tokens=130,777 waste=0.2% h100_mfu=29.09% vram=396/20,848MB peak=20,251/20,848MB
[step 180/8250] loss=2.6430 lr=5.93e-05 muon_lr=5.93e-04 grad_norm=0.6364 tok/s=1,188,467 raw_tok/s=1,192,077 step_tokens=130,675 waste=0.3% h100_mfu=29.00% vram=396/20,848MB peak=20,251/20,848MB
[step 181/8250] loss=2.6432 lr=5.96e-05 muon_lr=5.96e-04 grad_norm=0.7030 tok/s=1,175,280 raw_tok/s=1,177,220 step_tokens=130,856 waste=0.2% h100_mfu=28.64% vram=396/20,848MB peak=20,251/20,848MB
[step 182/8250] loss=2.6402 lr=5.99e-05 muon_lr=5.99e-04 grad_norm=0.6410 tok/s=1,156,110 raw_tok/s=1,157,983 step_tokens=130,860 waste=0.2% h100_mfu=28.17% vram=396/20,848MB peak=20,251/20,848MB
[step 183/8250] loss=2.6253 lr=6.03e-05 muon_lr=6.03e-04 grad_norm=0.5275 tok/s=1,102,873 raw_tok/s=1,106,571 step_tokens=130,634 waste=0.3% h100_mfu=26.92% vram=396/20,848MB peak=20,251/20,848MB
[step 184/8250] loss=2.6425 lr=6.06e-05 muon_lr=6.06e-04 grad_norm=0.4339 tok/s=1,190,572 raw_tok/s=1,190,953 step_tokens=131,030 waste=0.0% h100_mfu=28.98% vram=396/20,848MB peak=20,251/20,848MB
[step 185/8250] loss=2.6289 lr=6.09e-05 muon_lr=6.09e-04 grad_norm=0.4862 tok/s=1,194,165 raw_tok/s=1,194,940 step_tokens=130,987 waste=0.1% h100_mfu=29.07% vram=396/20,848MB peak=20,251/20,848MB
[step 186/8250] loss=2.6326 lr=6.13e-05 muon_lr=6.13e-04 grad_norm=0.5650 tok/s=1,232,154 raw_tok/s=1,238,579 step_tokens=130,392 waste=0.5% h100_mfu=30.13% vram=396/20,848MB peak=20,251/20,848MB
[step 187/8250] loss=2.6366 lr=6.16e-05 muon_lr=6.16e-04 grad_norm=0.6986 tok/s=1,212,809 raw_tok/s=1,216,047 step_tokens=130,723 waste=0.3% h100_mfu=29.59% vram=396/20,848MB peak=20,251/20,848MB
[step 188/8250] loss=2.6278 lr=6.19e-05 muon_lr=6.19e-04 grad_norm=0.5298 tok/s=1,193,734 raw_tok/s=1,195,002 step_tokens=130,933 waste=0.1% h100_mfu=29.07% vram=396/20,848MB peak=20,251/20,848MB
[step 189/8250] loss=2.6371 lr=6.23e-05 muon_lr=6.23e-04 grad_norm=0.4672 tok/s=1,162,472 raw_tok/s=1,167,237 step_tokens=130,537 waste=0.4% h100_mfu=28.40% vram=396/20,848MB peak=20,251/20,848MB
[step 190/8250] loss=2.6183 lr=6.26e-05 muon_lr=6.26e-04 grad_norm=0.5035 tok/s=1,155,997 raw_tok/s=1,158,853 step_tokens=130,749 waste=0.2% h100_mfu=28.19% vram=396/20,848MB peak=20,251/20,848MB
[step 191/8250] loss=2.6414 lr=6.29e-05 muon_lr=6.29e-04 grad_norm=0.6195 tok/s=1,159,905 raw_tok/s=1,164,703 step_tokens=130,532 waste=0.4% h100_mfu=28.34% vram=396/20,848MB peak=20,251/20,848MB
[step 192/8250] loss=2.6327 lr=6.32e-05 muon_lr=6.32e-04 grad_norm=0.6063 tok/s=1,193,770 raw_tok/s=1,194,992 step_tokens=130,938 waste=0.1% h100_mfu=29.07% vram=396/20,848MB peak=20,251/20,848MB
[step 193/8250] loss=2.6363 lr=6.36e-05 muon_lr=6.36e-04 grad_norm=0.5517 tok/s=1,236,534 raw_tok/s=1,238,603 step_tokens=130,853 waste=0.2% h100_mfu=30.14% vram=396/20,848MB peak=20,251/20,848MB
[step 194/8250] loss=2.6118 lr=6.39e-05 muon_lr=6.39e-04 grad_norm=0.5132 tok/s=1,205,242 raw_tok/s=1,209,533 step_tokens=130,607 waste=0.4% h100_mfu=29.43% vram=396/20,848MB peak=20,251/20,848MB
[step 195/8250] loss=2.6264 lr=6.42e-05 muon_lr=6.42e-04 grad_norm=0.6494 tok/s=1,177,807 raw_tok/s=1,182,806 step_tokens=130,518 waste=0.4% h100_mfu=28.78% vram=396/20,848MB peak=20,251/20,848MB
[step 196/8250] loss=2.6322 lr=6.46e-05 muon_lr=6.46e-04 grad_norm=0.7321 tok/s=1,091,179 raw_tok/s=1,092,521 step_tokens=130,911 waste=0.1% h100_mfu=26.58% vram=396/20,848MB peak=20,251/20,848MB
[step 197/8250] loss=2.6286 lr=6.49e-05 muon_lr=6.49e-04 grad_norm=0.4077 tok/s=1,229,590 raw_tok/s=1,232,355 step_tokens=130,778 waste=0.2% h100_mfu=29.98% vram=396/20,848MB peak=20,251/20,848MB
[step 198/8250] loss=2.6356 lr=6.52e-05 muon_lr=6.52e-04 grad_norm=0.6669 tok/s=1,191,114 raw_tok/s=1,195,584 step_tokens=130,582 waste=0.4% h100_mfu=29.09% vram=396/20,848MB peak=20,251/20,848MB
[step 199/8250] loss=2.6414 lr=6.56e-05 muon_lr=6.56e-04 grad_norm=0.6063 tok/s=1,233,407 raw_tok/s=1,237,173 step_tokens=130,673 waste=0.3% h100_mfu=30.10% vram=396/20,848MB peak=20,251/20,848MB
[step 200/8250] loss=2.6440 lr=6.59e-05 muon_lr=6.59e-04 grad_norm=0.5201 tok/s=1,153,433 raw_tok/s=1,154,984 step_tokens=130,896 waste=0.1% h100_mfu=28.10% vram=396/20,848MB peak=20,251/20,848MB
[step 201/8250] loss=2.6353 lr=6.62e-05 muon_lr=6.62e-04 grad_norm=0.4688 tok/s=1,090,678 raw_tok/s=1,091,278 step_tokens=131,000 waste=0.1% h100_mfu=26.55% vram=396/20,848MB peak=20,251/20,848MB
[step 202/8250] loss=2.6353 lr=6.66e-05 muon_lr=6.66e-04 grad_norm=0.4686 tok/s=1,128,277 raw_tok/s=1,133,257 step_tokens=130,496 waste=0.4% h100_mfu=27.57% vram=396/20,848MB peak=20,251/20,848MB
[step 203/8250] loss=2.6310 lr=6.69e-05 muon_lr=6.69e-04 grad_norm=0.5779 tok/s=1,207,009 raw_tok/s=1,208,549 step_tokens=130,905 waste=0.1% h100_mfu=29.40% vram=396/20,848MB peak=20,251/20,848MB
[step 204/8250] loss=2.6304 lr=6.72e-05 muon_lr=6.72e-04 grad_norm=0.4848 tok/s=1,183,203 raw_tok/s=1,185,075 step_tokens=130,865 waste=0.2% h100_mfu=28.83% vram=396/20,848MB peak=20,251/20,848MB
[step 205/8250] loss=2.6373 lr=6.75e-05 muon_lr=6.75e-04 grad_norm=0.5124 tok/s=1,057,910 raw_tok/s=1,060,272 step_tokens=130,780 waste=0.2% h100_mfu=25.80% vram=396/20,848MB peak=20,251/20,848MB
[step 206/8250] loss=2.6320 lr=6.79e-05 muon_lr=6.79e-04 grad_norm=0.5046 tok/s=1,158,573 raw_tok/s=1,161,311 step_tokens=130,763 waste=0.2% h100_mfu=28.25% vram=396/20,848MB peak=20,251/20,848MB
[step 207/8250] loss=2.6388 lr=6.82e-05 muon_lr=6.82e-04 grad_norm=0.5521 tok/s=1,053,540 raw_tok/s=1,055,416 step_tokens=130,839 waste=0.2% h100_mfu=25.68% vram=396/20,848MB peak=20,251/20,848MB
[step 208/8250] loss=2.6287 lr=6.85e-05 muon_lr=6.85e-04 grad_norm=0.5090 tok/s=1,160,997 raw_tok/s=1,164,881 step_tokens=130,635 waste=0.3% h100_mfu=28.34% vram=396/20,848MB peak=20,251/20,848MB
[step 209/8250] loss=2.6425 lr=6.89e-05 muon_lr=6.89e-04 grad_norm=0.5694 tok/s=1,065,787 raw_tok/s=1,067,041 step_tokens=130,918 waste=0.1% h100_mfu=25.96% vram=396/20,848MB peak=20,251/20,848MB
[step 210/8250] loss=2.6352 lr=6.92e-05 muon_lr=6.92e-04 grad_norm=0.4605 tok/s=1,111,427 raw_tok/s=1,113,449 step_tokens=130,834 waste=0.2% h100_mfu=27.09% vram=396/20,848MB peak=20,251/20,848MB
[step 211/8250] loss=2.6308 lr=6.95e-05 muon_lr=6.95e-04 grad_norm=0.4446 tok/s=1,122,897 raw_tok/s=1,125,223 step_tokens=130,801 waste=0.2% h100_mfu=27.38% vram=396/20,848MB peak=20,251/20,848MB
[step 212/8250] loss=2.6283 lr=6.99e-05 muon_lr=6.99e-04 grad_norm=0.4993 tok/s=1,112,658 raw_tok/s=1,113,252 step_tokens=131,002 waste=0.1% h100_mfu=27.09% vram=396/20,848MB peak=20,251/20,848MB
[step 213/8250] loss=2.6245 lr=7.02e-05 muon_lr=7.02e-04 grad_norm=0.4715 tok/s=1,169,102 raw_tok/s=1,173,390 step_tokens=130,593 waste=0.4% h100_mfu=28.55% vram=396/20,848MB peak=20,251/20,848MB
[step 214/8250] loss=2.6378 lr=7.05e-05 muon_lr=7.05e-04 grad_norm=0.4641 tok/s=1,197,884 raw_tok/s=1,200,945 step_tokens=130,738 waste=0.3% h100_mfu=29.22% vram=396/20,848MB peak=20,251/20,848MB
[step 215/8250] loss=2.6202 lr=7.09e-05 muon_lr=7.09e-04 grad_norm=0.4682 tok/s=1,232,376 raw_tok/s=1,234,099 step_tokens=130,889 waste=0.1% h100_mfu=30.03% vram=396/20,848MB peak=20,251/20,848MB
[step 216/8250] loss=2.6250 lr=7.12e-05 muon_lr=7.12e-04 grad_norm=0.5316 tok/s=1,180,440 raw_tok/s=1,187,398 step_tokens=130,304 waste=0.6% h100_mfu=28.89% vram=396/20,848MB peak=20,251/20,848MB
[step 217/8250] loss=2.6316 lr=7.15e-05 muon_lr=7.15e-04 grad_norm=0.3926 tok/s=1,038,635 raw_tok/s=1,040,469 step_tokens=130,841 waste=0.2% h100_mfu=25.31% vram=396/20,848MB peak=20,251/20,848MB
[step 218/8250] loss=2.6311 lr=7.19e-05 muon_lr=7.19e-04 grad_norm=0.4761 tok/s=1,186,899 raw_tok/s=1,190,505 step_tokens=130,675 waste=0.3% h100_mfu=28.97% vram=396/20,848MB peak=20,251/20,848MB
[step 219/8250] loss=2.6386 lr=7.22e-05 muon_lr=7.22e-04 grad_norm=0.5319 tok/s=1,215,086 raw_tok/s=1,215,634 step_tokens=131,013 waste=0.0% h100_mfu=29.58% vram=396/20,848MB peak=20,251/20,848MB
[step 220/8250] loss=2.6357 lr=7.25e-05 muon_lr=7.25e-04 grad_norm=0.4089 tok/s=1,207,606 raw_tok/s=1,213,848 step_tokens=130,398 waste=0.5% h100_mfu=29.53% vram=396/20,848MB peak=20,251/20,848MB
[step 221/8250] loss=2.6334 lr=7.28e-05 muon_lr=7.28e-04 grad_norm=0.5179 tok/s=1,137,283 raw_tok/s=1,138,151 step_tokens=130,972 waste=0.1% h100_mfu=27.69% vram=396/20,848MB peak=20,251/20,848MB
[step 222/8250] loss=2.6179 lr=7.32e-05 muon_lr=7.32e-04 grad_norm=0.4968 tok/s=1,167,398 raw_tok/s=1,170,533 step_tokens=130,721 waste=0.3% h100_mfu=28.48% vram=396/20,848MB peak=20,251/20,848MB
[step 223/8250] loss=2.6319 lr=7.35e-05 muon_lr=7.35e-04 grad_norm=0.5395 tok/s=1,105,164 raw_tok/s=1,108,004 step_tokens=130,736 waste=0.3% h100_mfu=26.96% vram=396/20,848MB peak=20,251/20,848MB
[step 224/8250] loss=2.6330 lr=7.38e-05 muon_lr=7.38e-04 grad_norm=0.5922 tok/s=1,233,337 raw_tok/s=1,234,590 step_tokens=130,939 waste=0.1% h100_mfu=30.04% vram=396/20,848MB peak=20,251/20,848MB
[step 225/8250] loss=2.6225 lr=7.42e-05 muon_lr=7.42e-04 grad_norm=0.5332 tok/s=1,187,024 raw_tok/s=1,190,448 step_tokens=130,695 waste=0.3% h100_mfu=28.96% vram=396/20,848MB peak=20,251/20,848MB
[step 226/8250] loss=2.6266 lr=7.45e-05 muon_lr=7.45e-04 grad_norm=0.5298 tok/s=1,232,924 raw_tok/s=1,235,676 step_tokens=130,780 waste=0.2% h100_mfu=30.06% vram=396/20,848MB peak=20,251/20,848MB
[step 227/8250] loss=2.6270 lr=7.48e-05 muon_lr=7.48e-04 grad_norm=0.3749 tok/s=1,200,527 raw_tok/s=1,202,609 step_tokens=130,845 waste=0.2% h100_mfu=29.26% vram=396/20,848MB peak=20,251/20,848MB
[step 228/8250] loss=2.6113 lr=7.52e-05 muon_lr=7.52e-04 grad_norm=0.4196 tok/s=1,198,417 raw_tok/s=1,202,141 step_tokens=130,666 waste=0.3% h100_mfu=29.25% vram=396/20,848MB peak=20,251/20,848MB
[step 229/8250] loss=2.6289 lr=7.55e-05 muon_lr=7.55e-04 grad_norm=0.5858 tok/s=1,185,465 raw_tok/s=1,191,648 step_tokens=130,392 waste=0.5% h100_mfu=28.99% vram=396/20,848MB peak=20,251/20,848MB
[step 230/8250] loss=2.6280 lr=7.58e-05 muon_lr=7.58e-04 grad_norm=0.4593 tok/s=1,135,145 raw_tok/s=1,140,776 step_tokens=130,425 waste=0.5% h100_mfu=27.76% vram=396/20,848MB peak=20,251/20,848MB
[step 231/8250] loss=2.6254 lr=7.62e-05 muon_lr=7.62e-04 grad_norm=0.5251 tok/s=1,190,194 raw_tok/s=1,191,367 step_tokens=130,943 waste=0.1% h100_mfu=28.99% vram=396/20,848MB peak=20,251/20,848MB
[step 232/8250] loss=2.6340 lr=7.65e-05 muon_lr=7.65e-04 grad_norm=0.5913 tok/s=1,231,332 raw_tok/s=1,232,639 step_tokens=130,933 waste=0.1% h100_mfu=29.99% vram=396/20,848MB peak=20,251/20,848MB
[step 233/8250] loss=2.6329 lr=7.68e-05 muon_lr=7.68e-04 grad_norm=0.5141 tok/s=1,177,470 raw_tok/s=1,181,798 step_tokens=130,592 waste=0.4% h100_mfu=28.75% vram=396/20,848MB peak=20,251/20,848MB
[step 234/8250] loss=2.6298 lr=7.72e-05 muon_lr=7.72e-04 grad_norm=0.4864 tok/s=1,233,758 raw_tok/s=1,236,370 step_tokens=130,795 waste=0.2% h100_mfu=30.08% vram=396/20,848MB peak=20,251/20,848MB
[step 235/8250] loss=2.6233 lr=7.75e-05 muon_lr=7.75e-04 grad_norm=0.4980 tok/s=1,084,734 raw_tok/s=1,088,163 step_tokens=130,659 waste=0.3% h100_mfu=26.48% vram=396/20,848MB peak=20,251/20,848MB
[step 236/8250] loss=2.6162 lr=7.78e-05 muon_lr=7.78e-04 grad_norm=0.4719 tok/s=1,182,627 raw_tok/s=1,185,984 step_tokens=130,701 waste=0.3% h100_mfu=28.86% vram=396/20,848MB peak=20,251/20,848MB
[step 237/8250] loss=2.6220 lr=7.81e-05 muon_lr=7.81e-04 grad_norm=0.4853 tok/s=1,153,000 raw_tok/s=1,155,592 step_tokens=130,778 waste=0.2% h100_mfu=28.12% vram=396/20,848MB peak=20,251/20,848MB
[step 238/8250] loss=2.6292 lr=7.85e-05 muon_lr=7.85e-04 grad_norm=0.5545 tok/s=1,190,350 raw_tok/s=1,190,841 step_tokens=131,018 waste=0.0% h100_mfu=28.97% vram=396/20,848MB peak=20,251/20,848MB
[step 239/8250] loss=2.6302 lr=7.88e-05 muon_lr=7.88e-04 grad_norm=0.4375 tok/s=1,189,221 raw_tok/s=1,191,849 step_tokens=130,783 waste=0.2% h100_mfu=29.00% vram=396/20,848MB peak=20,251/20,848MB
[step 240/8250] loss=2.6305 lr=7.91e-05 muon_lr=7.91e-04 grad_norm=0.4748 tok/s=1,223,703 raw_tok/s=1,229,142 step_tokens=130,492 waste=0.4% h100_mfu=29.91% vram=396/20,848MB peak=20,251/20,848MB
[step 241/8250] loss=2.6239 lr=7.95e-05 muon_lr=7.95e-04 grad_norm=0.5894 tok/s=1,183,809 raw_tok/s=1,185,872 step_tokens=130,844 waste=0.2% h100_mfu=28.85% vram=396/20,848MB peak=20,251/20,848MB
[step 242/8250] loss=2.6295 lr=7.98e-05 muon_lr=7.98e-04 grad_norm=0.4854 tok/s=1,169,427 raw_tok/s=1,171,393 step_tokens=130,852 waste=0.2% h100_mfu=28.50% vram=396/20,848MB peak=20,251/20,848MB
[step 243/8250] loss=2.6111 lr=8.01e-05 muon_lr=8.01e-04 grad_norm=0.4512 tok/s=1,183,423 raw_tok/s=1,187,546 step_tokens=130,617 waste=0.3% h100_mfu=28.89% vram=396/20,848MB peak=20,251/20,848MB
[step 244/8250] loss=2.6113 lr=8.05e-05 muon_lr=8.05e-04 grad_norm=0.5003 tok/s=1,161,552 raw_tok/s=1,164,360 step_tokens=130,756 waste=0.2% h100_mfu=28.33% vram=396/20,848MB peak=20,251/20,848MB
[step 245/8250] loss=2.6333 lr=8.08e-05 muon_lr=8.08e-04 grad_norm=0.4355 tok/s=1,173,918 raw_tok/s=1,176,503 step_tokens=130,784 waste=0.2% h100_mfu=28.62% vram=396/20,848MB peak=20,251/20,848MB
[step 246/8250] loss=2.6333 lr=8.11e-05 muon_lr=8.11e-04 grad_norm=0.4964 tok/s=1,124,619 raw_tok/s=1,127,768 step_tokens=130,706 waste=0.3% h100_mfu=27.44% vram=396/20,848MB peak=20,251/20,848MB
[step 247/8250] loss=2.6233 lr=8.15e-05 muon_lr=8.15e-04 grad_norm=0.5636 tok/s=1,066,392 raw_tok/s=1,068,560 step_tokens=130,806 waste=0.2% h100_mfu=26.00% vram=396/20,848MB peak=20,251/20,848MB
[step 248/8250] loss=2.6029 lr=8.18e-05 muon_lr=8.18e-04 grad_norm=0.4648 tok/s=1,159,425 raw_tok/s=1,161,233 step_tokens=130,868 waste=0.2% h100_mfu=28.25% vram=396/20,848MB peak=20,251/20,848MB
[step 249/8250] loss=2.6352 lr=8.21e-05 muon_lr=8.21e-04 grad_norm=0.4762 tok/s=1,200,369 raw_tok/s=1,204,523 step_tokens=130,620 waste=0.3% h100_mfu=29.31% vram=396/20,848MB peak=20,251/20,848MB
[step 250/8250] loss=2.6376 lr=8.25e-05 muon_lr=8.25e-04 grad_norm=0.4126 tok/s=1,232,820 raw_tok/s=1,235,629 step_tokens=130,774 waste=0.2% h100_mfu=30.06% vram=396/20,848MB peak=20,251/20,848MB
[step 250] eval_loss=2.6231
[step 251/8250] loss=2.6228 lr=8.28e-05 muon_lr=8.28e-04 grad_norm=0.4573 tok/s=1,245,971 raw_tok/s=1,246,694 step_tokens=130,996 waste=0.1% h100_mfu=30.33% vram=396/20,850MB peak=20,255/20,850MB
[step 252/8250] loss=2.6314 lr=8.31e-05 muon_lr=8.31e-04 grad_norm=0.6012 tok/s=1,173,837 raw_tok/s=1,177,439 step_tokens=130,671 waste=0.3% h100_mfu=28.65% vram=396/20,850MB peak=20,252/20,850MB
[step 253/8250] loss=2.6202 lr=8.34e-05 muon_lr=8.34e-04 grad_norm=0.5025 tok/s=1,190,597 raw_tok/s=1,192,298 step_tokens=130,885 waste=0.1% h100_mfu=29.01% vram=396/20,850MB peak=20,251/20,850MB
[step 254/8250] loss=2.6180 lr=8.38e-05 muon_lr=8.38e-04 grad_norm=0.4423 tok/s=1,163,157 raw_tok/s=1,163,868 step_tokens=130,992 waste=0.1% h100_mfu=28.32% vram=396/20,850MB peak=20,251/20,850MB
[step 255/8250] loss=2.6327 lr=8.41e-05 muon_lr=8.41e-04 grad_norm=0.5068 tok/s=1,227,484 raw_tok/s=1,232,146 step_tokens=130,576 waste=0.4% h100_mfu=29.98% vram=396/20,850MB peak=20,251/20,850MB
[step 256/8250] loss=2.6096 lr=8.44e-05 muon_lr=8.44e-04 grad_norm=0.5497 tok/s=1,033,200 raw_tok/s=1,036,608 step_tokens=130,641 waste=0.3% h100_mfu=25.22% vram=396/20,850MB peak=20,251/20,850MB
[step 257/8250] loss=2.6157 lr=8.48e-05 muon_lr=8.48e-04 grad_norm=0.5171 tok/s=1,187,839 raw_tok/s=1,190,810 step_tokens=130,745 waste=0.2% h100_mfu=28.97% vram=396/20,850MB peak=20,251/20,850MB
[step 258/8250] loss=2.6239 lr=8.51e-05 muon_lr=8.51e-04 grad_norm=0.5490 tok/s=1,183,687 raw_tok/s=1,188,693 step_tokens=130,520 waste=0.4% h100_mfu=28.92% vram=396/20,850MB peak=20,251/20,850MB
[step 259/8250] loss=2.6082 lr=8.54e-05 muon_lr=8.54e-04 grad_norm=0.4524 tok/s=1,186,987 raw_tok/s=1,190,393 step_tokens=130,697 waste=0.3% h100_mfu=28.96% vram=396/20,850MB peak=20,251/20,850MB
[step 260/8250] loss=2.6249 lr=8.58e-05 muon_lr=8.58e-04 grad_norm=0.4307 tok/s=1,082,974 raw_tok/s=1,086,373 step_tokens=130,662 waste=0.3% h100_mfu=26.43% vram=396/20,850MB peak=20,251/20,850MB
[step 261/8250] loss=2.6155 lr=8.61e-05 muon_lr=8.61e-04 grad_norm=0.4402 tok/s=1,090,556 raw_tok/s=1,093,961 step_tokens=130,664 waste=0.3% h100_mfu=26.62% vram=396/20,850MB peak=20,251/20,850MB
[step 262/8250] loss=2.6190 lr=8.64e-05 muon_lr=8.64e-04 grad_norm=0.3385 tok/s=1,188,000 raw_tok/s=1,192,330 step_tokens=130,596 waste=0.4% h100_mfu=29.01% vram=396/20,850MB peak=20,251/20,850MB
[step 263/8250] loss=2.6202 lr=8.68e-05 muon_lr=8.68e-04 grad_norm=0.5434 tok/s=1,194,152 raw_tok/s=1,195,858 step_tokens=130,885 waste=0.1% h100_mfu=29.10% vram=396/20,850MB peak=20,251/20,850MB
[step 264/8250] loss=2.6155 lr=8.71e-05 muon_lr=8.71e-04 grad_norm=0.4705 tok/s=1,187,420 raw_tok/s=1,189,389 step_tokens=130,855 waste=0.2% h100_mfu=28.94% vram=396/20,850MB peak=20,251/20,850MB
[step 265/8250] loss=2.6282 lr=8.74e-05 muon_lr=8.74e-04 grad_norm=0.4987 tok/s=1,182,298 raw_tok/s=1,184,766 step_tokens=130,799 waste=0.2% h100_mfu=28.83% vram=396/20,850MB peak=20,251/20,850MB
[step 266/8250] loss=2.6147 lr=8.77e-05 muon_lr=8.77e-04 grad_norm=0.5672 tok/s=1,187,043 raw_tok/s=1,188,503 step_tokens=130,911 waste=0.1% h100_mfu=28.92% vram=396/20,850MB peak=20,251/20,850MB
[step 267/8250] loss=2.6296 lr=8.81e-05 muon_lr=8.81e-04 grad_norm=0.3392 tok/s=1,056,163 raw_tok/s=1,062,478 step_tokens=130,293 waste=0.6% h100_mfu=25.85% vram=396/20,850MB peak=20,251/20,850MB
[step 268/8250] loss=2.6190 lr=8.84e-05 muon_lr=8.84e-04 grad_norm=0.3398 tok/s=1,188,791 raw_tok/s=1,191,573 step_tokens=130,766 waste=0.2% h100_mfu=28.99% vram=396/20,850MB peak=20,251/20,850MB
[step 269/8250] loss=2.6276 lr=8.87e-05 muon_lr=8.87e-04 grad_norm=0.4294 tok/s=1,191,865 raw_tok/s=1,194,782 step_tokens=130,752 waste=0.2% h100_mfu=29.07% vram=396/20,850MB peak=20,251/20,850MB
[step 270/8250] loss=2.6189 lr=8.91e-05 muon_lr=8.91e-04 grad_norm=0.5311 tok/s=1,187,817 raw_tok/s=1,191,608 step_tokens=130,655 waste=0.3% h100_mfu=28.99% vram=396/20,850MB peak=20,251/20,850MB
[step 271/8250] loss=2.6272 lr=8.94e-05 muon_lr=8.94e-04 grad_norm=0.4573 tok/s=1,203,318 raw_tok/s=1,208,055 step_tokens=130,558 waste=0.4% h100_mfu=29.39% vram=396/20,850MB peak=20,251/20,850MB
[step 272/8250] loss=2.6194 lr=8.97e-05 muon_lr=8.97e-04 grad_norm=0.5858 tok/s=1,180,939 raw_tok/s=1,182,383 step_tokens=130,912 waste=0.1% h100_mfu=28.77% vram=396/20,850MB peak=20,251/20,850MB
[step 273/8250] loss=2.6096 lr=9.01e-05 muon_lr=9.01e-04 grad_norm=0.5064 tok/s=1,161,719 raw_tok/s=1,163,387 step_tokens=130,884 waste=0.1% h100_mfu=28.31% vram=396/20,850MB peak=20,251/20,850MB
[step 274/8250] loss=2.6201 lr=9.04e-05 muon_lr=9.04e-04 grad_norm=0.6244 tok/s=1,161,269 raw_tok/s=1,164,289 step_tokens=130,732 waste=0.3% h100_mfu=28.33% vram=396/20,850MB peak=20,251/20,850MB
[step 275/8250] loss=2.6354 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.5071 tok/s=1,235,981 raw_tok/s=1,238,106 step_tokens=130,847 waste=0.2% h100_mfu=30.12% vram=396/20,850MB peak=20,251/20,850MB
[step 276/8250] loss=2.6199 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.6730 tok/s=1,174,416 raw_tok/s=1,174,766 step_tokens=131,033 waste=0.0% h100_mfu=28.58% vram=396/20,850MB peak=20,251/20,850MB
[step 277/8250] loss=2.6209 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.5964 tok/s=1,191,507 raw_tok/s=1,192,362 step_tokens=130,978 waste=0.1% h100_mfu=29.01% vram=396/20,850MB peak=20,251/20,850MB
[step 278/8250] loss=2.6035 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.4014 tok/s=1,189,393 raw_tok/s=1,193,025 step_tokens=130,673 waste=0.3% h100_mfu=29.03% vram=396/20,850MB peak=20,251/20,850MB
[step 279/8250] loss=2.6354 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.5043 tok/s=1,166,371 raw_tok/s=1,167,600 step_tokens=130,934 waste=0.1% h100_mfu=28.41% vram=396/20,850MB peak=20,251/20,850MB
[step 280/8250] loss=2.6197 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.5121 tok/s=1,173,633 raw_tok/s=1,175,480 step_tokens=130,866 waste=0.2% h100_mfu=28.60% vram=396/20,850MB peak=20,251/20,850MB
[step 281/8250] loss=2.6167 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.5373 tok/s=1,155,443 raw_tok/s=1,158,501 step_tokens=130,726 waste=0.3% h100_mfu=28.19% vram=396/20,850MB peak=20,251/20,850MB
[step 282/8250] loss=2.6176 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3490 tok/s=1,182,715 raw_tok/s=1,185,700 step_tokens=130,742 waste=0.3% h100_mfu=28.85% vram=396/20,850MB peak=20,251/20,850MB
[step 283/8250] loss=2.6123 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3113 tok/s=1,186,256 raw_tok/s=1,190,918 step_tokens=130,559 waste=0.4% h100_mfu=28.98% vram=396/20,850MB peak=20,251/20,850MB
[step 284/8250] loss=2.6182 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.4768 tok/s=1,239,305 raw_tok/s=1,240,857 step_tokens=130,908 waste=0.1% h100_mfu=30.19% vram=396/20,850MB peak=20,251/20,850MB
[step 285/8250] loss=2.6259 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.4374 tok/s=1,229,761 raw_tok/s=1,231,688 step_tokens=130,867 waste=0.2% h100_mfu=29.97% vram=396/20,850MB peak=20,251/20,850MB
[step 286/8250] loss=2.6081 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.4900 tok/s=1,162,303 raw_tok/s=1,166,147 step_tokens=130,640 waste=0.3% h100_mfu=28.37% vram=396/20,850MB peak=20,251/20,850MB
[step 287/8250] loss=2.6166 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3804 tok/s=1,232,663 raw_tok/s=1,234,208 step_tokens=130,908 waste=0.1% h100_mfu=30.03% vram=396/20,850MB peak=20,251/20,850MB
[step 288/8250] loss=2.6233 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3881 tok/s=1,183,383 raw_tok/s=1,186,052 step_tokens=130,777 waste=0.2% h100_mfu=28.86% vram=396/20,850MB peak=20,251/20,850MB
[step 289/8250] loss=2.6147 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.4328 tok/s=1,228,277 raw_tok/s=1,229,929 step_tokens=130,896 waste=0.1% h100_mfu=29.92% vram=396/20,850MB peak=20,251/20,850MB
[step 290/8250] loss=2.6095 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.5398 tok/s=1,159,099 raw_tok/s=1,161,749 step_tokens=130,773 waste=0.2% h100_mfu=28.27% vram=396/20,850MB peak=20,251/20,850MB
[step 291/8250] loss=2.6162 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.5430 tok/s=1,231,207 raw_tok/s=1,232,843 step_tokens=130,898 waste=0.1% h100_mfu=30.00% vram=396/20,850MB peak=20,251/20,850MB
[step 292/8250] loss=2.6235 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.4564 tok/s=1,174,519 raw_tok/s=1,177,240 step_tokens=130,769 waste=0.2% h100_mfu=28.64% vram=396/20,850MB peak=20,251/20,850MB
[step 293/8250] loss=2.6233 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.4640 tok/s=1,159,429 raw_tok/s=1,164,422 step_tokens=130,510 waste=0.4% h100_mfu=28.33% vram=396/20,850MB peak=20,251/20,850MB
[step 294/8250] loss=2.6157 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.5704 tok/s=1,156,861 raw_tok/s=1,159,684 step_tokens=130,753 waste=0.2% h100_mfu=28.22% vram=396/20,850MB peak=20,251/20,850MB
[step 295/8250] loss=2.6221 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.4703 tok/s=1,168,205 raw_tok/s=1,171,172 step_tokens=130,740 waste=0.3% h100_mfu=28.49% vram=396/20,850MB peak=20,251/20,850MB
[step 296/8250] loss=2.6187 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.4491 tok/s=1,182,049 raw_tok/s=1,184,172 step_tokens=130,837 waste=0.2% h100_mfu=28.81% vram=396/20,850MB peak=20,251/20,850MB
[step 297/8250] loss=2.6140 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.4432 tok/s=1,227,211 raw_tok/s=1,231,505 step_tokens=130,615 waste=0.3% h100_mfu=29.96% vram=396/20,850MB peak=20,251/20,850MB
[step 298/8250] loss=2.6141 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.4911 tok/s=1,221,457 raw_tok/s=1,224,662 step_tokens=130,729 waste=0.3% h100_mfu=29.80% vram=396/20,850MB peak=20,251/20,850MB
[step 299/8250] loss=2.6233 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.5062 tok/s=1,183,091 raw_tok/s=1,188,540 step_tokens=130,471 waste=0.5% h100_mfu=28.92% vram=396/20,850MB peak=20,251/20,850MB
[step 300/8250] loss=2.5986 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.4456 tok/s=1,178,838 raw_tok/s=1,183,252 step_tokens=130,583 waste=0.4% h100_mfu=28.79% vram=396/20,850MB peak=20,251/20,850MB
[step 301/8250] loss=2.6186 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.4531 tok/s=1,130,904 raw_tok/s=1,134,600 step_tokens=130,645 waste=0.3% h100_mfu=27.60% vram=396/20,850MB peak=20,251/20,850MB
[step 302/8250] loss=2.5928 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.4661 tok/s=1,115,513 raw_tok/s=1,117,030 step_tokens=130,894 waste=0.1% h100_mfu=27.18% vram=396/20,850MB peak=20,251/20,850MB
[step 303/8250] loss=2.6037 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3421 tok/s=1,183,254 raw_tok/s=1,186,903 step_tokens=130,669 waste=0.3% h100_mfu=28.88% vram=396/20,850MB peak=20,251/20,850MB
[step 304/8250] loss=2.6121 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3475 tok/s=1,231,033 raw_tok/s=1,232,500 step_tokens=130,916 waste=0.1% h100_mfu=29.99% vram=396/20,850MB peak=20,251/20,850MB
[step 305/8250] loss=2.6159 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.4195 tok/s=1,185,417 raw_tok/s=1,190,832 step_tokens=130,476 waste=0.5% h100_mfu=28.97% vram=396/20,850MB peak=20,251/20,850MB
[step 306/8250] loss=2.6227 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3129 tok/s=1,159,139 raw_tok/s=1,165,094 step_tokens=130,402 waste=0.5% h100_mfu=28.35% vram=396/20,850MB peak=20,251/20,850MB
[step 307/8250] loss=2.5977 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3956 tok/s=1,180,589 raw_tok/s=1,183,424 step_tokens=130,758 waste=0.2% h100_mfu=28.79% vram=396/20,850MB peak=20,251/20,850MB
[step 308/8250] loss=2.6112 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4567 tok/s=1,183,869 raw_tok/s=1,185,832 step_tokens=130,855 waste=0.2% h100_mfu=28.85% vram=396/20,850MB peak=20,251/20,850MB
[step 309/8250] loss=2.6183 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4354 tok/s=1,181,401 raw_tok/s=1,185,752 step_tokens=130,591 waste=0.4% h100_mfu=28.85% vram=396/20,850MB peak=20,251/20,850MB
[step 310/8250] loss=2.6208 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4315 tok/s=1,233,033 raw_tok/s=1,237,054 step_tokens=130,646 waste=0.3% h100_mfu=30.10% vram=396/20,850MB peak=20,251/20,850MB
[step 311/8250] loss=2.6166 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4330 tok/s=1,187,905 raw_tok/s=1,188,766 step_tokens=130,977 waste=0.1% h100_mfu=28.92% vram=396/20,850MB peak=20,251/20,850MB
[step 312/8250] loss=2.6189 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.5375 tok/s=1,190,122 raw_tok/s=1,193,537 step_tokens=130,697 waste=0.3% h100_mfu=29.04% vram=396/20,850MB peak=20,251/20,850MB
[step 313/8250] loss=2.6171 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.3676 tok/s=1,213,344 raw_tok/s=1,215,737 step_tokens=130,814 waste=0.2% h100_mfu=29.58% vram=396/20,850MB peak=20,251/20,850MB
[step 314/8250] loss=2.6127 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4315 tok/s=1,161,800 raw_tok/s=1,164,946 step_tokens=130,718 waste=0.3% h100_mfu=28.34% vram=396/20,850MB peak=20,251/20,850MB
[step 315/8250] loss=2.6057 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.3656 tok/s=1,189,293 raw_tok/s=1,192,250 step_tokens=130,747 waste=0.2% h100_mfu=29.01% vram=396/20,850MB peak=20,251/20,850MB
[step 316/8250] loss=2.6123 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.2903 tok/s=1,128,380 raw_tok/s=1,130,380 step_tokens=130,840 waste=0.2% h100_mfu=27.50% vram=396/20,850MB peak=20,251/20,850MB
[step 317/8250] loss=2.6120 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4505 tok/s=1,224,318 raw_tok/s=1,227,512 step_tokens=130,731 waste=0.3% h100_mfu=29.87% vram=396/20,850MB peak=20,251/20,850MB
[step 318/8250] loss=2.6141 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3918 tok/s=1,183,297 raw_tok/s=1,186,139 step_tokens=130,758 waste=0.2% h100_mfu=28.86% vram=396/20,850MB peak=20,251/20,850MB
[step 319/8250] loss=2.6037 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4812 tok/s=1,186,931 raw_tok/s=1,191,257 step_tokens=130,596 waste=0.4% h100_mfu=28.98% vram=396/20,850MB peak=20,251/20,850MB
[step 320/8250] loss=2.6038 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4815 tok/s=1,156,222 raw_tok/s=1,156,699 step_tokens=131,018 waste=0.0% h100_mfu=28.14% vram=396/20,850MB peak=20,251/20,850MB
[step 321/8250] loss=2.6134 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.5230 tok/s=1,238,645 raw_tok/s=1,239,486 step_tokens=130,983 waste=0.1% h100_mfu=30.16% vram=396/20,850MB peak=20,251/20,850MB
[step 322/8250] loss=2.6024 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4651 tok/s=1,064,733 raw_tok/s=1,065,611 step_tokens=130,964 waste=0.1% h100_mfu=25.93% vram=396/20,850MB peak=20,251/20,850MB
[step 323/8250] loss=2.6045 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.5157 tok/s=1,220,016 raw_tok/s=1,222,441 step_tokens=130,812 waste=0.2% h100_mfu=29.74% vram=396/20,850MB peak=20,251/20,850MB
[step 324/8250] loss=2.6133 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4714 tok/s=1,177,406 raw_tok/s=1,179,926 step_tokens=130,792 waste=0.2% h100_mfu=28.71% vram=396/20,850MB peak=20,251/20,850MB
[step 325/8250] loss=2.6176 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3496 tok/s=1,186,073 raw_tok/s=1,189,613 step_tokens=130,682 waste=0.3% h100_mfu=28.94% vram=396/20,850MB peak=20,251/20,850MB
[step 326/8250] loss=2.6236 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.4899 tok/s=1,182,322 raw_tok/s=1,183,875 step_tokens=130,900 waste=0.1% h100_mfu=28.80% vram=396/20,850MB peak=20,251/20,850MB
[step 327/8250] loss=2.6088 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3973 tok/s=1,164,907 raw_tok/s=1,168,732 step_tokens=130,643 waste=0.3% h100_mfu=28.44% vram=396/20,850MB peak=20,251/20,850MB
[step 328/8250] loss=2.6156 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3896 tok/s=1,031,552 raw_tok/s=1,036,558 step_tokens=130,439 waste=0.5% h100_mfu=25.22% vram=396/20,850MB peak=20,251/20,850MB
[step 329/8250] loss=2.6045 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3149 tok/s=1,217,509 raw_tok/s=1,219,686 step_tokens=130,838 waste=0.2% h100_mfu=29.68% vram=396/20,850MB peak=20,251/20,850MB
[step 330/8250] loss=2.6064 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3474 tok/s=1,234,758 raw_tok/s=1,240,959 step_tokens=130,417 waste=0.5% h100_mfu=30.19% vram=396/20,850MB peak=20,251/20,850MB
[step 331/8250] loss=2.6306 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3739 tok/s=1,191,196 raw_tok/s=1,192,233 step_tokens=130,958 waste=0.1% h100_mfu=29.01% vram=396/20,850MB peak=20,251/20,850MB
[step 332/8250] loss=2.6223 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.2815 tok/s=1,171,161 raw_tok/s=1,173,597 step_tokens=130,800 waste=0.2% h100_mfu=28.55% vram=396/20,850MB peak=20,251/20,850MB
[step 333/8250] loss=2.6049 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.4428 tok/s=1,179,991 raw_tok/s=1,180,658 step_tokens=130,998 waste=0.1% h100_mfu=28.73% vram=396/20,850MB peak=20,251/20,850MB
[step 334/8250] loss=2.6048 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3430 tok/s=1,175,657 raw_tok/s=1,177,381 step_tokens=130,880 waste=0.1% h100_mfu=28.65% vram=396/20,850MB peak=20,251/20,850MB
[step 335/8250] loss=2.6001 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.2982 tok/s=1,143,328 raw_tok/s=1,145,154 step_tokens=130,863 waste=0.2% h100_mfu=27.86% vram=396/20,850MB peak=20,251/20,850MB
[step 336/8250] loss=2.6139 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3413 tok/s=1,173,308 raw_tok/s=1,177,287 step_tokens=130,629 waste=0.3% h100_mfu=28.64% vram=396/20,850MB peak=20,251/20,850MB
[step 337/8250] loss=2.6066 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.4093 tok/s=1,174,421 raw_tok/s=1,178,973 step_tokens=130,566 waste=0.4% h100_mfu=28.68% vram=396/20,850MB peak=20,251/20,850MB
[step 338/8250] loss=2.6053 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3330 tok/s=1,191,681 raw_tok/s=1,194,945 step_tokens=130,714 waste=0.3% h100_mfu=29.07% vram=396/20,850MB peak=20,251/20,850MB
[step 339/8250] loss=2.6032 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.2999 tok/s=1,213,435 raw_tok/s=1,215,419 step_tokens=130,858 waste=0.2% h100_mfu=29.57% vram=396/20,850MB peak=20,251/20,850MB
[step 340/8250] loss=2.5909 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3171 tok/s=1,123,898 raw_tok/s=1,126,554 step_tokens=130,763 waste=0.2% h100_mfu=27.41% vram=396/20,850MB peak=20,251/20,850MB
[step 341/8250] loss=2.5972 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3588 tok/s=1,125,160 raw_tok/s=1,131,627 step_tokens=130,323 waste=0.6% h100_mfu=27.53% vram=396/20,850MB peak=20,251/20,850MB
[step 342/8250] loss=2.6077 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3803 tok/s=1,184,442 raw_tok/s=1,185,672 step_tokens=130,936 waste=0.1% h100_mfu=28.85% vram=396/20,850MB peak=20,251/20,850MB
[step 343/8250] loss=2.5824 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3279 tok/s=1,185,383 raw_tok/s=1,187,212 step_tokens=130,870 waste=0.2% h100_mfu=28.88% vram=396/20,850MB peak=20,251/20,850MB
[step 344/8250] loss=2.6069 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3001 tok/s=1,235,737 raw_tok/s=1,240,298 step_tokens=130,590 waste=0.4% h100_mfu=30.18% vram=396/20,850MB peak=20,251/20,850MB
[step 345/8250] loss=2.6046 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.2634 tok/s=1,194,817 raw_tok/s=1,196,589 step_tokens=130,878 waste=0.1% h100_mfu=29.11% vram=396/20,850MB peak=20,251/20,850MB
[step 346/8250] loss=2.6013 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4080 tok/s=1,227,477 raw_tok/s=1,230,829 step_tokens=130,715 waste=0.3% h100_mfu=29.95% vram=396/20,850MB peak=20,251/20,850MB
[step 347/8250] loss=2.6145 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3837 tok/s=1,062,233 raw_tok/s=1,065,517 step_tokens=130,668 waste=0.3% h100_mfu=25.92% vram=396/20,850MB peak=20,251/20,850MB
[step 348/8250] loss=2.6014 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4247 tok/s=1,124,884 raw_tok/s=1,127,335 step_tokens=130,787 waste=0.2% h100_mfu=27.43% vram=396/20,850MB peak=20,251/20,850MB
[step 349/8250] loss=2.6039 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3783 tok/s=1,192,239 raw_tok/s=1,194,070 step_tokens=130,871 waste=0.2% h100_mfu=29.05% vram=396/20,850MB peak=20,251/20,850MB
[step 350/8250] loss=2.6158 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3454 tok/s=1,170,783 raw_tok/s=1,174,708 step_tokens=130,634 waste=0.3% h100_mfu=28.58% vram=396/20,850MB peak=20,251/20,850MB
[step 351/8250] loss=2.6246 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3538 tok/s=1,224,467 raw_tok/s=1,224,990 step_tokens=131,016 waste=0.0% h100_mfu=29.80% vram=396/20,850MB peak=20,251/20,850MB
[step 352/8250] loss=2.6001 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.4178 tok/s=1,053,444 raw_tok/s=1,056,160 step_tokens=130,735 waste=0.3% h100_mfu=25.70% vram=396/20,850MB peak=20,251/20,850MB
[step 353/8250] loss=2.6100 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3327 tok/s=1,192,944 raw_tok/s=1,197,475 step_tokens=130,576 waste=0.4% h100_mfu=29.13% vram=396/20,850MB peak=20,251/20,850MB
[step 354/8250] loss=2.5975 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3401 tok/s=1,106,336 raw_tok/s=1,110,513 step_tokens=130,579 waste=0.4% h100_mfu=27.02% vram=396/20,850MB peak=20,251/20,850MB
[step 355/8250] loss=2.5889 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.4796 tok/s=1,222,464 raw_tok/s=1,225,146 step_tokens=130,785 waste=0.2% h100_mfu=29.81% vram=396/20,850MB peak=20,251/20,850MB
[step 356/8250] loss=2.5990 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.2722 tok/s=1,185,399 raw_tok/s=1,190,859 step_tokens=130,471 waste=0.5% h100_mfu=28.97% vram=396/20,850MB peak=20,251/20,850MB
[step 357/8250] loss=2.6078 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.2877 tok/s=1,147,885 raw_tok/s=1,151,716 step_tokens=130,636 waste=0.3% h100_mfu=28.02% vram=396/20,850MB peak=20,251/20,850MB
[step 358/8250] loss=2.5944 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3453 tok/s=1,106,336 raw_tok/s=1,109,400 step_tokens=130,710 waste=0.3% h100_mfu=26.99% vram=396/20,850MB peak=20,251/20,850MB
[step 359/8250] loss=2.6118 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.4673 tok/s=1,151,660 raw_tok/s=1,156,761 step_tokens=130,494 waste=0.4% h100_mfu=28.14% vram=396/20,850MB peak=20,251/20,850MB
[step 360/8250] loss=2.6031 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3091 tok/s=1,193,475 raw_tok/s=1,194,122 step_tokens=131,001 waste=0.1% h100_mfu=29.05% vram=396/20,850MB peak=20,251/20,850MB
[step 361/8250] loss=2.5973 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3836 tok/s=1,183,819 raw_tok/s=1,184,950 step_tokens=130,947 waste=0.1% h100_mfu=28.83% vram=396/20,850MB peak=20,251/20,850MB
[step 362/8250] loss=2.6093 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3791 tok/s=1,229,853 raw_tok/s=1,232,637 step_tokens=130,776 waste=0.2% h100_mfu=29.99% vram=396/20,850MB peak=20,251/20,850MB
[step 363/8250] loss=2.6011 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3122 tok/s=1,169,353 raw_tok/s=1,173,902 step_tokens=130,564 waste=0.4% h100_mfu=28.56% vram=396/20,850MB peak=20,251/20,850MB
[step 364/8250] loss=2.6043 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.4794 tok/s=1,186,832 raw_tok/s=1,188,446 step_tokens=130,894 waste=0.1% h100_mfu=28.92% vram=396/20,850MB peak=20,251/20,850MB
[step 365/8250] loss=2.5968 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3852 tok/s=1,121,893 raw_tok/s=1,124,226 step_tokens=130,800 waste=0.2% h100_mfu=27.35% vram=396/20,850MB peak=20,251/20,850MB
[step 366/8250] loss=2.5941 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3604 tok/s=1,134,101 raw_tok/s=1,136,755 step_tokens=130,766 waste=0.2% h100_mfu=27.66% vram=396/20,850MB peak=20,251/20,850MB
[step 367/8250] loss=2.5896 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3189 tok/s=1,231,433 raw_tok/s=1,234,834 step_tokens=130,711 waste=0.3% h100_mfu=30.04% vram=396/20,850MB peak=20,251/20,850MB
[step 368/8250] loss=2.5978 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3414 tok/s=1,197,985 raw_tok/s=1,200,119 step_tokens=130,839 waste=0.2% h100_mfu=29.20% vram=396/20,850MB peak=20,251/20,850MB
[step 369/8250] loss=2.6003 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.4155 tok/s=1,109,158 raw_tok/s=1,110,065 step_tokens=130,965 waste=0.1% h100_mfu=27.01% vram=396/20,850MB peak=20,251/20,850MB
[step 370/8250] loss=2.5935 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.3985 tok/s=1,114,382 raw_tok/s=1,117,229 step_tokens=130,738 waste=0.3% h100_mfu=27.18% vram=396/20,850MB peak=20,251/20,850MB
[step 371/8250] loss=2.5927 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4058 tok/s=1,115,875 raw_tok/s=1,117,153 step_tokens=130,922 waste=0.1% h100_mfu=27.18% vram=396/20,850MB peak=20,251/20,850MB
W0220 02:32:14.422000 30851 torch/distributed/run.py:852] 
W0220 02:32:14.422000 30851 torch/distributed/run.py:852] *****************************************
W0220 02:32:14.422000 30851 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:32:14.422000 30851 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 9034.58it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 20919.22it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 3356.79it/s]
Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1971.01it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 71415608
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Total Trainable Parameters: 71415608
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020232
Auto-setting num_workers to 8 for 2 GPU(s).
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020232-2
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=52 tensors, resid_scalar_params=1 tensors, x0_scalar_params=1 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=52 tensors, resid_scalar_params=1 tensors, x0_scalar_params=1 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run 6pjo3q2y
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_023223-6pjo3q2y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020232-2
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/6pjo3q2y
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
W0220 02:32:33.488000 31241 torch/distributed/run.py:852] 
W0220 02:32:33.488000 31241 torch/distributed/run.py:852] *****************************************
W0220 02:32:33.488000 31241 torch/distributed/run.py:852] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0220 02:32:33.488000 31241 torch/distributed/run.py:852] *****************************************
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 70492.50it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 21103.42it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 1075.32it/s]
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 59493.67it/s]
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Validating pretraining dataset: output/data/pretrain_data
Validating training shards...
Validating 210 binary shards in train...
âœ“ All 210 binary shards validated successfully
Validating validation shards...
Validating 24 binary shards in val...
âœ“ All 24 binary shards validated successfully
âœ“ Pretraining dataset validation complete
  Train shards: 210
  Val shards: 24
  Train sequences: 419281
  Val sequences: 46586
Using Transformer Engine model and training loop
Total Trainable Parameters: 71413280
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020232-3
Auto-setting num_workers to 8 for 2 GPU(s).
Total Trainable Parameters: 71413280
Flash attention available: True
Using device: cuda
Loaded config from manifest: output/data/pretrain_data
  train_shards: output/data/pretrain_data/train
  val_shards: output/data/pretrain_data/val
  max_length: 512
  train_sequences: 419281
  val_sequences: 46586
Using ShardedDataset for pre-tokenized binary shards
Found 210 binary shards in output/data/pretrain_data/train
Loaded 419,281 pre-tokenized sequences from 210 shards
Found 24 binary shards in output/data/pretrain_data/val
Loaded 46,586 pre-tokenized sequences from 24 shards
Batch setup: target_global_batch_size=128,000 tokens, micro_step_tokens=131,072, grad_accum_steps=1, effective_global_batch_size=131,072 tokens
Created directory: output/pretraining_checkpoints/run-20020232-4
Auto-setting num_workers to 8 for 2 GPU(s).
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=28 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Sequence packing ENABLED (static input size + bucketed metadata), target=65,536 tokens, min_tokens_per_seq=21, seq_count_buckets=[32, 64, 128, 256, 512, 1024, 2048, 3121], max_seqlen_buckets=[32, 64, 128, 256, 512]
Muon grouping: muon_params=49 tensors, adamw_params=28 tensors, resid_scalar_params=0 tensors, x0_scalar_params=0 tensors
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
Capping micro-batches per epoch to 825 (from 950) to prevent distributed deadlock with variable packing
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /root/.netrc.
wandb: Currently logged in as: alint99 (alint99-manchester-metropolitan-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run b34f1kl3
wandb: Tracking run with wandb version 0.25.0
wandb: Run data is saved locally in /workspace/nanoplm/wandb/run-20260220_023242-b34f1kl3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run run-20020232-4
wandb: â­ï¸ View project at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining
wandb: ðŸš€ View run at https://wandb.ai/alint99-manchester-metropolitan-university/nanoplm-pretraining/runs/b34f1kl3
MFU estimation: 481,443,840 training FLOPs/token, H100 peak = 989.4 TFLOPS
Starting Transformer Engine training: epochs=10, total_steps=8250, warmup_steps=302, grad_accum=1, achieved_global_batch_size=131,072 tokens
Precision config: bf16=True, fp16=False, tf32=True, fp8=True
[WARNING  | DotProductAttention]: flash-attn may provide important feature support or performance improvement. Please install flash-attn >= 2.1.1, <= 2.8.3 by pip3 install flash-attn==<version>.
[step 1/8250] loss=3.8078 lr=0.00e+00 muon_lr=0.00e+00 grad_norm=8.0953 tok/s=5,642 raw_tok/s=5,666 step_tokens=130,519 waste=0.4% h100_mfu=0.14% vram=397/17,336MB peak=16,603/17,336MB
[step 2/8250] loss=3.7890 lr=3.31e-07 muon_lr=3.31e-06 grad_norm=12.1594 tok/s=1,151,720 raw_tok/s=1,154,203 step_tokens=130,790 waste=0.2% h100_mfu=28.08% vram=397/17,338MB peak=16,746/17,338MB
[step 3/8250] loss=3.7753 lr=6.62e-07 muon_lr=6.62e-06 grad_norm=11.9908 tok/s=1,478,517 raw_tok/s=1,482,102 step_tokens=130,755 waste=0.2% h100_mfu=36.06% vram=397/17,338MB peak=16,747/17,338MB
[step 4/8250] loss=3.7915 lr=9.93e-07 muon_lr=9.93e-06 grad_norm=12.0662 tok/s=1,444,360 raw_tok/s=1,446,114 step_tokens=130,913 waste=0.1% h100_mfu=35.18% vram=397/17,338MB peak=16,747/17,338MB
[step 5/8250] loss=3.7918 lr=1.32e-06 muon_lr=1.32e-05 grad_norm=12.0880 tok/s=1,521,842 raw_tok/s=1,527,330 step_tokens=130,601 waste=0.4% h100_mfu=37.16% vram=397/17,338MB peak=16,747/17,338MB
[step 6/8250] loss=3.7828 lr=1.66e-06 muon_lr=1.66e-05 grad_norm=11.9478 tok/s=1,449,413 raw_tok/s=1,453,427 step_tokens=130,710 waste=0.3% h100_mfu=35.36% vram=397/17,338MB peak=16,747/17,338MB
[step 7/8250] loss=3.7891 lr=1.99e-06 muon_lr=1.99e-05 grad_norm=12.0769 tok/s=1,451,872 raw_tok/s=1,454,279 step_tokens=130,855 waste=0.2% h100_mfu=35.38% vram=397/17,338MB peak=16,747/17,338MB
[step 8/8250] loss=3.7901 lr=2.32e-06 muon_lr=2.32e-05 grad_norm=12.1626 tok/s=1,379,783 raw_tok/s=1,383,318 step_tokens=130,737 waste=0.3% h100_mfu=33.66% vram=397/17,338MB peak=16,747/17,338MB
[step 9/8250] loss=3.7819 lr=2.65e-06 muon_lr=2.65e-05 grad_norm=12.2138 tok/s=1,501,520 raw_tok/s=1,504,297 step_tokens=130,830 waste=0.2% h100_mfu=36.60% vram=397/17,338MB peak=16,747/17,338MB
[step 10/8250] loss=3.7581 lr=2.98e-06 muon_lr=2.98e-05 grad_norm=11.8041 tok/s=1,448,035 raw_tok/s=1,450,536 step_tokens=130,846 waste=0.2% h100_mfu=35.29% vram=397/17,338MB peak=16,747/17,338MB
[step 11/8250] loss=3.7499 lr=3.31e-06 muon_lr=3.31e-05 grad_norm=11.8244 tok/s=1,419,122 raw_tok/s=1,422,911 step_tokens=130,723 waste=0.3% h100_mfu=34.62% vram=397/17,338MB peak=16,747/17,338MB
[step 12/8250] loss=3.7329 lr=3.64e-06 muon_lr=3.64e-05 grad_norm=11.6642 tok/s=1,299,131 raw_tok/s=1,300,728 step_tokens=130,911 waste=0.1% h100_mfu=31.65% vram=397/17,338MB peak=16,747/17,338MB
[step 13/8250] loss=3.7230 lr=3.97e-06 muon_lr=3.97e-05 grad_norm=11.7485 tok/s=1,425,624 raw_tok/s=1,427,911 step_tokens=130,862 waste=0.2% h100_mfu=34.74% vram=397/17,338MB peak=16,747/17,338MB
[step 14/8250] loss=3.7007 lr=4.30e-06 muon_lr=4.30e-05 grad_norm=11.6816 tok/s=1,365,083 raw_tok/s=1,371,066 step_tokens=130,500 waste=0.4% h100_mfu=33.36% vram=397/17,338MB peak=16,747/17,338MB
[step 15/8250] loss=3.6778 lr=4.64e-06 muon_lr=4.64e-05 grad_norm=11.3840 tok/s=1,354,720 raw_tok/s=1,356,914 step_tokens=130,860 waste=0.2% h100_mfu=33.01% vram=397/17,338MB peak=16,747/17,338MB
[step 16/8250] loss=3.6472 lr=4.97e-06 muon_lr=4.97e-05 grad_norm=11.1250 tok/s=1,351,195 raw_tok/s=1,351,917 step_tokens=131,002 waste=0.1% h100_mfu=32.89% vram=397/17,338MB peak=16,747/17,338MB
[step 17/8250] loss=3.6315 lr=5.30e-06 muon_lr=5.30e-05 grad_norm=11.0904 tok/s=1,501,350 raw_tok/s=1,504,185 step_tokens=130,825 waste=0.2% h100_mfu=36.60% vram=397/17,338MB peak=16,747/17,338MB
[step 18/8250] loss=3.5882 lr=5.63e-06 muon_lr=5.63e-05 grad_norm=10.6570 tok/s=1,354,439 raw_tok/s=1,358,419 step_tokens=130,688 waste=0.3% h100_mfu=33.05% vram=397/17,338MB peak=16,747/17,338MB
[step 19/8250] loss=3.5697 lr=5.96e-06 muon_lr=5.96e-05 grad_norm=10.8100 tok/s=1,385,514 raw_tok/s=1,393,787 step_tokens=130,294 waste=0.6% h100_mfu=33.91% vram=397/17,338MB peak=16,747/17,338MB
[step 20/8250] loss=3.5423 lr=6.29e-06 muon_lr=6.29e-05 grad_norm=10.5616 tok/s=1,363,799 raw_tok/s=1,367,147 step_tokens=130,751 waste=0.2% h100_mfu=33.26% vram=397/17,338MB peak=16,747/17,338MB
[step 21/8250] loss=3.5162 lr=6.62e-06 muon_lr=6.62e-05 grad_norm=10.3498 tok/s=1,357,201 raw_tok/s=1,360,314 step_tokens=130,772 waste=0.2% h100_mfu=33.10% vram=397/17,338MB peak=16,747/17,338MB
[step 22/8250] loss=3.4802 lr=6.95e-06 muon_lr=6.95e-05 grad_norm=10.2115 tok/s=1,398,616 raw_tok/s=1,400,786 step_tokens=130,869 waste=0.2% h100_mfu=34.08% vram=397/17,338MB peak=16,747/17,338MB
[step 23/8250] loss=3.4437 lr=7.28e-06 muon_lr=7.28e-05 grad_norm=9.7699 tok/s=1,361,358 raw_tok/s=1,367,567 step_tokens=130,477 waste=0.5% h100_mfu=33.27% vram=397/17,338MB peak=16,747/17,338MB
[step 24/8250] loss=3.4123 lr=7.62e-06 muon_lr=7.62e-05 grad_norm=9.6784 tok/s=1,373,475 raw_tok/s=1,376,479 step_tokens=130,786 waste=0.2% h100_mfu=33.49% vram=397/17,338MB peak=16,747/17,338MB
[step 25/8250] loss=3.3806 lr=7.95e-06 muon_lr=7.95e-05 grad_norm=9.5051 tok/s=1,395,016 raw_tok/s=1,400,090 step_tokens=130,597 waste=0.4% h100_mfu=34.06% vram=397/17,338MB peak=16,747/17,338MB
[step 26/8250] loss=3.3506 lr=8.28e-06 muon_lr=8.28e-05 grad_norm=9.2406 tok/s=1,255,739 raw_tok/s=1,259,448 step_tokens=130,686 waste=0.3% h100_mfu=30.64% vram=397/17,338MB peak=16,747/17,338MB
[step 27/8250] loss=3.3176 lr=8.61e-06 muon_lr=8.61e-05 grad_norm=8.9316 tok/s=1,386,764 raw_tok/s=1,394,627 step_tokens=130,333 waste=0.6% h100_mfu=33.93% vram=397/17,338MB peak=16,747/17,338MB
[step 28/8250] loss=3.2651 lr=8.94e-06 muon_lr=8.94e-05 grad_norm=8.4728 tok/s=1,437,020 raw_tok/s=1,441,849 step_tokens=130,633 waste=0.3% h100_mfu=35.08% vram=397/17,338MB peak=16,747/17,338MB
[step 29/8250] loss=3.2374 lr=9.27e-06 muon_lr=9.27e-05 grad_norm=8.3243 tok/s=1,304,465 raw_tok/s=1,307,038 step_tokens=130,814 waste=0.2% h100_mfu=31.80% vram=397/17,338MB peak=16,747/17,338MB
[step 30/8250] loss=3.2100 lr=9.60e-06 muon_lr=9.60e-05 grad_norm=8.1316 tok/s=1,413,141 raw_tok/s=1,417,934 step_tokens=130,629 waste=0.3% h100_mfu=34.50% vram=397/17,338MB peak=16,747/17,338MB
[step 31/8250] loss=3.1660 lr=9.93e-06 muon_lr=9.93e-05 grad_norm=7.7647 tok/s=1,408,906 raw_tok/s=1,412,743 step_tokens=130,716 waste=0.3% h100_mfu=34.37% vram=397/17,338MB peak=16,747/17,338MB
[step 32/8250] loss=3.1464 lr=1.03e-05 muon_lr=1.03e-04 grad_norm=7.2854 tok/s=1,506,388 raw_tok/s=1,510,201 step_tokens=130,741 waste=0.3% h100_mfu=36.74% vram=397/17,338MB peak=16,747/17,338MB
[step 33/8250] loss=3.0993 lr=1.06e-05 muon_lr=1.06e-04 grad_norm=6.9912 tok/s=1,361,217 raw_tok/s=1,364,444 step_tokens=130,762 waste=0.2% h100_mfu=33.20% vram=397/17,338MB peak=16,747/17,338MB
[step 34/8250] loss=3.0711 lr=1.09e-05 muon_lr=1.09e-04 grad_norm=6.7914 tok/s=1,440,334 raw_tok/s=1,441,753 step_tokens=130,943 waste=0.1% h100_mfu=35.08% vram=397/17,338MB peak=16,747/17,338MB
[step 35/8250] loss=3.0539 lr=1.13e-05 muon_lr=1.13e-04 grad_norm=6.3078 tok/s=1,409,412 raw_tok/s=1,412,970 step_tokens=130,742 waste=0.3% h100_mfu=34.38% vram=397/17,338MB peak=16,747/17,338MB
[step 36/8250] loss=3.0236 lr=1.16e-05 muon_lr=1.16e-04 grad_norm=5.8096 tok/s=1,410,651 raw_tok/s=1,414,785 step_tokens=130,689 waste=0.3% h100_mfu=34.42% vram=397/17,338MB peak=16,747/17,338MB
[step 37/8250] loss=2.9999 lr=1.19e-05 muon_lr=1.19e-04 grad_norm=5.4057 tok/s=1,447,746 raw_tok/s=1,448,354 step_tokens=131,017 waste=0.0% h100_mfu=35.24% vram=397/17,338MB peak=16,747/17,338MB
[step 38/8250] loss=2.9606 lr=1.23e-05 muon_lr=1.23e-04 grad_norm=4.9927 tok/s=1,390,253 raw_tok/s=1,397,836 step_tokens=130,361 waste=0.5% h100_mfu=34.01% vram=397/17,338MB peak=16,747/17,338MB
[step 39/8250] loss=2.9506 lr=1.26e-05 muon_lr=1.26e-04 grad_norm=4.5557 tok/s=1,398,442 raw_tok/s=1,403,721 step_tokens=130,579 waste=0.4% h100_mfu=34.15% vram=397/17,338MB peak=16,747/17,338MB
[step 40/8250] loss=2.9218 lr=1.29e-05 muon_lr=1.29e-04 grad_norm=4.2071 tok/s=1,441,470 raw_tok/s=1,442,075 step_tokens=131,017 waste=0.0% h100_mfu=35.09% vram=397/17,338MB peak=16,747/17,338MB
[step 41/8250] loss=2.9170 lr=1.32e-05 muon_lr=1.32e-04 grad_norm=3.9787 tok/s=1,434,748 raw_tok/s=1,436,678 step_tokens=130,896 waste=0.1% h100_mfu=34.95% vram=397/17,338MB peak=16,747/17,338MB
[step 42/8250] loss=2.9001 lr=1.36e-05 muon_lr=1.36e-04 grad_norm=3.7668 tok/s=1,354,650 raw_tok/s=1,356,109 step_tokens=130,931 waste=0.1% h100_mfu=32.99% vram=397/17,338MB peak=16,747/17,338MB
[step 43/8250] loss=2.8841 lr=1.39e-05 muon_lr=1.39e-04 grad_norm=3.5732 tok/s=1,372,926 raw_tok/s=1,374,909 step_tokens=130,883 waste=0.1% h100_mfu=33.45% vram=397/17,338MB peak=16,747/17,338MB
[step 44/8250] loss=2.8795 lr=1.42e-05 muon_lr=1.42e-04 grad_norm=3.4878 tok/s=1,453,567 raw_tok/s=1,456,211 step_tokens=130,834 waste=0.2% h100_mfu=35.43% vram=397/17,338MB peak=16,747/17,338MB
[step 45/8250] loss=2.8591 lr=1.46e-05 muon_lr=1.46e-04 grad_norm=3.5456 tok/s=1,408,361 raw_tok/s=1,414,610 step_tokens=130,493 waste=0.4% h100_mfu=34.42% vram=397/17,338MB peak=16,747/17,338MB
[step 46/8250] loss=2.8513 lr=1.49e-05 muon_lr=1.49e-04 grad_norm=3.2821 tok/s=1,384,687 raw_tok/s=1,390,714 step_tokens=130,504 waste=0.4% h100_mfu=33.84% vram=397/17,338MB peak=16,747/17,338MB
[step 47/8250] loss=2.8596 lr=1.52e-05 muon_lr=1.52e-04 grad_norm=3.3925 tok/s=1,432,731 raw_tok/s=1,436,216 step_tokens=130,754 waste=0.2% h100_mfu=34.94% vram=397/17,338MB peak=16,747/17,338MB
[step 48/8250] loss=2.8464 lr=1.56e-05 muon_lr=1.56e-04 grad_norm=3.2336 tok/s=1,492,334 raw_tok/s=1,496,181 step_tokens=130,735 waste=0.3% h100_mfu=36.40% vram=397/17,338MB peak=16,747/17,338MB
[step 49/8250] loss=2.8335 lr=1.59e-05 muon_lr=1.59e-04 grad_norm=3.0404 tok/s=1,421,116 raw_tok/s=1,422,038 step_tokens=130,987 waste=0.1% h100_mfu=34.60% vram=397/17,338MB peak=16,747/17,338MB
[step 50/8250] loss=2.8329 lr=1.62e-05 muon_lr=1.62e-04 grad_norm=3.0500 tok/s=1,398,033 raw_tok/s=1,400,245 step_tokens=130,865 waste=0.2% h100_mfu=34.07% vram=397/17,338MB peak=16,747/17,338MB
[step 51/8250] loss=2.8203 lr=1.66e-05 muon_lr=1.66e-04 grad_norm=2.5874 tok/s=1,423,558 raw_tok/s=1,425,472 step_tokens=130,896 waste=0.1% h100_mfu=34.68% vram=397/17,338MB peak=16,747/17,338MB
[step 52/8250] loss=2.8207 lr=1.69e-05 muon_lr=1.69e-04 grad_norm=2.4201 tok/s=1,363,390 raw_tok/s=1,365,348 step_tokens=130,884 waste=0.1% h100_mfu=33.22% vram=397/17,338MB peak=16,747/17,338MB
[step 53/8250] loss=2.8208 lr=1.72e-05 muon_lr=1.72e-04 grad_norm=2.2911 tok/s=1,415,524 raw_tok/s=1,420,139 step_tokens=130,646 waste=0.3% h100_mfu=34.55% vram=397/17,338MB peak=16,747/17,338MB
[step 54/8250] loss=2.8083 lr=1.75e-05 muon_lr=1.75e-04 grad_norm=2.0304 tok/s=1,381,826 raw_tok/s=1,382,491 step_tokens=131,009 waste=0.0% h100_mfu=33.64% vram=397/17,338MB peak=16,747/17,338MB
[step 55/8250] loss=2.8037 lr=1.79e-05 muon_lr=1.79e-04 grad_norm=1.9773 tok/s=1,415,768 raw_tok/s=1,417,834 step_tokens=130,881 waste=0.1% h100_mfu=34.50% vram=397/17,338MB peak=16,747/17,338MB
[step 56/8250] loss=2.7934 lr=1.82e-05 muon_lr=1.82e-04 grad_norm=1.9985 tok/s=1,391,819 raw_tok/s=1,394,202 step_tokens=130,848 waste=0.2% h100_mfu=33.92% vram=397/17,338MB peak=16,747/17,338MB
[step 57/8250] loss=2.7872 lr=1.85e-05 muon_lr=1.85e-04 grad_norm=1.8405 tok/s=1,397,892 raw_tok/s=1,402,450 step_tokens=130,646 waste=0.3% h100_mfu=34.12% vram=397/17,338MB peak=16,747/17,338MB
[step 58/8250] loss=2.7721 lr=1.89e-05 muon_lr=1.89e-04 grad_norm=1.8176 tok/s=1,371,171 raw_tok/s=1,373,277 step_tokens=130,871 waste=0.2% h100_mfu=33.41% vram=397/17,338MB peak=16,747/17,338MB
[step 59/8250] loss=2.7790 lr=1.92e-05 muon_lr=1.92e-04 grad_norm=1.7032 tok/s=1,379,772 raw_tok/s=1,383,964 step_tokens=130,675 waste=0.3% h100_mfu=33.67% vram=397/17,338MB peak=16,747/17,338MB
[step 60/8250] loss=2.7501 lr=1.95e-05 muon_lr=1.95e-04 grad_norm=1.4838 tok/s=1,496,578 raw_tok/s=1,497,789 step_tokens=130,966 waste=0.1% h100_mfu=36.44% vram=397/17,338MB peak=16,747/17,338MB
[step 61/8250] loss=2.7706 lr=1.99e-05 muon_lr=1.99e-04 grad_norm=1.6201 tok/s=1,381,510 raw_tok/s=1,386,492 step_tokens=130,601 waste=0.4% h100_mfu=33.73% vram=397/17,338MB peak=16,747/17,338MB
[step 62/8250] loss=2.7616 lr=2.02e-05 muon_lr=2.02e-04 grad_norm=1.3352 tok/s=1,395,353 raw_tok/s=1,398,051 step_tokens=130,819 waste=0.2% h100_mfu=34.01% vram=397/17,338MB peak=16,747/17,338MB
[step 63/8250] loss=2.7612 lr=2.05e-05 muon_lr=2.05e-04 grad_norm=1.1799 tok/s=1,265,824 raw_tok/s=1,268,137 step_tokens=130,833 waste=0.2% h100_mfu=30.85% vram=397/17,338MB peak=16,747/17,338MB
[step 64/8250] loss=2.7534 lr=2.09e-05 muon_lr=2.09e-04 grad_norm=1.0447 tok/s=1,372,643 raw_tok/s=1,375,560 step_tokens=130,794 waste=0.2% h100_mfu=33.47% vram=397/17,338MB peak=16,747/17,338MB
[step 65/8250] loss=2.7353 lr=2.12e-05 muon_lr=2.12e-04 grad_norm=1.0697 tok/s=1,222,290 raw_tok/s=1,228,815 step_tokens=130,376 waste=0.5% h100_mfu=29.90% vram=397/17,338MB peak=16,747/17,338MB
[step 66/8250] loss=2.7299 lr=2.15e-05 muon_lr=2.15e-04 grad_norm=1.0028 tok/s=1,363,409 raw_tok/s=1,369,354 step_tokens=130,503 waste=0.4% h100_mfu=33.32% vram=397/17,338MB peak=16,747/17,338MB
[step 67/8250] loss=2.7261 lr=2.19e-05 muon_lr=2.19e-04 grad_norm=1.0156 tok/s=1,451,000 raw_tok/s=1,453,951 step_tokens=130,806 waste=0.2% h100_mfu=35.37% vram=397/17,338MB peak=16,747/17,338MB
[step 68/8250] loss=2.7258 lr=2.22e-05 muon_lr=2.22e-04 grad_norm=1.0522 tok/s=1,338,824 raw_tok/s=1,338,946 step_tokens=131,060 waste=0.0% h100_mfu=32.58% vram=397/17,338MB peak=16,747/17,338MB
[step 69/8250] loss=2.7212 lr=2.25e-05 muon_lr=2.25e-04 grad_norm=1.0241 tok/s=1,443,223 raw_tok/s=1,448,272 step_tokens=130,615 waste=0.3% h100_mfu=35.24% vram=397/17,338MB peak=16,747/17,338MB
[step 70/8250] loss=2.7273 lr=2.28e-05 muon_lr=2.28e-04 grad_norm=0.9497 tok/s=1,379,788 raw_tok/s=1,383,853 step_tokens=130,687 waste=0.3% h100_mfu=33.67% vram=397/17,338MB peak=16,747/17,338MB
[step 71/8250] loss=2.7138 lr=2.32e-05 muon_lr=2.32e-04 grad_norm=0.7917 tok/s=1,419,792 raw_tok/s=1,427,163 step_tokens=130,395 waste=0.5% h100_mfu=34.72% vram=397/17,338MB peak=16,747/17,338MB
[step 72/8250] loss=2.7078 lr=2.35e-05 muon_lr=2.35e-04 grad_norm=0.7161 tok/s=1,417,357 raw_tok/s=1,423,111 step_tokens=130,542 waste=0.4% h100_mfu=34.62% vram=397/17,338MB peak=16,747/17,338MB
[step 73/8250] loss=2.7240 lr=2.38e-05 muon_lr=2.38e-04 grad_norm=0.6184 tok/s=1,376,901 raw_tok/s=1,382,439 step_tokens=130,547 waste=0.4% h100_mfu=33.63% vram=397/17,338MB peak=16,747/17,338MB
[step 74/8250] loss=2.7122 lr=2.42e-05 muon_lr=2.42e-04 grad_norm=0.6268 tok/s=1,385,023 raw_tok/s=1,386,165 step_tokens=130,964 waste=0.1% h100_mfu=33.73% vram=397/17,338MB peak=16,747/17,338MB
[step 75/8250] loss=2.7033 lr=2.45e-05 muon_lr=2.45e-04 grad_norm=0.6760 tok/s=1,469,355 raw_tok/s=1,476,632 step_tokens=130,426 waste=0.5% h100_mfu=35.93% vram=397/17,338MB peak=16,747/17,338MB
[step 76/8250] loss=2.7216 lr=2.48e-05 muon_lr=2.48e-04 grad_norm=0.7444 tok/s=1,484,805 raw_tok/s=1,490,182 step_tokens=130,599 waste=0.4% h100_mfu=36.26% vram=397/17,338MB peak=16,747/17,338MB
[step 77/8250] loss=2.7054 lr=2.52e-05 muon_lr=2.52e-04 grad_norm=0.6745 tok/s=1,498,099 raw_tok/s=1,500,160 step_tokens=130,892 waste=0.1% h100_mfu=36.50% vram=397/17,338MB peak=16,747/17,338MB
[step 78/8250] loss=2.7062 lr=2.55e-05 muon_lr=2.55e-04 grad_norm=0.6565 tok/s=1,391,297 raw_tok/s=1,394,606 step_tokens=130,761 waste=0.2% h100_mfu=33.93% vram=397/17,338MB peak=16,747/17,338MB
[step 79/8250] loss=2.7132 lr=2.58e-05 muon_lr=2.58e-04 grad_norm=0.6611 tok/s=1,444,259 raw_tok/s=1,447,119 step_tokens=130,813 waste=0.2% h100_mfu=35.21% vram=397/17,338MB peak=16,747/17,338MB
[step 80/8250] loss=2.7129 lr=2.62e-05 muon_lr=2.62e-04 grad_norm=0.5766 tok/s=1,423,071 raw_tok/s=1,427,799 step_tokens=130,638 waste=0.3% h100_mfu=34.74% vram=397/17,338MB peak=16,747/17,338MB
[step 81/8250] loss=2.6903 lr=2.65e-05 muon_lr=2.65e-04 grad_norm=0.5533 tok/s=1,441,917 raw_tok/s=1,444,242 step_tokens=130,861 waste=0.2% h100_mfu=35.14% vram=397/17,338MB peak=16,747/17,338MB
[step 82/8250] loss=2.6887 lr=2.68e-05 muon_lr=2.68e-04 grad_norm=0.5290 tok/s=1,488,265 raw_tok/s=1,489,834 step_tokens=130,934 waste=0.1% h100_mfu=36.25% vram=397/17,338MB peak=16,747/17,338MB
[step 83/8250] loss=2.7013 lr=2.72e-05 muon_lr=2.72e-04 grad_norm=0.5873 tok/s=1,360,791 raw_tok/s=1,362,183 step_tokens=130,938 waste=0.1% h100_mfu=33.14% vram=397/17,338MB peak=16,747/17,338MB
[step 84/8250] loss=2.6941 lr=2.75e-05 muon_lr=2.75e-04 grad_norm=0.4433 tok/s=1,401,269 raw_tok/s=1,403,432 step_tokens=130,870 waste=0.2% h100_mfu=34.15% vram=397/17,338MB peak=16,747/17,338MB
[step 85/8250] loss=2.6935 lr=2.78e-05 muon_lr=2.78e-04 grad_norm=0.5638 tok/s=1,514,593 raw_tok/s=1,516,781 step_tokens=130,883 waste=0.1% h100_mfu=36.90% vram=397/17,338MB peak=16,747/17,338MB
[step 86/8250] loss=2.6913 lr=2.81e-05 muon_lr=2.81e-04 grad_norm=0.5949 tok/s=1,418,161 raw_tok/s=1,421,230 step_tokens=130,789 waste=0.2% h100_mfu=34.58% vram=397/17,338MB peak=16,747/17,338MB
[step 87/8250] loss=2.6749 lr=2.85e-05 muon_lr=2.85e-04 grad_norm=0.4770 tok/s=1,451,343 raw_tok/s=1,454,450 step_tokens=130,792 waste=0.2% h100_mfu=35.39% vram=397/17,338MB peak=16,747/17,338MB
[step 88/8250] loss=2.6798 lr=2.88e-05 muon_lr=2.88e-04 grad_norm=0.4918 tok/s=1,448,261 raw_tok/s=1,449,975 step_tokens=130,917 waste=0.1% h100_mfu=35.28% vram=397/17,338MB peak=16,747/17,338MB
[step 89/8250] loss=2.6848 lr=2.91e-05 muon_lr=2.91e-04 grad_norm=0.3627 tok/s=1,495,876 raw_tok/s=1,502,502 step_tokens=130,494 waste=0.4% h100_mfu=36.56% vram=397/17,338MB peak=16,747/17,338MB
[step 90/8250] loss=2.6899 lr=2.95e-05 muon_lr=2.95e-04 grad_norm=0.4650 tok/s=1,514,143 raw_tok/s=1,517,710 step_tokens=130,764 waste=0.2% h100_mfu=36.93% vram=397/17,338MB peak=16,747/17,338MB
[step 91/8250] loss=2.6810 lr=2.98e-05 muon_lr=2.98e-04 grad_norm=0.5357 tok/s=1,514,489 raw_tok/s=1,520,172 step_tokens=130,582 waste=0.4% h100_mfu=36.99% vram=397/17,338MB peak=16,747/17,338MB
[step 92/8250] loss=2.6707 lr=3.01e-05 muon_lr=3.01e-04 grad_norm=0.5521 tok/s=1,386,777 raw_tok/s=1,389,459 step_tokens=130,819 waste=0.2% h100_mfu=33.81% vram=397/17,338MB peak=16,747/17,338MB
[step 93/8250] loss=2.6803 lr=3.05e-05 muon_lr=3.05e-04 grad_norm=0.5514 tok/s=1,441,671 raw_tok/s=1,444,989 step_tokens=130,771 waste=0.2% h100_mfu=35.16% vram=397/17,338MB peak=16,747/17,338MB
[step 94/8250] loss=2.6656 lr=3.08e-05 muon_lr=3.08e-04 grad_norm=0.4971 tok/s=1,507,271 raw_tok/s=1,511,734 step_tokens=130,685 waste=0.3% h100_mfu=36.78% vram=397/17,338MB peak=16,747/17,338MB
[step 95/8250] loss=2.6917 lr=3.11e-05 muon_lr=3.11e-04 grad_norm=0.4575 tok/s=1,357,346 raw_tok/s=1,361,418 step_tokens=130,680 waste=0.3% h100_mfu=33.12% vram=397/17,338MB peak=16,747/17,338MB
[step 96/8250] loss=2.6772 lr=3.15e-05 muon_lr=3.15e-04 grad_norm=0.4381 tok/s=1,466,220 raw_tok/s=1,469,674 step_tokens=130,764 waste=0.2% h100_mfu=35.76% vram=397/17,338MB peak=16,747/17,338MB
[step 97/8250] loss=2.6813 lr=3.18e-05 muon_lr=3.18e-04 grad_norm=0.6913 tok/s=1,434,265 raw_tok/s=1,435,580 step_tokens=130,952 waste=0.1% h100_mfu=34.93% vram=397/17,338MB peak=16,747/17,338MB
[step 98/8250] loss=2.6706 lr=3.21e-05 muon_lr=3.21e-04 grad_norm=0.6701 tok/s=1,390,679 raw_tok/s=1,398,682 step_tokens=130,322 waste=0.6% h100_mfu=34.03% vram=397/17,338MB peak=16,747/17,338MB
[step 99/8250] loss=2.6741 lr=3.25e-05 muon_lr=3.25e-04 grad_norm=0.6302 tok/s=1,429,376 raw_tok/s=1,431,036 step_tokens=130,920 waste=0.1% h100_mfu=34.82% vram=397/17,338MB peak=16,747/17,338MB
[step 100/8250] loss=2.6719 lr=3.28e-05 muon_lr=3.28e-04 grad_norm=0.5838 tok/s=1,489,778 raw_tok/s=1,491,895 step_tokens=130,886 waste=0.1% h100_mfu=36.30% vram=397/17,338MB peak=16,747/17,338MB
[step 101/8250] loss=2.6785 lr=3.31e-05 muon_lr=3.31e-04 grad_norm=0.5948 tok/s=1,462,280 raw_tok/s=1,468,195 step_tokens=130,544 waste=0.4% h100_mfu=35.72% vram=397/17,338MB peak=16,747/17,338MB
[step 102/8250] loss=2.6816 lr=3.34e-05 muon_lr=3.34e-04 grad_norm=0.7299 tok/s=1,515,846 raw_tok/s=1,517,444 step_tokens=130,934 waste=0.1% h100_mfu=36.92% vram=397/17,338MB peak=16,747/17,338MB
[step 103/8250] loss=2.6653 lr=3.38e-05 muon_lr=3.38e-04 grad_norm=0.9657 tok/s=1,415,600 raw_tok/s=1,417,178 step_tokens=130,926 waste=0.1% h100_mfu=34.48% vram=397/17,338MB peak=16,747/17,338MB
[step 104/8250] loss=2.6850 lr=3.41e-05 muon_lr=3.41e-04 grad_norm=0.8839 tok/s=1,409,013 raw_tok/s=1,411,220 step_tokens=130,867 waste=0.2% h100_mfu=34.34% vram=397/17,338MB peak=16,747/17,338MB
[step 105/8250] loss=2.6899 lr=3.44e-05 muon_lr=3.44e-04 grad_norm=0.8411 tok/s=1,438,564 raw_tok/s=1,439,575 step_tokens=130,980 waste=0.1% h100_mfu=35.02% vram=397/17,338MB peak=16,747/17,338MB
[step 106/8250] loss=2.6723 lr=3.48e-05 muon_lr=3.48e-04 grad_norm=0.8771 tok/s=1,432,374 raw_tok/s=1,436,385 step_tokens=130,706 waste=0.3% h100_mfu=34.95% vram=397/17,338MB peak=16,747/17,338MB
[step 107/8250] loss=2.6765 lr=3.51e-05 muon_lr=3.51e-04 grad_norm=0.7046 tok/s=1,403,290 raw_tok/s=1,407,176 step_tokens=130,710 waste=0.3% h100_mfu=34.24% vram=397/17,338MB peak=16,747/17,338MB
[step 108/8250] loss=2.6720 lr=3.54e-05 muon_lr=3.54e-04 grad_norm=0.4752 tok/s=1,518,397 raw_tok/s=1,522,218 step_tokens=130,743 waste=0.3% h100_mfu=37.04% vram=397/17,338MB peak=16,747/17,338MB
[step 109/8250] loss=2.6697 lr=3.58e-05 muon_lr=3.58e-04 grad_norm=0.5914 tok/s=1,406,930 raw_tok/s=1,409,576 step_tokens=130,826 waste=0.2% h100_mfu=34.30% vram=397/17,338MB peak=16,747/17,338MB
[step 110/8250] loss=2.6789 lr=3.61e-05 muon_lr=3.61e-04 grad_norm=0.8588 tok/s=1,339,585 raw_tok/s=1,343,367 step_tokens=130,703 waste=0.3% h100_mfu=32.68% vram=397/17,338MB peak=16,747/17,338MB
[step 111/8250] loss=2.6671 lr=3.64e-05 muon_lr=3.64e-04 grad_norm=0.8220 tok/s=1,381,900 raw_tok/s=1,383,209 step_tokens=130,948 waste=0.1% h100_mfu=33.65% vram=397/17,338MB peak=16,747/17,338MB
[step 112/8250] loss=2.6592 lr=3.68e-05 muon_lr=3.68e-04 grad_norm=0.7585 tok/s=1,360,724 raw_tok/s=1,363,898 step_tokens=130,767 waste=0.2% h100_mfu=33.18% vram=397/17,338MB peak=16,747/17,338MB
[step 113/8250] loss=2.6749 lr=3.71e-05 muon_lr=3.71e-04 grad_norm=0.6983 tok/s=1,428,132 raw_tok/s=1,433,480 step_tokens=130,583 waste=0.4% h100_mfu=34.88% vram=397/17,338MB peak=16,747/17,338MB
[step 114/8250] loss=2.6802 lr=3.74e-05 muon_lr=3.74e-04 grad_norm=0.6860 tok/s=1,413,388 raw_tok/s=1,415,007 step_tokens=130,922 waste=0.1% h100_mfu=34.43% vram=397/17,338MB peak=16,747/17,338MB
[step 115/8250] loss=2.6780 lr=3.77e-05 muon_lr=3.77e-04 grad_norm=0.5846 tok/s=1,465,418 raw_tok/s=1,470,264 step_tokens=130,640 waste=0.3% h100_mfu=35.77% vram=397/17,338MB peak=16,747/17,338MB
[step 116/8250] loss=2.6800 lr=3.81e-05 muon_lr=3.81e-04 grad_norm=0.5971 tok/s=1,507,261 raw_tok/s=1,513,948 step_tokens=130,493 waste=0.4% h100_mfu=36.83% vram=397/17,338MB peak=16,747/17,338MB
[step 117/8250] loss=2.6824 lr=3.84e-05 muon_lr=3.84e-04 grad_norm=0.7155 tok/s=1,399,072 raw_tok/s=1,402,636 step_tokens=130,739 waste=0.3% h100_mfu=34.13% vram=397/17,338MB peak=16,747/17,338MB
[step 118/8250] loss=2.6659 lr=3.87e-05 muon_lr=3.87e-04 grad_norm=0.7858 tok/s=1,435,485 raw_tok/s=1,437,645 step_tokens=130,875 waste=0.2% h100_mfu=34.98% vram=397/17,338MB peak=16,747/17,338MB
[step 119/8250] loss=2.6645 lr=3.91e-05 muon_lr=3.91e-04 grad_norm=0.6717 tok/s=1,362,739 raw_tok/s=1,368,492 step_tokens=130,521 waste=0.4% h100_mfu=33.30% vram=397/17,338MB peak=16,747/17,338MB
[step 120/8250] loss=2.6644 lr=3.94e-05 muon_lr=3.94e-04 grad_norm=1.0404 tok/s=1,436,937 raw_tok/s=1,438,946 step_tokens=130,889 waste=0.1% h100_mfu=35.01% vram=397/17,338MB peak=16,747/17,338MB
[step 121/8250] loss=2.6629 lr=3.97e-05 muon_lr=3.97e-04 grad_norm=1.1070 tok/s=1,403,151 raw_tok/s=1,406,951 step_tokens=130,718 waste=0.3% h100_mfu=34.23% vram=397/17,338MB peak=16,747/17,338MB
[step 122/8250] loss=2.6701 lr=4.01e-05 muon_lr=4.01e-04 grad_norm=0.9377 tok/s=1,438,767 raw_tok/s=1,442,024 step_tokens=130,776 waste=0.2% h100_mfu=35.08% vram=397/17,338MB peak=16,747/17,338MB
[step 123/8250] loss=2.6863 lr=4.04e-05 muon_lr=4.04e-04 grad_norm=1.0737 tok/s=1,337,783 raw_tok/s=1,343,142 step_tokens=130,549 waste=0.4% h100_mfu=32.68% vram=397/17,338MB peak=16,747/17,338MB
[step 124/8250] loss=2.6720 lr=4.07e-05 muon_lr=4.07e-04 grad_norm=0.9371 tok/s=1,376,277 raw_tok/s=1,378,950 step_tokens=130,818 waste=0.2% h100_mfu=33.55% vram=397/17,338MB peak=16,747/17,338MB
[step 125/8250] loss=2.6611 lr=4.11e-05 muon_lr=4.11e-04 grad_norm=0.9399 tok/s=1,427,654 raw_tok/s=1,429,989 step_tokens=130,858 waste=0.2% h100_mfu=34.79% vram=397/17,338MB peak=16,747/17,338MB
[step 126/8250] loss=2.6818 lr=4.14e-05 muon_lr=4.14e-04 grad_norm=1.0244 tok/s=1,415,899 raw_tok/s=1,419,125 step_tokens=130,774 waste=0.2% h100_mfu=34.53% vram=397/17,338MB peak=16,747/17,338MB
[step 127/8250] loss=2.6667 lr=4.17e-05 muon_lr=4.17e-04 grad_norm=0.7022 tok/s=1,493,703 raw_tok/s=1,496,305 step_tokens=130,844 waste=0.2% h100_mfu=36.41% vram=397/17,338MB peak=16,747/17,338MB
[step 128/8250] loss=2.6581 lr=4.21e-05 muon_lr=4.21e-04 grad_norm=0.6742 tok/s=1,424,666 raw_tok/s=1,428,973 step_tokens=130,677 waste=0.3% h100_mfu=34.77% vram=397/17,338MB peak=16,747/17,338MB
[step 129/8250] loss=2.6528 lr=4.24e-05 muon_lr=4.24e-04 grad_norm=0.7176 tok/s=1,260,004 raw_tok/s=1,263,813 step_tokens=130,677 waste=0.3% h100_mfu=30.75% vram=397/17,338MB peak=16,747/17,338MB
[step 130/8250] loss=2.6609 lr=4.27e-05 muon_lr=4.27e-04 grad_norm=0.7784 tok/s=1,444,590 raw_tok/s=1,450,333 step_tokens=130,553 waste=0.4% h100_mfu=35.29% vram=397/17,338MB peak=16,747/17,338MB
[step 131/8250] loss=2.6665 lr=4.30e-05 muon_lr=4.30e-04 grad_norm=0.7092 tok/s=1,519,315 raw_tok/s=1,521,323 step_tokens=130,899 waste=0.1% h100_mfu=37.01% vram=397/17,338MB peak=16,747/17,338MB
[step 132/8250] loss=2.6731 lr=4.34e-05 muon_lr=4.34e-04 grad_norm=0.8104 tok/s=1,228,374 raw_tok/s=1,233,398 step_tokens=130,538 waste=0.4% h100_mfu=30.01% vram=397/17,338MB peak=16,747/17,338MB
[step 133/8250] loss=2.6724 lr=4.37e-05 muon_lr=4.37e-04 grad_norm=0.8302 tok/s=1,379,921 raw_tok/s=1,383,012 step_tokens=130,779 waste=0.2% h100_mfu=33.65% vram=397/17,338MB peak=16,747/17,338MB
[step 134/8250] loss=2.6595 lr=4.40e-05 muon_lr=4.40e-04 grad_norm=0.8729 tok/s=1,415,397 raw_tok/s=1,420,545 step_tokens=130,597 waste=0.4% h100_mfu=34.56% vram=397/17,338MB peak=16,747/17,338MB
[step 135/8250] loss=2.6626 lr=4.44e-05 muon_lr=4.44e-04 grad_norm=1.0716 tok/s=1,341,409 raw_tok/s=1,346,556 step_tokens=130,571 waste=0.4% h100_mfu=32.76% vram=397/17,338MB peak=16,747/17,338MB
[step 136/8250] loss=2.6697 lr=4.47e-05 muon_lr=4.47e-04 grad_norm=0.6585 tok/s=1,389,743 raw_tok/s=1,392,601 step_tokens=130,803 waste=0.2% h100_mfu=33.88% vram=397/17,338MB peak=16,747/17,338MB
[step 137/8250] loss=2.6650 lr=4.50e-05 muon_lr=4.50e-04 grad_norm=0.5728 tok/s=1,300,036 raw_tok/s=1,305,724 step_tokens=130,501 waste=0.4% h100_mfu=31.77% vram=397/17,338MB peak=16,747/17,338MB
[step 138/8250] loss=2.6676 lr=4.54e-05 muon_lr=4.54e-04 grad_norm=0.6982 tok/s=1,518,249 raw_tok/s=1,520,615 step_tokens=130,868 waste=0.2% h100_mfu=37.00% vram=397/17,338MB peak=16,747/17,338MB
[step 139/8250] loss=2.6596 lr=4.57e-05 muon_lr=4.57e-04 grad_norm=0.8058 tok/s=1,447,514 raw_tok/s=1,450,258 step_tokens=130,824 waste=0.2% h100_mfu=35.28% vram=397/17,338MB peak=16,747/17,338MB
[step 140/8250] loss=2.6588 lr=4.60e-05 muon_lr=4.60e-04 grad_norm=0.9595 tok/s=1,507,897 raw_tok/s=1,509,613 step_tokens=130,923 waste=0.1% h100_mfu=36.73% vram=397/17,338MB peak=16,747/17,338MB
[step 141/8250] loss=2.6443 lr=4.64e-05 muon_lr=4.64e-04 grad_norm=1.0391 tok/s=1,453,851 raw_tok/s=1,455,339 step_tokens=130,938 waste=0.1% h100_mfu=35.41% vram=397/17,338MB peak=16,747/17,338MB
[step 142/8250] loss=2.6733 lr=4.67e-05 muon_lr=4.67e-04 grad_norm=1.1928 tok/s=1,425,074 raw_tok/s=1,428,474 step_tokens=130,760 waste=0.2% h100_mfu=34.75% vram=397/17,338MB peak=16,747/17,338MB
[step 143/8250] loss=2.6617 lr=4.70e-05 muon_lr=4.70e-04 grad_norm=1.0468 tok/s=1,515,678 raw_tok/s=1,522,287 step_tokens=130,503 waste=0.4% h100_mfu=37.04% vram=397/17,338MB peak=16,747/17,338MB
[step 144/8250] loss=2.6606 lr=4.74e-05 muon_lr=4.74e-04 grad_norm=0.8887 tok/s=1,428,513 raw_tok/s=1,431,385 step_tokens=130,809 waste=0.2% h100_mfu=34.83% vram=397/17,338MB peak=16,747/17,338MB
[step 145/8250] loss=2.6589 lr=4.77e-05 muon_lr=4.77e-04 grad_norm=0.9232 tok/s=1,447,881 raw_tok/s=1,455,186 step_tokens=130,414 waste=0.5% h100_mfu=35.40% vram=397/17,338MB peak=16,747/17,338MB
[step 146/8250] loss=2.6542 lr=4.80e-05 muon_lr=4.80e-04 grad_norm=0.9452 tok/s=1,448,159 raw_tok/s=1,449,818 step_tokens=130,922 waste=0.1% h100_mfu=35.27% vram=397/17,338MB peak=16,747/17,338MB
[step 147/8250] loss=2.6625 lr=4.83e-05 muon_lr=4.83e-04 grad_norm=0.9381 tok/s=1,415,313 raw_tok/s=1,421,135 step_tokens=130,535 waste=0.4% h100_mfu=34.58% vram=397/17,338MB peak=16,747/17,338MB
[step 148/8250] loss=2.6529 lr=4.87e-05 muon_lr=4.87e-04 grad_norm=0.9927 tok/s=1,385,752 raw_tok/s=1,388,040 step_tokens=130,856 waste=0.2% h100_mfu=33.77% vram=397/17,338MB peak=16,747/17,338MB
[step 149/8250] loss=2.6564 lr=4.90e-05 muon_lr=4.90e-04 grad_norm=0.9096 tok/s=1,445,617 raw_tok/s=1,447,395 step_tokens=130,911 waste=0.1% h100_mfu=35.22% vram=397/17,338MB peak=16,747/17,338MB
[step 150/8250] loss=2.6541 lr=4.93e-05 muon_lr=4.93e-04 grad_norm=0.9746 tok/s=1,356,982 raw_tok/s=1,362,251 step_tokens=130,565 waste=0.4% h100_mfu=33.14% vram=397/17,338MB peak=16,747/17,338MB
[step 151/8250] loss=2.6628 lr=4.97e-05 muon_lr=4.97e-04 grad_norm=1.0256 tok/s=1,437,444 raw_tok/s=1,442,705 step_tokens=130,594 waste=0.4% h100_mfu=35.10% vram=397/17,338MB peak=16,747/17,338MB
[step 152/8250] loss=2.6578 lr=5.00e-05 muon_lr=5.00e-04 grad_norm=0.7052 tok/s=1,502,940 raw_tok/s=1,505,823 step_tokens=130,821 waste=0.2% h100_mfu=36.64% vram=397/17,338MB peak=16,747/17,338MB
[step 153/8250] loss=2.6521 lr=5.03e-05 muon_lr=5.03e-04 grad_norm=0.7936 tok/s=1,424,043 raw_tok/s=1,427,933 step_tokens=130,715 waste=0.3% h100_mfu=34.74% vram=397/17,338MB peak=16,747/17,338MB
[step 154/8250] loss=2.6436 lr=5.07e-05 muon_lr=5.07e-04 grad_norm=0.7325 tok/s=1,485,981 raw_tok/s=1,488,411 step_tokens=130,858 waste=0.2% h100_mfu=36.21% vram=397/17,338MB peak=16,747/17,338MB
[step 155/8250] loss=2.6556 lr=5.10e-05 muon_lr=5.10e-04 grad_norm=0.5826 tok/s=1,427,547 raw_tok/s=1,428,059 step_tokens=131,025 waste=0.0% h100_mfu=34.74% vram=397/17,338MB peak=16,747/17,338MB
[step 156/8250] loss=2.6609 lr=5.13e-05 muon_lr=5.13e-04 grad_norm=0.7620 tok/s=1,282,013 raw_tok/s=1,284,325 step_tokens=130,836 waste=0.2% h100_mfu=31.25% vram=397/17,338MB peak=16,747/17,338MB
[step 157/8250] loss=2.6559 lr=5.17e-05 muon_lr=5.17e-04 grad_norm=0.6623 tok/s=1,411,631 raw_tok/s=1,415,551 step_tokens=130,709 waste=0.3% h100_mfu=34.44% vram=397/17,338MB peak=16,747/17,338MB
[step 158/8250] loss=2.6548 lr=5.20e-05 muon_lr=5.20e-04 grad_norm=0.5419 tok/s=1,442,459 raw_tok/s=1,446,631 step_tokens=130,694 waste=0.3% h100_mfu=35.20% vram=397/17,338MB peak=16,747/17,338MB
[step 159/8250] loss=2.6464 lr=5.23e-05 muon_lr=5.23e-04 grad_norm=0.7407 tok/s=1,365,222 raw_tok/s=1,369,915 step_tokens=130,623 waste=0.3% h100_mfu=33.33% vram=397/17,338MB peak=16,747/17,338MB
[step 160/8250] loss=2.6508 lr=5.26e-05 muon_lr=5.26e-04 grad_norm=0.8085 tok/s=1,293,328 raw_tok/s=1,298,977 step_tokens=130,502 waste=0.4% h100_mfu=31.60% vram=397/17,338MB peak=16,747/17,338MB
[step 161/8250] loss=2.6524 lr=5.30e-05 muon_lr=5.30e-04 grad_norm=0.6340 tok/s=1,445,902 raw_tok/s=1,449,641 step_tokens=130,734 waste=0.3% h100_mfu=35.27% vram=397/17,338MB peak=16,747/17,338MB
[step 162/8250] loss=2.6752 lr=5.33e-05 muon_lr=5.33e-04 grad_norm=0.7721 tok/s=1,408,126 raw_tok/s=1,411,367 step_tokens=130,771 waste=0.2% h100_mfu=34.34% vram=397/17,338MB peak=16,747/17,338MB
[step 163/8250] loss=2.6528 lr=5.36e-05 muon_lr=5.36e-04 grad_norm=0.7078 tok/s=1,403,849 raw_tok/s=1,410,759 step_tokens=130,430 waste=0.5% h100_mfu=34.32% vram=397/17,338MB peak=16,747/17,338MB
[step 164/8250] loss=2.6591 lr=5.40e-05 muon_lr=5.40e-04 grad_norm=0.7074 tok/s=1,378,038 raw_tok/s=1,381,284 step_tokens=130,764 waste=0.2% h100_mfu=33.61% vram=397/17,338MB peak=16,747/17,338MB
[step 165/8250] loss=2.6486 lr=5.43e-05 muon_lr=5.43e-04 grad_norm=0.7797 tok/s=1,302,524 raw_tok/s=1,306,082 step_tokens=130,715 waste=0.3% h100_mfu=31.78% vram=397/17,338MB peak=16,747/17,338MB
[step 166/8250] loss=2.6468 lr=5.46e-05 muon_lr=5.46e-04 grad_norm=0.8271 tok/s=1,428,781 raw_tok/s=1,433,308 step_tokens=130,658 waste=0.3% h100_mfu=34.87% vram=397/17,338MB peak=16,747/17,338MB
[step 167/8250] loss=2.6608 lr=5.50e-05 muon_lr=5.50e-04 grad_norm=0.7776 tok/s=1,501,520 raw_tok/s=1,502,655 step_tokens=130,973 waste=0.1% h100_mfu=36.56% vram=397/17,338MB peak=16,747/17,338MB
[step 168/8250] loss=2.6561 lr=5.53e-05 muon_lr=5.53e-04 grad_norm=0.7243 tok/s=1,377,740 raw_tok/s=1,379,624 step_tokens=130,893 waste=0.1% h100_mfu=33.57% vram=397/17,338MB peak=16,747/17,338MB
[step 169/8250] loss=2.6602 lr=5.56e-05 muon_lr=5.56e-04 grad_norm=0.7050 tok/s=1,337,296 raw_tok/s=1,341,595 step_tokens=130,652 waste=0.3% h100_mfu=32.64% vram=397/17,338MB peak=16,747/17,338MB
[step 170/8250] loss=2.6458 lr=5.60e-05 muon_lr=5.60e-04 grad_norm=0.6980 tok/s=1,417,095 raw_tok/s=1,421,520 step_tokens=130,664 waste=0.3% h100_mfu=34.59% vram=397/17,338MB peak=16,747/17,338MB
[step 171/8250] loss=2.6523 lr=5.63e-05 muon_lr=5.63e-04 grad_norm=0.8012 tok/s=1,437,170 raw_tok/s=1,440,798 step_tokens=130,742 waste=0.3% h100_mfu=35.05% vram=397/17,338MB peak=16,747/17,338MB
[step 172/8250] loss=2.6632 lr=5.66e-05 muon_lr=5.66e-04 grad_norm=0.6657 tok/s=1,504,602 raw_tok/s=1,505,245 step_tokens=131,016 waste=0.0% h100_mfu=36.62% vram=397/17,338MB peak=16,747/17,338MB
[step 173/8250] loss=2.6461 lr=5.70e-05 muon_lr=5.70e-04 grad_norm=0.6264 tok/s=1,371,474 raw_tok/s=1,376,631 step_tokens=130,581 waste=0.4% h100_mfu=33.49% vram=397/17,338MB peak=16,747/17,338MB
[step 174/8250] loss=2.6388 lr=5.73e-05 muon_lr=5.73e-04 grad_norm=0.7043 tok/s=1,434,664 raw_tok/s=1,441,605 step_tokens=130,441 waste=0.5% h100_mfu=35.07% vram=397/17,338MB peak=16,747/17,338MB
[step 175/8250] loss=2.6431 lr=5.76e-05 muon_lr=5.76e-04 grad_norm=0.6486 tok/s=1,448,502 raw_tok/s=1,452,425 step_tokens=130,718 waste=0.3% h100_mfu=35.34% vram=397/17,338MB peak=16,747/17,338MB
[step 176/8250] loss=2.6380 lr=5.79e-05 muon_lr=5.79e-04 grad_norm=0.5790 tok/s=1,407,814 raw_tok/s=1,410,537 step_tokens=130,819 waste=0.2% h100_mfu=34.32% vram=397/17,338MB peak=16,747/17,338MB
[step 177/8250] loss=2.6376 lr=5.83e-05 muon_lr=5.83e-04 grad_norm=0.5317 tok/s=1,444,426 raw_tok/s=1,449,669 step_tokens=130,598 waste=0.4% h100_mfu=35.27% vram=397/17,338MB peak=16,747/17,338MB
[step 178/8250] loss=2.6646 lr=5.86e-05 muon_lr=5.86e-04 grad_norm=0.6342 tok/s=1,415,336 raw_tok/s=1,419,854 step_tokens=130,655 waste=0.3% h100_mfu=34.55% vram=397/17,338MB peak=16,747/17,338MB
[step 179/8250] loss=2.6392 lr=5.89e-05 muon_lr=5.89e-04 grad_norm=0.5500 tok/s=1,448,512 raw_tok/s=1,451,780 step_tokens=130,777 waste=0.2% h100_mfu=35.32% vram=397/17,338MB peak=16,747/17,338MB
[step 180/8250] loss=2.6482 lr=5.93e-05 muon_lr=5.93e-04 grad_norm=0.5065 tok/s=1,348,521 raw_tok/s=1,352,618 step_tokens=130,675 waste=0.3% h100_mfu=32.91% vram=397/17,338MB peak=16,747/17,338MB
[step 181/8250] loss=2.6489 lr=5.96e-05 muon_lr=5.96e-04 grad_norm=0.7693 tok/s=1,482,204 raw_tok/s=1,484,651 step_tokens=130,856 waste=0.2% h100_mfu=36.12% vram=397/17,338MB peak=16,747/17,338MB
[step 182/8250] loss=2.6488 lr=5.99e-05 muon_lr=5.99e-04 grad_norm=0.6907 tok/s=1,438,439 raw_tok/s=1,440,769 step_tokens=130,860 waste=0.2% h100_mfu=35.05% vram=397/17,338MB peak=16,747/17,338MB
[step 183/8250] loss=2.6317 lr=6.03e-05 muon_lr=6.03e-04 grad_norm=0.5382 tok/s=1,515,385 raw_tok/s=1,520,466 step_tokens=130,634 waste=0.3% h100_mfu=36.99% vram=397/17,338MB peak=16,747/17,338MB
[step 184/8250] loss=2.6523 lr=6.06e-05 muon_lr=6.06e-04 grad_norm=0.5649 tok/s=1,445,447 raw_tok/s=1,445,910 step_tokens=131,030 waste=0.0% h100_mfu=35.18% vram=397/17,338MB peak=16,747/17,338MB
[step 185/8250] loss=2.6337 lr=6.09e-05 muon_lr=6.09e-04 grad_norm=0.6580 tok/s=1,424,849 raw_tok/s=1,425,774 step_tokens=130,987 waste=0.1% h100_mfu=34.69% vram=397/17,338MB peak=16,747/17,338MB
[step 186/8250] loss=2.6410 lr=6.13e-05 muon_lr=6.13e-04 grad_norm=0.6614 tok/s=1,432,055 raw_tok/s=1,439,523 step_tokens=130,392 waste=0.5% h100_mfu=35.02% vram=397/17,338MB peak=16,747/17,338MB
[step 187/8250] loss=2.6409 lr=6.16e-05 muon_lr=6.16e-04 grad_norm=0.7713 tok/s=1,448,576 raw_tok/s=1,452,444 step_tokens=130,723 waste=0.3% h100_mfu=35.34% vram=397/17,338MB peak=16,747/17,338MB
[step 188/8250] loss=2.6359 lr=6.19e-05 muon_lr=6.19e-04 grad_norm=0.5861 tok/s=1,442,825 raw_tok/s=1,444,357 step_tokens=130,933 waste=0.1% h100_mfu=35.14% vram=397/17,338MB peak=16,747/17,338MB
[step 189/8250] loss=2.6472 lr=6.23e-05 muon_lr=6.23e-04 grad_norm=0.4662 tok/s=1,432,207 raw_tok/s=1,438,077 step_tokens=130,537 waste=0.4% h100_mfu=34.99% vram=397/17,338MB peak=16,747/17,338MB
[step 190/8250] loss=2.6285 lr=6.26e-05 muon_lr=6.26e-04 grad_norm=0.6059 tok/s=1,450,588 raw_tok/s=1,454,172 step_tokens=130,749 waste=0.2% h100_mfu=35.38% vram=397/17,338MB peak=16,747/17,338MB
[step 191/8250] loss=2.6509 lr=6.29e-05 muon_lr=6.29e-04 grad_norm=0.7228 tok/s=1,434,033 raw_tok/s=1,439,965 step_tokens=130,532 waste=0.4% h100_mfu=35.03% vram=397/17,338MB peak=16,747/17,338MB
[step 192/8250] loss=2.6446 lr=6.32e-05 muon_lr=6.32e-04 grad_norm=0.6339 tok/s=1,386,721 raw_tok/s=1,388,140 step_tokens=130,938 waste=0.1% h100_mfu=33.77% vram=397/17,338MB peak=16,747/17,338MB
[step 193/8250] loss=2.6426 lr=6.36e-05 muon_lr=6.36e-04 grad_norm=0.5896 tok/s=1,408,626 raw_tok/s=1,410,983 step_tokens=130,853 waste=0.2% h100_mfu=34.33% vram=397/17,338MB peak=16,747/17,338MB
[step 194/8250] loss=2.6224 lr=6.39e-05 muon_lr=6.39e-04 grad_norm=0.5513 tok/s=1,446,183 raw_tok/s=1,451,332 step_tokens=130,607 waste=0.4% h100_mfu=35.31% vram=397/17,338MB peak=16,747/17,338MB
[step 195/8250] loss=2.6280 lr=6.42e-05 muon_lr=6.42e-04 grad_norm=0.5906 tok/s=1,315,563 raw_tok/s=1,321,147 step_tokens=130,518 waste=0.4% h100_mfu=32.14% vram=397/17,338MB peak=16,747/17,338MB
[step 196/8250] loss=2.6378 lr=6.46e-05 muon_lr=6.46e-04 grad_norm=0.7212 tok/s=1,446,788 raw_tok/s=1,448,567 step_tokens=130,911 waste=0.1% h100_mfu=35.24% vram=397/17,338MB peak=16,747/17,338MB
[step 197/8250] loss=2.6371 lr=6.49e-05 muon_lr=6.49e-04 grad_norm=0.5026 tok/s=1,489,515 raw_tok/s=1,492,864 step_tokens=130,778 waste=0.2% h100_mfu=36.32% vram=397/17,338MB peak=16,747/17,338MB
[step 198/8250] loss=2.6375 lr=6.52e-05 muon_lr=6.52e-04 grad_norm=0.5753 tok/s=1,432,845 raw_tok/s=1,438,221 step_tokens=130,582 waste=0.4% h100_mfu=34.99% vram=397/17,338MB peak=16,747/17,338MB
[step 199/8250] loss=2.6476 lr=6.56e-05 muon_lr=6.56e-04 grad_norm=0.5923 tok/s=1,399,195 raw_tok/s=1,403,467 step_tokens=130,673 waste=0.3% h100_mfu=34.15% vram=397/17,338MB peak=16,747/17,338MB
[step 200/8250] loss=2.6448 lr=6.59e-05 muon_lr=6.59e-04 grad_norm=0.5316 tok/s=1,434,968 raw_tok/s=1,436,897 step_tokens=130,896 waste=0.1% h100_mfu=34.96% vram=397/17,338MB peak=16,747/17,338MB
[step 201/8250] loss=2.6414 lr=6.62e-05 muon_lr=6.62e-04 grad_norm=0.4653 tok/s=1,419,248 raw_tok/s=1,420,028 step_tokens=131,000 waste=0.1% h100_mfu=34.55% vram=397/17,338MB peak=16,747/17,338MB
[step 202/8250] loss=2.6416 lr=6.66e-05 muon_lr=6.66e-04 grad_norm=0.5267 tok/s=1,366,204 raw_tok/s=1,372,235 step_tokens=130,496 waste=0.4% h100_mfu=33.39% vram=397/17,338MB peak=16,747/17,338MB
[step 203/8250] loss=2.6392 lr=6.69e-05 muon_lr=6.69e-04 grad_norm=0.7439 tok/s=1,498,107 raw_tok/s=1,500,018 step_tokens=130,905 waste=0.1% h100_mfu=36.50% vram=397/17,338MB peak=16,747/17,338MB
[step 204/8250] loss=2.6335 lr=6.72e-05 muon_lr=6.72e-04 grad_norm=0.6008 tok/s=1,428,896 raw_tok/s=1,431,156 step_tokens=130,865 waste=0.2% h100_mfu=34.82% vram=397/17,338MB peak=16,747/17,338MB
[step 205/8250] loss=2.6402 lr=6.75e-05 muon_lr=6.75e-04 grad_norm=0.5804 tok/s=1,381,618 raw_tok/s=1,384,702 step_tokens=130,780 waste=0.2% h100_mfu=33.69% vram=397/17,338MB peak=16,747/17,338MB
[step 206/8250] loss=2.6333 lr=6.79e-05 muon_lr=6.79e-04 grad_norm=0.4932 tok/s=1,323,487 raw_tok/s=1,326,615 step_tokens=130,763 waste=0.2% h100_mfu=32.28% vram=397/17,338MB peak=16,747/17,338MB
[step 207/8250] loss=2.6457 lr=6.82e-05 muon_lr=6.82e-04 grad_norm=0.5594 tok/s=1,364,822 raw_tok/s=1,367,252 step_tokens=130,839 waste=0.2% h100_mfu=33.27% vram=397/17,338MB peak=16,747/17,338MB
[step 208/8250] loss=2.6355 lr=6.85e-05 muon_lr=6.85e-04 grad_norm=0.4910 tok/s=1,383,312 raw_tok/s=1,387,939 step_tokens=130,635 waste=0.3% h100_mfu=33.77% vram=397/17,338MB peak=16,747/17,338MB
[step 209/8250] loss=2.6421 lr=6.89e-05 muon_lr=6.89e-04 grad_norm=0.4104 tok/s=1,359,069 raw_tok/s=1,360,668 step_tokens=130,918 waste=0.1% h100_mfu=33.11% vram=397/17,338MB peak=16,747/17,338MB
[step 210/8250] loss=2.6368 lr=6.92e-05 muon_lr=6.92e-04 grad_norm=0.6612 tok/s=1,400,437 raw_tok/s=1,402,984 step_tokens=130,834 waste=0.2% h100_mfu=34.13% vram=397/17,338MB peak=16,747/17,338MB
[step 211/8250] loss=2.6319 lr=6.95e-05 muon_lr=6.95e-04 grad_norm=0.5802 tok/s=1,446,313 raw_tok/s=1,449,310 step_tokens=130,801 waste=0.2% h100_mfu=35.26% vram=397/17,338MB peak=16,747/17,338MB
[step 212/8250] loss=2.6297 lr=6.99e-05 muon_lr=6.99e-04 grad_norm=0.5051 tok/s=1,384,087 raw_tok/s=1,384,826 step_tokens=131,002 waste=0.1% h100_mfu=33.69% vram=397/17,338MB peak=16,747/17,338MB
[step 213/8250] loss=2.6276 lr=7.02e-05 muon_lr=7.02e-04 grad_norm=0.5206 tok/s=1,388,635 raw_tok/s=1,393,728 step_tokens=130,593 waste=0.4% h100_mfu=33.91% vram=397/17,338MB peak=16,747/17,338MB
[step 214/8250] loss=2.6416 lr=7.05e-05 muon_lr=7.05e-04 grad_norm=0.5268 tok/s=1,438,376 raw_tok/s=1,442,051 step_tokens=130,738 waste=0.3% h100_mfu=35.09% vram=397/17,338MB peak=16,747/17,338MB
[step 215/8250] loss=2.6248 lr=7.09e-05 muon_lr=7.09e-04 grad_norm=0.5031 tok/s=1,482,905 raw_tok/s=1,484,978 step_tokens=130,889 waste=0.1% h100_mfu=36.13% vram=397/17,338MB peak=16,747/17,338MB
[step 216/8250] loss=2.6317 lr=7.12e-05 muon_lr=7.12e-04 grad_norm=0.6213 tok/s=1,463,231 raw_tok/s=1,471,855 step_tokens=130,304 waste=0.6% h100_mfu=35.81% vram=397/17,338MB peak=16,747/17,338MB
[step 217/8250] loss=2.6378 lr=7.15e-05 muon_lr=7.15e-04 grad_norm=0.5364 tok/s=1,426,040 raw_tok/s=1,428,557 step_tokens=130,841 waste=0.2% h100_mfu=34.76% vram=397/17,338MB peak=16,747/17,338MB
[step 218/8250] loss=2.6348 lr=7.19e-05 muon_lr=7.19e-04 grad_norm=0.5785 tok/s=1,463,400 raw_tok/s=1,467,846 step_tokens=130,675 waste=0.3% h100_mfu=35.71% vram=397/17,338MB peak=16,747/17,338MB
[step 219/8250] loss=2.6397 lr=7.22e-05 muon_lr=7.22e-04 grad_norm=0.5101 tok/s=1,399,037 raw_tok/s=1,399,667 step_tokens=131,013 waste=0.0% h100_mfu=34.05% vram=397/17,338MB peak=16,747/17,338MB
[step 220/8250] loss=2.6441 lr=7.25e-05 muon_lr=7.25e-04 grad_norm=0.3444 tok/s=1,402,814 raw_tok/s=1,410,065 step_tokens=130,398 waste=0.5% h100_mfu=34.31% vram=397/17,338MB peak=16,747/17,338MB
[step 221/8250] loss=2.6373 lr=7.28e-05 muon_lr=7.28e-04 grad_norm=0.5235 tok/s=1,420,954 raw_tok/s=1,422,038 step_tokens=130,972 waste=0.1% h100_mfu=34.60% vram=397/17,338MB peak=16,747/17,338MB
[step 222/8250] loss=2.6248 lr=7.32e-05 muon_lr=7.32e-04 grad_norm=0.5201 tok/s=1,340,519 raw_tok/s=1,344,119 step_tokens=130,721 waste=0.3% h100_mfu=32.70% vram=397/17,338MB peak=16,747/17,338MB
[step 223/8250] loss=2.6344 lr=7.35e-05 muon_lr=7.35e-04 grad_norm=0.4700 tok/s=1,512,562 raw_tok/s=1,516,449 step_tokens=130,736 waste=0.3% h100_mfu=36.90% vram=397/17,338MB peak=16,747/17,338MB
[step 224/8250] loss=2.6371 lr=7.38e-05 muon_lr=7.38e-04 grad_norm=0.6038 tok/s=1,505,320 raw_tok/s=1,506,849 step_tokens=130,939 waste=0.1% h100_mfu=36.66% vram=397/17,338MB peak=16,747/17,338MB
[step 225/8250] loss=2.6254 lr=7.42e-05 muon_lr=7.42e-04 grad_norm=0.5873 tok/s=1,432,196 raw_tok/s=1,436,327 step_tokens=130,695 waste=0.3% h100_mfu=34.95% vram=397/17,338MB peak=16,747/17,338MB
[step 226/8250] loss=2.6321 lr=7.45e-05 muon_lr=7.45e-04 grad_norm=0.6020 tok/s=1,502,684 raw_tok/s=1,506,039 step_tokens=130,780 waste=0.2% h100_mfu=36.64% vram=397/17,338MB peak=16,747/17,338MB
[step 227/8250] loss=2.6291 lr=7.48e-05 muon_lr=7.48e-04 grad_norm=0.3917 tok/s=1,490,774 raw_tok/s=1,493,361 step_tokens=130,845 waste=0.2% h100_mfu=36.33% vram=397/17,338MB peak=16,747/17,338MB
[step 228/8250] loss=2.6174 lr=7.52e-05 muon_lr=7.52e-04 grad_norm=0.5925 tok/s=1,509,232 raw_tok/s=1,513,922 step_tokens=130,666 waste=0.3% h100_mfu=36.83% vram=397/17,338MB peak=16,747/17,338MB
[step 229/8250] loss=2.6307 lr=7.55e-05 muon_lr=7.55e-04 grad_norm=0.6141 tok/s=1,514,941 raw_tok/s=1,522,841 step_tokens=130,392 waste=0.5% h100_mfu=37.05% vram=397/17,338MB peak=16,747/17,338MB
[step 230/8250] loss=2.6334 lr=7.58e-05 muon_lr=7.58e-04 grad_norm=0.5472 tok/s=1,368,403 raw_tok/s=1,375,192 step_tokens=130,425 waste=0.5% h100_mfu=33.46% vram=397/17,338MB peak=16,747/17,338MB
[step 231/8250] loss=2.6261 lr=7.62e-05 muon_lr=7.62e-04 grad_norm=0.3974 tok/s=1,418,881 raw_tok/s=1,420,279 step_tokens=130,943 waste=0.1% h100_mfu=34.56% vram=397/17,338MB peak=16,747/17,338MB
[step 232/8250] loss=2.6351 lr=7.65e-05 muon_lr=7.65e-04 grad_norm=0.5135 tok/s=1,479,060 raw_tok/s=1,480,631 step_tokens=130,933 waste=0.1% h100_mfu=36.02% vram=397/17,338MB peak=16,747/17,338MB
[step 233/8250] loss=2.6368 lr=7.68e-05 muon_lr=7.68e-04 grad_norm=0.5978 tok/s=1,386,926 raw_tok/s=1,392,024 step_tokens=130,592 waste=0.4% h100_mfu=33.87% vram=397/17,338MB peak=16,747/17,338MB
[step 234/8250] loss=2.6298 lr=7.72e-05 muon_lr=7.72e-04 grad_norm=0.4586 tok/s=1,490,453 raw_tok/s=1,493,610 step_tokens=130,795 waste=0.2% h100_mfu=36.34% vram=397/17,338MB peak=16,747/17,338MB
[step 235/8250] loss=2.6269 lr=7.75e-05 muon_lr=7.75e-04 grad_norm=0.5523 tok/s=1,433,280 raw_tok/s=1,437,811 step_tokens=130,659 waste=0.3% h100_mfu=34.98% vram=397/17,338MB peak=16,747/17,338MB
[step 236/8250] loss=2.6163 lr=7.78e-05 muon_lr=7.78e-04 grad_norm=0.5098 tok/s=1,380,509 raw_tok/s=1,384,427 step_tokens=130,701 waste=0.3% h100_mfu=33.68% vram=397/17,338MB peak=16,747/17,338MB
[step 237/8250] loss=2.6256 lr=7.81e-05 muon_lr=7.81e-04 grad_norm=0.4698 tok/s=1,304,076 raw_tok/s=1,307,008 step_tokens=130,778 waste=0.2% h100_mfu=31.80% vram=397/17,338MB peak=16,747/17,338MB
[step 238/8250] loss=2.6265 lr=7.85e-05 muon_lr=7.85e-04 grad_norm=0.5605 tok/s=1,403,525 raw_tok/s=1,404,104 step_tokens=131,018 waste=0.0% h100_mfu=34.16% vram=397/17,338MB peak=16,747/17,338MB
[step 239/8250] loss=2.6311 lr=7.88e-05 muon_lr=7.88e-04 grad_norm=0.4637 tok/s=1,403,429 raw_tok/s=1,406,531 step_tokens=130,783 waste=0.2% h100_mfu=34.22% vram=397/17,338MB peak=16,747/17,338MB
[step 240/8250] loss=2.6351 lr=7.91e-05 muon_lr=7.91e-04 grad_norm=0.5217 tok/s=1,435,473 raw_tok/s=1,441,853 step_tokens=130,492 waste=0.4% h100_mfu=35.08% vram=397/17,338MB peak=16,747/17,338MB
[step 241/8250] loss=2.6284 lr=7.95e-05 muon_lr=7.95e-04 grad_norm=0.5697 tok/s=1,424,579 raw_tok/s=1,427,061 step_tokens=130,844 waste=0.2% h100_mfu=34.72% vram=397/17,338MB peak=16,747/17,338MB
[step 242/8250] loss=2.6319 lr=7.98e-05 muon_lr=7.98e-04 grad_norm=0.5052 tok/s=1,426,188 raw_tok/s=1,428,586 step_tokens=130,852 waste=0.2% h100_mfu=34.76% vram=397/17,338MB peak=16,747/17,338MB
[step 243/8250] loss=2.6175 lr=8.01e-05 muon_lr=8.01e-04 grad_norm=0.5137 tok/s=1,426,525 raw_tok/s=1,431,494 step_tokens=130,617 waste=0.3% h100_mfu=34.83% vram=397/17,338MB peak=16,747/17,338MB
[step 244/8250] loss=2.6158 lr=8.05e-05 muon_lr=8.05e-04 grad_norm=0.5991 tok/s=1,337,688 raw_tok/s=1,340,921 step_tokens=130,756 waste=0.2% h100_mfu=32.62% vram=397/17,338MB peak=16,747/17,338MB
[step 245/8250] loss=2.6351 lr=8.08e-05 muon_lr=8.08e-04 grad_norm=0.4543 tok/s=1,444,538 raw_tok/s=1,447,719 step_tokens=130,784 waste=0.2% h100_mfu=35.22% vram=397/17,338MB peak=16,747/17,338MB
[step 246/8250] loss=2.6329 lr=8.11e-05 muon_lr=8.11e-04 grad_norm=0.5041 tok/s=1,424,973 raw_tok/s=1,428,963 step_tokens=130,706 waste=0.3% h100_mfu=34.77% vram=397/17,338MB peak=16,747/17,338MB
[step 247/8250] loss=2.6229 lr=8.15e-05 muon_lr=8.15e-04 grad_norm=0.5804 tok/s=1,426,608 raw_tok/s=1,429,509 step_tokens=130,806 waste=0.2% h100_mfu=34.78% vram=397/17,338MB peak=16,747/17,338MB
[step 248/8250] loss=2.6078 lr=8.18e-05 muon_lr=8.18e-04 grad_norm=0.4784 tok/s=1,496,284 raw_tok/s=1,498,616 step_tokens=130,868 waste=0.2% h100_mfu=36.46% vram=397/17,338MB peak=16,747/17,338MB
[step 249/8250] loss=2.6363 lr=8.21e-05 muon_lr=8.21e-04 grad_norm=0.5393 tok/s=1,421,038 raw_tok/s=1,425,956 step_tokens=130,620 waste=0.3% h100_mfu=34.69% vram=397/17,338MB peak=16,747/17,338MB
[step 250/8250] loss=2.6411 lr=8.25e-05 muon_lr=8.25e-04 grad_norm=0.4748 tok/s=1,436,427 raw_tok/s=1,439,700 step_tokens=130,774 waste=0.2% h100_mfu=35.03% vram=397/17,338MB peak=16,747/17,338MB
[step 250] eval_loss=2.6261
[step 251/8250] loss=2.6257 lr=8.28e-05 muon_lr=8.28e-04 grad_norm=0.4935 tok/s=1,464,176 raw_tok/s=1,465,025 step_tokens=130,996 waste=0.1% h100_mfu=35.64% vram=397/17,340MB peak=16,750/17,340MB
[step 252/8250] loss=2.6356 lr=8.31e-05 muon_lr=8.31e-04 grad_norm=0.5563 tok/s=1,413,393 raw_tok/s=1,417,731 step_tokens=130,671 waste=0.3% h100_mfu=34.49% vram=397/17,340MB peak=16,747/17,340MB
[step 253/8250] loss=2.6247 lr=8.34e-05 muon_lr=8.34e-04 grad_norm=0.4645 tok/s=1,397,604 raw_tok/s=1,399,600 step_tokens=130,885 waste=0.1% h100_mfu=34.05% vram=397/17,340MB peak=16,747/17,340MB
[step 254/8250] loss=2.6192 lr=8.38e-05 muon_lr=8.38e-04 grad_norm=0.4582 tok/s=1,359,595 raw_tok/s=1,360,425 step_tokens=130,992 waste=0.1% h100_mfu=33.10% vram=397/17,340MB peak=16,747/17,340MB
[step 255/8250] loss=2.6280 lr=8.41e-05 muon_lr=8.41e-04 grad_norm=0.4465 tok/s=1,480,429 raw_tok/s=1,486,052 step_tokens=130,576 waste=0.4% h100_mfu=36.16% vram=397/17,340MB peak=16,747/17,340MB
[step 256/8250] loss=2.6083 lr=8.44e-05 muon_lr=8.44e-04 grad_norm=0.5035 tok/s=1,395,486 raw_tok/s=1,400,090 step_tokens=130,641 waste=0.3% h100_mfu=34.06% vram=397/17,340MB peak=16,747/17,340MB
[step 257/8250] loss=2.6168 lr=8.48e-05 muon_lr=8.48e-04 grad_norm=0.5189 tok/s=1,430,628 raw_tok/s=1,434,206 step_tokens=130,745 waste=0.2% h100_mfu=34.89% vram=397/17,340MB peak=16,747/17,340MB
[step 258/8250] loss=2.6286 lr=8.51e-05 muon_lr=8.51e-04 grad_norm=0.4589 tok/s=1,439,544 raw_tok/s=1,445,632 step_tokens=130,520 waste=0.4% h100_mfu=35.17% vram=397/17,340MB peak=16,747/17,340MB
[step 259/8250] loss=2.6078 lr=8.54e-05 muon_lr=8.54e-04 grad_norm=0.4412 tok/s=1,444,256 raw_tok/s=1,448,400 step_tokens=130,697 waste=0.3% h100_mfu=35.24% vram=397/17,340MB peak=16,747/17,340MB
[step 260/8250] loss=2.6264 lr=8.58e-05 muon_lr=8.58e-04 grad_norm=0.5025 tok/s=1,515,216 raw_tok/s=1,519,971 step_tokens=130,662 waste=0.3% h100_mfu=36.98% vram=397/17,340MB peak=16,747/17,340MB
[step 261/8250] loss=2.6157 lr=8.61e-05 muon_lr=8.61e-04 grad_norm=0.4326 tok/s=1,413,753 raw_tok/s=1,418,168 step_tokens=130,664 waste=0.3% h100_mfu=34.50% vram=397/17,340MB peak=16,747/17,340MB
[step 262/8250] loss=2.6227 lr=8.64e-05 muon_lr=8.64e-04 grad_norm=0.3693 tok/s=1,412,186 raw_tok/s=1,417,333 step_tokens=130,596 waste=0.4% h100_mfu=34.48% vram=397/17,340MB peak=16,747/17,340MB
[step 263/8250] loss=2.6256 lr=8.68e-05 muon_lr=8.68e-04 grad_norm=0.5449 tok/s=1,360,506 raw_tok/s=1,362,450 step_tokens=130,885 waste=0.1% h100_mfu=33.15% vram=397/17,340MB peak=16,747/17,340MB
[step 264/8250] loss=2.6162 lr=8.71e-05 muon_lr=8.71e-04 grad_norm=0.4231 tok/s=1,273,682 raw_tok/s=1,275,794 step_tokens=130,855 waste=0.2% h100_mfu=31.04% vram=397/17,340MB peak=16,747/17,340MB
[step 265/8250] loss=2.6338 lr=8.74e-05 muon_lr=8.74e-04 grad_norm=0.4569 tok/s=1,328,834 raw_tok/s=1,331,608 step_tokens=130,799 waste=0.2% h100_mfu=32.40% vram=397/17,340MB peak=16,747/17,340MB
[step 266/8250] loss=2.6149 lr=8.77e-05 muon_lr=8.77e-04 grad_norm=0.5624 tok/s=1,390,184 raw_tok/s=1,391,894 step_tokens=130,911 waste=0.1% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 267/8250] loss=2.6317 lr=8.81e-05 muon_lr=8.81e-04 grad_norm=0.3204 tok/s=1,423,528 raw_tok/s=1,432,039 step_tokens=130,293 waste=0.6% h100_mfu=34.84% vram=397/17,340MB peak=16,747/17,340MB
[step 268/8250] loss=2.6184 lr=8.84e-05 muon_lr=8.84e-04 grad_norm=0.3403 tok/s=1,423,060 raw_tok/s=1,426,390 step_tokens=130,766 waste=0.2% h100_mfu=34.70% vram=397/17,340MB peak=16,747/17,340MB
[step 269/8250] loss=2.6277 lr=8.87e-05 muon_lr=8.87e-04 grad_norm=0.4893 tok/s=1,294,583 raw_tok/s=1,297,752 step_tokens=130,752 waste=0.2% h100_mfu=31.57% vram=397/17,340MB peak=16,747/17,340MB
[step 270/8250] loss=2.6202 lr=8.91e-05 muon_lr=8.91e-04 grad_norm=0.6060 tok/s=1,406,825 raw_tok/s=1,411,315 step_tokens=130,655 waste=0.3% h100_mfu=34.34% vram=397/17,340MB peak=16,747/17,340MB
[step 271/8250] loss=2.6305 lr=8.94e-05 muon_lr=8.94e-04 grad_norm=0.4406 tok/s=1,451,339 raw_tok/s=1,457,053 step_tokens=130,558 waste=0.4% h100_mfu=35.45% vram=397/17,340MB peak=16,747/17,340MB
[step 272/8250] loss=2.6246 lr=8.97e-05 muon_lr=8.97e-04 grad_norm=0.6007 tok/s=1,227,888 raw_tok/s=1,229,389 step_tokens=130,912 waste=0.1% h100_mfu=29.91% vram=397/17,340MB peak=16,747/17,340MB
[step 273/8250] loss=2.6143 lr=9.01e-05 muon_lr=9.01e-04 grad_norm=0.4919 tok/s=1,295,640 raw_tok/s=1,297,501 step_tokens=130,884 waste=0.1% h100_mfu=31.57% vram=397/17,340MB peak=16,747/17,340MB
[step 274/8250] loss=2.6227 lr=9.04e-05 muon_lr=9.04e-04 grad_norm=0.5120 tok/s=1,434,691 raw_tok/s=1,438,423 step_tokens=130,732 waste=0.3% h100_mfu=35.00% vram=397/17,340MB peak=16,747/17,340MB
[step 275/8250] loss=2.6370 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3816 tok/s=1,397,295 raw_tok/s=1,399,698 step_tokens=130,847 waste=0.2% h100_mfu=34.05% vram=397/17,340MB peak=16,747/17,340MB
[step 276/8250] loss=2.6188 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4874 tok/s=1,436,254 raw_tok/s=1,436,682 step_tokens=131,033 waste=0.0% h100_mfu=34.95% vram=397/17,340MB peak=16,747/17,340MB
[step 277/8250] loss=2.6218 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.5825 tok/s=1,417,973 raw_tok/s=1,418,991 step_tokens=130,978 waste=0.1% h100_mfu=34.52% vram=397/17,340MB peak=16,747/17,340MB
[step 278/8250] loss=2.6091 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.5503 tok/s=1,404,548 raw_tok/s=1,408,837 step_tokens=130,673 waste=0.3% h100_mfu=34.28% vram=397/17,340MB peak=16,747/17,340MB
[step 279/8250] loss=2.6341 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.4512 tok/s=1,392,815 raw_tok/s=1,394,283 step_tokens=130,934 waste=0.1% h100_mfu=33.92% vram=397/17,340MB peak=16,747/17,340MB
[step 280/8250] loss=2.6213 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.3912 tok/s=1,442,978 raw_tok/s=1,445,249 step_tokens=130,866 waste=0.2% h100_mfu=35.16% vram=397/17,340MB peak=16,747/17,340MB
[step 281/8250] loss=2.6146 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.5478 tok/s=1,370,308 raw_tok/s=1,373,935 step_tokens=130,726 waste=0.3% h100_mfu=33.43% vram=397/17,340MB peak=16,747/17,340MB
[step 282/8250] loss=2.6266 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.4002 tok/s=1,409,830 raw_tok/s=1,413,389 step_tokens=130,742 waste=0.3% h100_mfu=34.39% vram=397/17,340MB peak=16,747/17,340MB
[step 283/8250] loss=2.6140 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3773 tok/s=1,352,833 raw_tok/s=1,358,149 step_tokens=130,559 waste=0.4% h100_mfu=33.04% vram=397/17,340MB peak=16,747/17,340MB
[step 284/8250] loss=2.6211 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.5227 tok/s=1,416,590 raw_tok/s=1,418,365 step_tokens=130,908 waste=0.1% h100_mfu=34.51% vram=397/17,340MB peak=16,747/17,340MB
[step 285/8250] loss=2.6219 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.4854 tok/s=1,429,749 raw_tok/s=1,431,989 step_tokens=130,867 waste=0.2% h100_mfu=34.84% vram=397/17,340MB peak=16,747/17,340MB
[step 286/8250] loss=2.6087 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.5288 tok/s=1,422,887 raw_tok/s=1,427,592 step_tokens=130,640 waste=0.3% h100_mfu=34.73% vram=397/17,340MB peak=16,747/17,340MB
[step 287/8250] loss=2.6174 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.4264 tok/s=1,396,677 raw_tok/s=1,398,427 step_tokens=130,908 waste=0.1% h100_mfu=34.02% vram=397/17,340MB peak=16,747/17,340MB
[step 288/8250] loss=2.6260 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.4108 tok/s=1,435,227 raw_tok/s=1,438,465 step_tokens=130,777 waste=0.2% h100_mfu=35.00% vram=397/17,340MB peak=16,747/17,340MB
[step 289/8250] loss=2.6118 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.4104 tok/s=1,427,236 raw_tok/s=1,429,155 step_tokens=130,896 waste=0.1% h100_mfu=34.77% vram=397/17,340MB peak=16,747/17,340MB
[step 290/8250] loss=2.6102 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.4978 tok/s=1,346,979 raw_tok/s=1,350,059 step_tokens=130,773 waste=0.2% h100_mfu=32.85% vram=397/17,340MB peak=16,747/17,340MB
[step 291/8250] loss=2.6174 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.4679 tok/s=1,296,090 raw_tok/s=1,297,813 step_tokens=130,898 waste=0.1% h100_mfu=31.58% vram=397/17,340MB peak=16,747/17,340MB
[step 292/8250] loss=2.6251 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.4765 tok/s=1,505,784 raw_tok/s=1,509,273 step_tokens=130,769 waste=0.2% h100_mfu=36.72% vram=397/17,340MB peak=16,747/17,340MB
[step 293/8250] loss=2.6253 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.5800 tok/s=1,318,566 raw_tok/s=1,324,244 step_tokens=130,510 waste=0.4% h100_mfu=32.22% vram=397/17,340MB peak=16,747/17,340MB
[step 294/8250] loss=2.6190 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.5751 tok/s=1,401,604 raw_tok/s=1,405,024 step_tokens=130,753 waste=0.2% h100_mfu=34.18% vram=397/17,340MB peak=16,747/17,340MB
[step 295/8250] loss=2.6261 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.4275 tok/s=1,355,697 raw_tok/s=1,359,139 step_tokens=130,740 waste=0.3% h100_mfu=33.07% vram=397/17,340MB peak=16,747/17,340MB
[step 296/8250] loss=2.6223 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.3989 tok/s=1,379,888 raw_tok/s=1,382,367 step_tokens=130,837 waste=0.2% h100_mfu=33.63% vram=397/17,340MB peak=16,747/17,340MB
[step 297/8250] loss=2.6173 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.4103 tok/s=1,468,997 raw_tok/s=1,474,136 step_tokens=130,615 waste=0.3% h100_mfu=35.87% vram=397/17,340MB peak=16,747/17,340MB
[step 298/8250] loss=2.6155 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.4576 tok/s=1,382,146 raw_tok/s=1,385,772 step_tokens=130,729 waste=0.3% h100_mfu=33.72% vram=397/17,340MB peak=16,747/17,340MB
[step 299/8250] loss=2.6233 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.4378 tok/s=1,385,684 raw_tok/s=1,392,067 step_tokens=130,471 waste=0.5% h100_mfu=33.87% vram=397/17,340MB peak=16,747/17,340MB
[step 300/8250] loss=2.6082 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.4755 tok/s=1,387,466 raw_tok/s=1,392,661 step_tokens=130,583 waste=0.4% h100_mfu=33.88% vram=397/17,340MB peak=16,747/17,340MB
[step 301/8250] loss=2.6210 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.4060 tok/s=1,415,887 raw_tok/s=1,420,514 step_tokens=130,645 waste=0.3% h100_mfu=34.56% vram=397/17,340MB peak=16,747/17,340MB
[step 302/8250] loss=2.5975 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.4165 tok/s=1,370,272 raw_tok/s=1,372,136 step_tokens=130,894 waste=0.1% h100_mfu=33.38% vram=397/17,340MB peak=16,747/17,340MB
[step 303/8250] loss=2.6096 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3481 tok/s=1,405,233 raw_tok/s=1,409,567 step_tokens=130,669 waste=0.3% h100_mfu=34.29% vram=397/17,340MB peak=16,747/17,340MB
[step 304/8250] loss=2.6135 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3575 tok/s=1,432,411 raw_tok/s=1,434,117 step_tokens=130,916 waste=0.1% h100_mfu=34.89% vram=397/17,340MB peak=16,747/17,340MB
[step 305/8250] loss=2.6202 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.4878 tok/s=1,378,859 raw_tok/s=1,385,157 step_tokens=130,476 waste=0.5% h100_mfu=33.70% vram=397/17,340MB peak=16,747/17,340MB
[step 306/8250] loss=2.6234 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3430 tok/s=1,213,169 raw_tok/s=1,219,402 step_tokens=130,402 waste=0.5% h100_mfu=29.67% vram=397/17,340MB peak=16,747/17,340MB
[step 307/8250] loss=2.5977 lr=1.00e-04 muon_lr=1.00e-03 grad_norm=0.3027 tok/s=1,367,818 raw_tok/s=1,371,103 step_tokens=130,758 waste=0.2% h100_mfu=33.36% vram=397/17,340MB peak=16,747/17,340MB
[step 308/8250] loss=2.6151 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.3838 tok/s=1,411,512 raw_tok/s=1,413,853 step_tokens=130,855 waste=0.2% h100_mfu=34.40% vram=397/17,340MB peak=16,747/17,340MB
[step 309/8250] loss=2.6163 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4398 tok/s=1,295,993 raw_tok/s=1,300,767 step_tokens=130,591 waste=0.4% h100_mfu=31.65% vram=397/17,340MB peak=16,747/17,340MB
[step 310/8250] loss=2.6250 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.5313 tok/s=1,410,341 raw_tok/s=1,414,940 step_tokens=130,646 waste=0.3% h100_mfu=34.43% vram=397/17,340MB peak=16,747/17,340MB
[step 311/8250] loss=2.6196 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.5710 tok/s=1,329,374 raw_tok/s=1,330,338 step_tokens=130,977 waste=0.1% h100_mfu=32.37% vram=397/17,340MB peak=16,747/17,340MB
[step 312/8250] loss=2.6202 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4933 tok/s=1,256,357 raw_tok/s=1,259,962 step_tokens=130,697 waste=0.3% h100_mfu=30.65% vram=397/17,340MB peak=16,747/17,340MB
[step 313/8250] loss=2.6183 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.3817 tok/s=1,422,726 raw_tok/s=1,425,532 step_tokens=130,814 waste=0.2% h100_mfu=34.68% vram=397/17,340MB peak=16,747/17,340MB
[step 314/8250] loss=2.6096 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4111 tok/s=1,390,094 raw_tok/s=1,393,859 step_tokens=130,718 waste=0.3% h100_mfu=33.91% vram=397/17,340MB peak=16,747/17,340MB
[step 315/8250] loss=2.6094 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.4332 tok/s=1,388,401 raw_tok/s=1,391,852 step_tokens=130,747 waste=0.2% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 316/8250] loss=2.6139 lr=9.99e-05 muon_lr=9.99e-04 grad_norm=0.3509 tok/s=1,325,979 raw_tok/s=1,328,330 step_tokens=130,840 waste=0.2% h100_mfu=32.32% vram=397/17,340MB peak=16,747/17,340MB
[step 317/8250] loss=2.6127 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3622 tok/s=1,361,582 raw_tok/s=1,365,133 step_tokens=130,731 waste=0.3% h100_mfu=33.21% vram=397/17,340MB peak=16,747/17,340MB
[step 318/8250] loss=2.6147 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3869 tok/s=1,410,906 raw_tok/s=1,414,294 step_tokens=130,758 waste=0.2% h100_mfu=34.41% vram=397/17,340MB peak=16,747/17,340MB
[step 319/8250] loss=2.6012 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.2595 tok/s=1,415,677 raw_tok/s=1,420,836 step_tokens=130,596 waste=0.4% h100_mfu=34.57% vram=397/17,340MB peak=16,747/17,340MB
[step 320/8250] loss=2.6056 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3936 tok/s=1,420,680 raw_tok/s=1,421,265 step_tokens=131,018 waste=0.0% h100_mfu=34.58% vram=397/17,340MB peak=16,747/17,340MB
[step 321/8250] loss=2.6161 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4205 tok/s=1,431,942 raw_tok/s=1,432,915 step_tokens=130,983 waste=0.1% h100_mfu=34.86% vram=397/17,340MB peak=16,747/17,340MB
[step 322/8250] loss=2.6064 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3809 tok/s=1,373,089 raw_tok/s=1,374,221 step_tokens=130,964 waste=0.1% h100_mfu=33.43% vram=397/17,340MB peak=16,747/17,340MB
[step 323/8250] loss=2.6055 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4795 tok/s=1,458,051 raw_tok/s=1,460,949 step_tokens=130,812 waste=0.2% h100_mfu=35.55% vram=397/17,340MB peak=16,747/17,340MB
[step 324/8250] loss=2.6113 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.4285 tok/s=1,409,609 raw_tok/s=1,412,627 step_tokens=130,792 waste=0.2% h100_mfu=34.37% vram=397/17,340MB peak=16,747/17,340MB
[step 325/8250] loss=2.6214 lr=9.98e-05 muon_lr=9.98e-04 grad_norm=0.3356 tok/s=1,425,596 raw_tok/s=1,429,850 step_tokens=130,682 waste=0.3% h100_mfu=34.79% vram=397/17,340MB peak=16,747/17,340MB
[step 326/8250] loss=2.6270 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.5251 tok/s=1,428,604 raw_tok/s=1,430,481 step_tokens=130,900 waste=0.1% h100_mfu=34.80% vram=397/17,340MB peak=16,747/17,340MB
[step 327/8250] loss=2.6120 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.4521 tok/s=1,388,955 raw_tok/s=1,393,516 step_tokens=130,643 waste=0.3% h100_mfu=33.90% vram=397/17,340MB peak=16,747/17,340MB
[step 328/8250] loss=2.6126 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3546 tok/s=1,451,706 raw_tok/s=1,458,751 step_tokens=130,439 waste=0.5% h100_mfu=35.49% vram=397/17,340MB peak=16,747/17,340MB
[step 329/8250] loss=2.6068 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3456 tok/s=1,390,072 raw_tok/s=1,392,558 step_tokens=130,838 waste=0.2% h100_mfu=33.88% vram=397/17,340MB peak=16,747/17,340MB
[step 330/8250] loss=2.6088 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3553 tok/s=1,493,199 raw_tok/s=1,500,698 step_tokens=130,417 waste=0.5% h100_mfu=36.51% vram=397/17,340MB peak=16,747/17,340MB
[step 331/8250] loss=2.6321 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.3192 tok/s=1,328,683 raw_tok/s=1,329,840 step_tokens=130,958 waste=0.1% h100_mfu=32.36% vram=397/17,340MB peak=16,747/17,340MB
[step 332/8250] loss=2.6237 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.2814 tok/s=1,407,757 raw_tok/s=1,410,684 step_tokens=130,800 waste=0.2% h100_mfu=34.32% vram=397/17,340MB peak=16,747/17,340MB
[step 333/8250] loss=2.6068 lr=9.97e-05 muon_lr=9.97e-04 grad_norm=0.4252 tok/s=1,308,941 raw_tok/s=1,309,680 step_tokens=130,998 waste=0.1% h100_mfu=31.86% vram=397/17,340MB peak=16,747/17,340MB
[step 334/8250] loss=2.6077 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3848 tok/s=1,223,040 raw_tok/s=1,224,834 step_tokens=130,880 waste=0.1% h100_mfu=29.80% vram=397/17,340MB peak=16,747/17,340MB
[step 335/8250] loss=2.6024 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.4326 tok/s=1,352,619 raw_tok/s=1,354,779 step_tokens=130,863 waste=0.2% h100_mfu=32.96% vram=397/17,340MB peak=16,747/17,340MB
[step 336/8250] loss=2.6165 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.4362 tok/s=1,399,977 raw_tok/s=1,404,724 step_tokens=130,629 waste=0.3% h100_mfu=34.18% vram=397/17,340MB peak=16,747/17,340MB
[step 337/8250] loss=2.6087 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.4287 tok/s=1,424,038 raw_tok/s=1,429,556 step_tokens=130,566 waste=0.4% h100_mfu=34.78% vram=397/17,340MB peak=16,747/17,340MB
[step 338/8250] loss=2.6088 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3339 tok/s=1,437,715 raw_tok/s=1,441,653 step_tokens=130,714 waste=0.3% h100_mfu=35.08% vram=397/17,340MB peak=16,747/17,340MB
[step 339/8250] loss=2.6062 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3483 tok/s=1,454,445 raw_tok/s=1,456,824 step_tokens=130,858 waste=0.2% h100_mfu=35.44% vram=397/17,340MB peak=16,747/17,340MB
[step 340/8250] loss=2.5965 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.3279 tok/s=1,438,921 raw_tok/s=1,442,321 step_tokens=130,763 waste=0.2% h100_mfu=35.09% vram=397/17,340MB peak=16,747/17,340MB
[step 341/8250] loss=2.6042 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.4860 tok/s=1,379,160 raw_tok/s=1,387,086 step_tokens=130,323 waste=0.6% h100_mfu=33.75% vram=397/17,340MB peak=16,747/17,340MB
[step 342/8250] loss=2.6076 lr=9.96e-05 muon_lr=9.96e-04 grad_norm=0.4673 tok/s=1,416,308 raw_tok/s=1,417,779 step_tokens=130,936 waste=0.1% h100_mfu=34.49% vram=397/17,340MB peak=16,747/17,340MB
[step 343/8250] loss=2.5886 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4315 tok/s=1,382,264 raw_tok/s=1,384,398 step_tokens=130,870 waste=0.2% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 344/8250] loss=2.6075 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4130 tok/s=1,474,037 raw_tok/s=1,479,477 step_tokens=130,590 waste=0.4% h100_mfu=36.00% vram=397/17,340MB peak=16,747/17,340MB
[step 345/8250] loss=2.6077 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3636 tok/s=1,268,492 raw_tok/s=1,270,372 step_tokens=130,878 waste=0.1% h100_mfu=30.91% vram=397/17,340MB peak=16,747/17,340MB
[step 346/8250] loss=2.5989 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4353 tok/s=1,482,771 raw_tok/s=1,486,821 step_tokens=130,715 waste=0.3% h100_mfu=36.17% vram=397/17,340MB peak=16,747/17,340MB
[step 347/8250] loss=2.6177 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4436 tok/s=1,384,078 raw_tok/s=1,388,357 step_tokens=130,668 waste=0.3% h100_mfu=33.78% vram=397/17,340MB peak=16,747/17,340MB
[step 348/8250] loss=2.6063 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4019 tok/s=1,379,471 raw_tok/s=1,382,477 step_tokens=130,787 waste=0.2% h100_mfu=33.64% vram=397/17,340MB peak=16,747/17,340MB
[step 349/8250] loss=2.6058 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4439 tok/s=1,441,177 raw_tok/s=1,443,391 step_tokens=130,871 waste=0.2% h100_mfu=35.12% vram=397/17,340MB peak=16,747/17,340MB
[step 350/8250] loss=2.6173 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.4658 tok/s=1,365,293 raw_tok/s=1,369,871 step_tokens=130,634 waste=0.3% h100_mfu=33.33% vram=397/17,340MB peak=16,747/17,340MB
[step 351/8250] loss=2.6308 lr=9.95e-05 muon_lr=9.95e-04 grad_norm=0.3592 tok/s=1,509,006 raw_tok/s=1,509,651 step_tokens=131,016 waste=0.0% h100_mfu=36.73% vram=397/17,340MB peak=16,747/17,340MB
[step 352/8250] loss=2.6006 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3098 tok/s=1,369,396 raw_tok/s=1,372,926 step_tokens=130,735 waste=0.3% h100_mfu=33.40% vram=397/17,340MB peak=16,747/17,340MB
[step 353/8250] loss=2.6113 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3855 tok/s=1,495,792 raw_tok/s=1,501,474 step_tokens=130,576 waste=0.4% h100_mfu=36.53% vram=397/17,340MB peak=16,747/17,340MB
[step 354/8250] loss=2.5972 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3447 tok/s=1,281,000 raw_tok/s=1,285,837 step_tokens=130,579 waste=0.4% h100_mfu=31.28% vram=397/17,340MB peak=16,747/17,340MB
[step 355/8250] loss=2.5943 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3857 tok/s=1,448,930 raw_tok/s=1,452,109 step_tokens=130,785 waste=0.2% h100_mfu=35.33% vram=397/17,340MB peak=16,747/17,340MB
[step 356/8250] loss=2.5993 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3575 tok/s=1,406,962 raw_tok/s=1,413,443 step_tokens=130,471 waste=0.5% h100_mfu=34.39% vram=397/17,340MB peak=16,747/17,340MB
[step 357/8250] loss=2.6069 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3686 tok/s=1,407,208 raw_tok/s=1,411,904 step_tokens=130,636 waste=0.3% h100_mfu=34.35% vram=397/17,340MB peak=16,747/17,340MB
[step 358/8250] loss=2.5962 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.3546 tok/s=1,398,807 raw_tok/s=1,402,681 step_tokens=130,710 waste=0.3% h100_mfu=34.13% vram=397/17,340MB peak=16,747/17,340MB
[step 359/8250] loss=2.6131 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.4114 tok/s=1,357,261 raw_tok/s=1,363,273 step_tokens=130,494 waste=0.4% h100_mfu=33.17% vram=397/17,340MB peak=16,747/17,340MB
[step 360/8250] loss=2.6079 lr=9.94e-05 muon_lr=9.94e-04 grad_norm=0.4116 tok/s=1,408,683 raw_tok/s=1,409,446 step_tokens=131,001 waste=0.1% h100_mfu=34.29% vram=397/17,340MB peak=16,747/17,340MB
[step 361/8250] loss=2.5976 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3926 tok/s=1,431,600 raw_tok/s=1,432,967 step_tokens=130,947 waste=0.1% h100_mfu=34.86% vram=397/17,340MB peak=16,747/17,340MB
[step 362/8250] loss=2.6093 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3641 tok/s=1,388,476 raw_tok/s=1,391,618 step_tokens=130,776 waste=0.2% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 363/8250] loss=2.6011 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3231 tok/s=1,365,373 raw_tok/s=1,370,686 step_tokens=130,564 waste=0.4% h100_mfu=33.35% vram=397/17,340MB peak=16,747/17,340MB
[step 364/8250] loss=2.6109 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.4303 tok/s=1,355,011 raw_tok/s=1,356,853 step_tokens=130,894 waste=0.1% h100_mfu=33.01% vram=397/17,340MB peak=16,747/17,340MB
[step 365/8250] loss=2.5940 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3332 tok/s=1,469,945 raw_tok/s=1,473,002 step_tokens=130,800 waste=0.2% h100_mfu=35.84% vram=397/17,340MB peak=16,747/17,340MB
[step 366/8250] loss=2.5965 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3192 tok/s=1,363,854 raw_tok/s=1,367,045 step_tokens=130,766 waste=0.2% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 367/8250] loss=2.5911 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3094 tok/s=1,268,916 raw_tok/s=1,272,421 step_tokens=130,711 waste=0.3% h100_mfu=30.96% vram=397/17,340MB peak=16,747/17,340MB
[step 368/8250] loss=2.5927 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.3017 tok/s=1,288,713 raw_tok/s=1,291,008 step_tokens=130,839 waste=0.2% h100_mfu=31.41% vram=397/17,340MB peak=16,747/17,340MB
[step 369/8250] loss=2.6072 lr=9.93e-05 muon_lr=9.93e-04 grad_norm=0.4278 tok/s=1,425,303 raw_tok/s=1,426,468 step_tokens=130,965 waste=0.1% h100_mfu=34.71% vram=397/17,340MB peak=16,747/17,340MB
[step 370/8250] loss=2.5963 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.5118 tok/s=1,485,317 raw_tok/s=1,489,112 step_tokens=130,738 waste=0.3% h100_mfu=36.23% vram=397/17,340MB peak=16,747/17,340MB
[step 371/8250] loss=2.5964 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4638 tok/s=1,464,499 raw_tok/s=1,466,176 step_tokens=130,922 waste=0.1% h100_mfu=35.67% vram=397/17,340MB peak=16,747/17,340MB
[step 372/8250] loss=2.5988 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4034 tok/s=1,420,051 raw_tok/s=1,422,145 step_tokens=130,879 waste=0.1% h100_mfu=34.60% vram=397/17,340MB peak=16,747/17,340MB
[step 373/8250] loss=2.5975 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4076 tok/s=1,435,761 raw_tok/s=1,439,804 step_tokens=130,704 waste=0.3% h100_mfu=35.03% vram=397/17,340MB peak=16,747/17,340MB
[step 374/8250] loss=2.6081 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4746 tok/s=1,475,183 raw_tok/s=1,478,137 step_tokens=130,810 waste=0.2% h100_mfu=35.96% vram=397/17,340MB peak=16,747/17,340MB
[step 375/8250] loss=2.6093 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4600 tok/s=1,336,555 raw_tok/s=1,340,944 step_tokens=130,643 waste=0.3% h100_mfu=32.63% vram=397/17,340MB peak=16,747/17,340MB
[step 376/8250] loss=2.5757 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.3884 tok/s=1,292,004 raw_tok/s=1,296,306 step_tokens=130,637 waste=0.3% h100_mfu=31.54% vram=397/17,340MB peak=16,747/17,340MB
[step 377/8250] loss=2.6022 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.3401 tok/s=1,443,318 raw_tok/s=1,447,038 step_tokens=130,735 waste=0.3% h100_mfu=35.21% vram=397/17,340MB peak=16,747/17,340MB
[step 378/8250] loss=2.6089 lr=9.92e-05 muon_lr=9.92e-04 grad_norm=0.4393 tok/s=1,444,454 raw_tok/s=1,447,480 step_tokens=130,798 waste=0.2% h100_mfu=35.22% vram=397/17,340MB peak=16,747/17,340MB
[step 379/8250] loss=2.6099 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.3749 tok/s=1,472,448 raw_tok/s=1,474,416 step_tokens=130,897 waste=0.1% h100_mfu=35.87% vram=397/17,340MB peak=16,747/17,340MB
[step 380/8250] loss=2.5956 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.3556 tok/s=1,339,925 raw_tok/s=1,342,085 step_tokens=130,861 waste=0.2% h100_mfu=32.65% vram=397/17,340MB peak=16,747/17,340MB
[step 381/8250] loss=2.5928 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.3440 tok/s=1,391,957 raw_tok/s=1,395,961 step_tokens=130,696 waste=0.3% h100_mfu=33.96% vram=397/17,340MB peak=16,747/17,340MB
[step 382/8250] loss=2.5840 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.2837 tok/s=1,424,604 raw_tok/s=1,431,945 step_tokens=130,400 waste=0.5% h100_mfu=34.84% vram=397/17,340MB peak=16,747/17,340MB
[step 383/8250] loss=2.5928 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.3394 tok/s=1,358,369 raw_tok/s=1,362,152 step_tokens=130,708 waste=0.3% h100_mfu=33.14% vram=397/17,340MB peak=16,747/17,340MB
[step 384/8250] loss=2.5972 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.3507 tok/s=1,359,037 raw_tok/s=1,367,340 step_tokens=130,276 waste=0.6% h100_mfu=33.27% vram=397/17,340MB peak=16,747/17,340MB
[step 385/8250] loss=2.5885 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.3523 tok/s=1,333,820 raw_tok/s=1,335,083 step_tokens=130,948 waste=0.1% h100_mfu=32.48% vram=397/17,340MB peak=16,747/17,340MB
[step 386/8250] loss=2.5796 lr=9.91e-05 muon_lr=9.91e-04 grad_norm=0.2919 tok/s=1,425,838 raw_tok/s=1,431,824 step_tokens=130,524 waste=0.4% h100_mfu=34.84% vram=397/17,340MB peak=16,747/17,340MB
[step 387/8250] loss=2.5943 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.3470 tok/s=1,408,345 raw_tok/s=1,411,025 step_tokens=130,823 waste=0.2% h100_mfu=34.33% vram=397/17,340MB peak=16,747/17,340MB
[step 388/8250] loss=2.5901 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.3716 tok/s=1,313,480 raw_tok/s=1,316,563 step_tokens=130,765 waste=0.2% h100_mfu=32.03% vram=397/17,340MB peak=16,747/17,340MB
[step 389/8250] loss=2.5806 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.2726 tok/s=1,485,602 raw_tok/s=1,488,361 step_tokens=130,829 waste=0.2% h100_mfu=36.21% vram=397/17,340MB peak=16,747/17,340MB
[step 390/8250] loss=2.6011 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.2664 tok/s=1,293,316 raw_tok/s=1,297,444 step_tokens=130,655 waste=0.3% h100_mfu=31.57% vram=397/17,340MB peak=16,747/17,340MB
[step 391/8250] loss=2.5936 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.2724 tok/s=1,393,425 raw_tok/s=1,396,889 step_tokens=130,747 waste=0.2% h100_mfu=33.99% vram=397/17,340MB peak=16,747/17,340MB
[step 392/8250] loss=2.5722 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.3007 tok/s=1,391,912 raw_tok/s=1,396,739 step_tokens=130,619 waste=0.3% h100_mfu=33.98% vram=397/17,340MB peak=16,747/17,340MB
[step 393/8250] loss=2.5815 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.3474 tok/s=1,390,532 raw_tok/s=1,395,889 step_tokens=130,569 waste=0.4% h100_mfu=33.96% vram=397/17,340MB peak=16,747/17,340MB
[step 394/8250] loss=2.5768 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.3271 tok/s=1,383,544 raw_tok/s=1,387,227 step_tokens=130,724 waste=0.3% h100_mfu=33.75% vram=397/17,340MB peak=16,747/17,340MB
[step 395/8250] loss=2.6032 lr=9.90e-05 muon_lr=9.90e-04 grad_norm=0.3618 tok/s=1,500,576 raw_tok/s=1,505,273 step_tokens=130,663 waste=0.3% h100_mfu=36.62% vram=397/17,340MB peak=16,747/17,340MB
[step 396/8250] loss=2.6051 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.3137 tok/s=1,349,586 raw_tok/s=1,353,572 step_tokens=130,686 waste=0.3% h100_mfu=32.93% vram=397/17,340MB peak=16,747/17,340MB
[step 397/8250] loss=2.5905 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.3713 tok/s=1,380,845 raw_tok/s=1,384,744 step_tokens=130,703 waste=0.3% h100_mfu=33.69% vram=397/17,340MB peak=16,747/17,340MB
[step 398/8250] loss=2.6050 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.3108 tok/s=1,491,503 raw_tok/s=1,495,279 step_tokens=130,741 waste=0.3% h100_mfu=36.38% vram=397/17,340MB peak=16,747/17,340MB
[step 399/8250] loss=2.6000 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.3181 tok/s=1,226,340 raw_tok/s=1,228,120 step_tokens=130,882 waste=0.1% h100_mfu=29.88% vram=397/17,340MB peak=16,747/17,340MB
[step 400/8250] loss=2.5964 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.3925 tok/s=1,442,807 raw_tok/s=1,444,482 step_tokens=130,920 waste=0.1% h100_mfu=35.14% vram=397/17,340MB peak=16,747/17,340MB
[step 401/8250] loss=2.5631 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.3116 tok/s=1,400,090 raw_tok/s=1,404,268 step_tokens=130,682 waste=0.3% h100_mfu=34.17% vram=397/17,340MB peak=16,747/17,340MB
[step 402/8250] loss=2.5840 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.2736 tok/s=1,400,345 raw_tok/s=1,405,406 step_tokens=130,600 waste=0.4% h100_mfu=34.19% vram=397/17,340MB peak=16,747/17,340MB
[step 403/8250] loss=2.6108 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.2836 tok/s=1,359,884 raw_tok/s=1,362,088 step_tokens=130,860 waste=0.2% h100_mfu=33.14% vram=397/17,340MB peak=16,747/17,340MB
[step 404/8250] loss=2.5902 lr=9.89e-05 muon_lr=9.89e-04 grad_norm=0.2981 tok/s=1,416,960 raw_tok/s=1,418,877 step_tokens=130,895 waste=0.1% h100_mfu=34.52% vram=397/17,340MB peak=16,747/17,340MB
[step 405/8250] loss=2.5912 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.2823 tok/s=1,425,353 raw_tok/s=1,430,165 step_tokens=130,631 waste=0.3% h100_mfu=34.80% vram=397/17,340MB peak=16,747/17,340MB
[step 406/8250] loss=2.5891 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.2860 tok/s=1,400,509 raw_tok/s=1,405,269 step_tokens=130,628 waste=0.3% h100_mfu=34.19% vram=397/17,340MB peak=16,747/17,340MB
[step 407/8250] loss=2.5986 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.2873 tok/s=1,358,603 raw_tok/s=1,361,314 step_tokens=130,811 waste=0.2% h100_mfu=33.12% vram=397/17,340MB peak=16,747/17,340MB
[step 408/8250] loss=2.5677 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.2580 tok/s=1,417,560 raw_tok/s=1,421,312 step_tokens=130,726 waste=0.3% h100_mfu=34.58% vram=397/17,340MB peak=16,747/17,340MB
[step 409/8250] loss=2.5905 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.3186 tok/s=1,321,964 raw_tok/s=1,325,676 step_tokens=130,705 waste=0.3% h100_mfu=32.25% vram=397/17,340MB peak=16,747/17,340MB
[step 410/8250] loss=2.5873 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.2626 tok/s=1,407,686 raw_tok/s=1,409,967 step_tokens=130,860 waste=0.2% h100_mfu=34.30% vram=397/17,340MB peak=16,747/17,340MB
[step 411/8250] loss=2.5811 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.3214 tok/s=1,418,952 raw_tok/s=1,420,610 step_tokens=130,919 waste=0.1% h100_mfu=34.56% vram=397/17,340MB peak=16,747/17,340MB
[step 412/8250] loss=2.5850 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.3419 tok/s=1,426,338 raw_tok/s=1,428,354 step_tokens=130,887 waste=0.1% h100_mfu=34.75% vram=397/17,340MB peak=16,747/17,340MB
[step 413/8250] loss=2.5990 lr=9.88e-05 muon_lr=9.88e-04 grad_norm=0.3721 tok/s=1,402,545 raw_tok/s=1,408,304 step_tokens=130,536 waste=0.4% h100_mfu=34.26% vram=397/17,340MB peak=16,747/17,340MB
[step 414/8250] loss=2.5780 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.3551 tok/s=1,349,047 raw_tok/s=1,352,669 step_tokens=130,721 waste=0.3% h100_mfu=32.91% vram=397/17,340MB peak=16,747/17,340MB
[step 415/8250] loss=2.5854 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.3087 tok/s=1,426,684 raw_tok/s=1,427,958 step_tokens=130,955 waste=0.1% h100_mfu=34.74% vram=397/17,340MB peak=16,747/17,340MB
[step 416/8250] loss=2.5907 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.3608 tok/s=1,410,660 raw_tok/s=1,412,589 step_tokens=130,893 waste=0.1% h100_mfu=34.37% vram=397/17,340MB peak=16,747/17,340MB
[step 417/8250] loss=2.5928 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.3174 tok/s=1,472,058 raw_tok/s=1,474,770 step_tokens=130,831 waste=0.2% h100_mfu=35.88% vram=397/17,340MB peak=16,747/17,340MB
[step 418/8250] loss=2.5795 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.3724 tok/s=1,424,911 raw_tok/s=1,429,349 step_tokens=130,665 waste=0.3% h100_mfu=34.78% vram=397/17,340MB peak=16,747/17,340MB
[step 419/8250] loss=2.5889 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.3745 tok/s=1,368,830 raw_tok/s=1,371,645 step_tokens=130,803 waste=0.2% h100_mfu=33.37% vram=397/17,340MB peak=16,747/17,340MB
[step 420/8250] loss=2.5860 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.4550 tok/s=1,359,411 raw_tok/s=1,361,582 step_tokens=130,863 waste=0.2% h100_mfu=33.13% vram=397/17,340MB peak=16,747/17,340MB
[step 421/8250] loss=2.5733 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.4441 tok/s=1,437,431 raw_tok/s=1,441,489 step_tokens=130,703 waste=0.3% h100_mfu=35.07% vram=397/17,340MB peak=16,747/17,340MB
[step 422/8250] loss=2.5802 lr=9.87e-05 muon_lr=9.87e-04 grad_norm=0.2823 tok/s=1,323,521 raw_tok/s=1,328,905 step_tokens=130,541 waste=0.4% h100_mfu=32.33% vram=397/17,340MB peak=16,747/17,340MB
[step 423/8250] loss=2.5879 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.2404 tok/s=1,407,669 raw_tok/s=1,413,514 step_tokens=130,530 waste=0.4% h100_mfu=34.39% vram=397/17,340MB peak=16,747/17,340MB
[step 424/8250] loss=2.6094 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.3015 tok/s=1,470,729 raw_tok/s=1,474,170 step_tokens=130,766 waste=0.2% h100_mfu=35.87% vram=397/17,340MB peak=16,747/17,340MB
[step 425/8250] loss=2.5955 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.3914 tok/s=1,329,094 raw_tok/s=1,333,836 step_tokens=130,606 waste=0.4% h100_mfu=32.45% vram=397/17,340MB peak=16,747/17,340MB
[step 426/8250] loss=2.5871 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.3634 tok/s=1,484,186 raw_tok/s=1,486,283 step_tokens=130,887 waste=0.1% h100_mfu=36.16% vram=397/17,340MB peak=16,747/17,340MB
[step 427/8250] loss=2.6061 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.3618 tok/s=1,316,720 raw_tok/s=1,318,531 step_tokens=130,892 waste=0.1% h100_mfu=32.08% vram=397/17,340MB peak=16,747/17,340MB
[step 428/8250] loss=2.5908 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.2903 tok/s=1,364,505 raw_tok/s=1,368,567 step_tokens=130,683 waste=0.3% h100_mfu=33.30% vram=397/17,340MB peak=16,747/17,340MB
[step 429/8250] loss=2.5787 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.2556 tok/s=1,441,901 raw_tok/s=1,444,667 step_tokens=130,821 waste=0.2% h100_mfu=35.15% vram=397/17,340MB peak=16,747/17,340MB
[step 430/8250] loss=2.5654 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.4043 tok/s=1,405,308 raw_tok/s=1,406,703 step_tokens=130,942 waste=0.1% h100_mfu=34.23% vram=397/17,340MB peak=16,747/17,340MB
[step 431/8250] loss=2.5499 lr=9.86e-05 muon_lr=9.86e-04 grad_norm=0.2703 tok/s=1,332,355 raw_tok/s=1,335,626 step_tokens=130,751 waste=0.2% h100_mfu=32.50% vram=397/17,340MB peak=16,747/17,340MB
[step 432/8250] loss=2.5666 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.2181 tok/s=1,402,474 raw_tok/s=1,405,004 step_tokens=130,836 waste=0.2% h100_mfu=34.18% vram=397/17,340MB peak=16,747/17,340MB
[step 433/8250] loss=2.5591 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.2962 tok/s=1,396,659 raw_tok/s=1,398,548 step_tokens=130,895 waste=0.1% h100_mfu=34.03% vram=397/17,340MB peak=16,747/17,340MB
[step 434/8250] loss=2.5991 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.3167 tok/s=1,425,603 raw_tok/s=1,433,015 step_tokens=130,394 waste=0.5% h100_mfu=34.87% vram=397/17,340MB peak=16,747/17,340MB
[step 435/8250] loss=2.5704 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.3050 tok/s=1,287,404 raw_tok/s=1,291,483 step_tokens=130,658 waste=0.3% h100_mfu=31.42% vram=397/17,340MB peak=16,747/17,340MB
[step 436/8250] loss=2.5677 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.3125 tok/s=1,448,602 raw_tok/s=1,453,905 step_tokens=130,594 waste=0.4% h100_mfu=35.37% vram=397/17,340MB peak=16,747/17,340MB
[step 437/8250] loss=2.5887 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.2562 tok/s=1,413,584 raw_tok/s=1,414,836 step_tokens=130,956 waste=0.1% h100_mfu=34.42% vram=397/17,340MB peak=16,747/17,340MB
[step 438/8250] loss=2.5854 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.2872 tok/s=1,440,103 raw_tok/s=1,442,922 step_tokens=130,816 waste=0.2% h100_mfu=35.11% vram=397/17,340MB peak=16,747/17,340MB
[step 439/8250] loss=2.5970 lr=9.85e-05 muon_lr=9.85e-04 grad_norm=0.2941 tok/s=1,431,990 raw_tok/s=1,437,507 step_tokens=130,569 waste=0.4% h100_mfu=34.97% vram=397/17,340MB peak=16,747/17,340MB
[step 440/8250] loss=2.5788 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.4210 tok/s=1,398,659 raw_tok/s=1,399,054 step_tokens=131,035 waste=0.0% h100_mfu=34.04% vram=397/17,340MB peak=16,747/17,340MB
[step 441/8250] loss=2.5804 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.2595 tok/s=1,307,916 raw_tok/s=1,313,638 step_tokens=130,501 waste=0.4% h100_mfu=31.96% vram=397/17,340MB peak=16,747/17,340MB
[step 442/8250] loss=2.5937 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.3583 tok/s=1,374,480 raw_tok/s=1,381,340 step_tokens=130,421 waste=0.5% h100_mfu=33.61% vram=397/17,340MB peak=16,747/17,340MB
[step 443/8250] loss=2.5637 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.2969 tok/s=1,362,750 raw_tok/s=1,365,855 step_tokens=130,774 waste=0.2% h100_mfu=33.23% vram=397/17,340MB peak=16,747/17,340MB
[step 444/8250] loss=2.5776 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.3072 tok/s=1,385,464 raw_tok/s=1,388,525 step_tokens=130,783 waste=0.2% h100_mfu=33.78% vram=397/17,340MB peak=16,747/17,340MB
[step 445/8250] loss=2.5622 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.3755 tok/s=1,441,695 raw_tok/s=1,444,395 step_tokens=130,827 waste=0.2% h100_mfu=35.14% vram=397/17,340MB peak=16,747/17,340MB
[step 446/8250] loss=2.5851 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.2691 tok/s=1,401,947 raw_tok/s=1,404,454 step_tokens=130,838 waste=0.2% h100_mfu=34.17% vram=397/17,340MB peak=16,747/17,340MB
[step 447/8250] loss=2.5789 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.3484 tok/s=1,387,034 raw_tok/s=1,389,791 step_tokens=130,812 waste=0.2% h100_mfu=33.81% vram=397/17,340MB peak=16,747/17,340MB
[step 448/8250] loss=2.5676 lr=9.84e-05 muon_lr=9.84e-04 grad_norm=0.3626 tok/s=1,408,079 raw_tok/s=1,411,525 step_tokens=130,752 waste=0.2% h100_mfu=34.34% vram=397/17,340MB peak=16,747/17,340MB
[step 449/8250] loss=2.5851 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.2929 tok/s=1,336,706 raw_tok/s=1,341,835 step_tokens=130,571 waste=0.4% h100_mfu=32.65% vram=397/17,340MB peak=16,747/17,340MB
[step 450/8250] loss=2.5847 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.3323 tok/s=1,435,872 raw_tok/s=1,440,653 step_tokens=130,637 waste=0.3% h100_mfu=35.05% vram=397/17,340MB peak=16,747/17,340MB
[step 451/8250] loss=2.5793 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.3856 tok/s=1,411,327 raw_tok/s=1,414,489 step_tokens=130,779 waste=0.2% h100_mfu=34.41% vram=397/17,340MB peak=16,747/17,340MB
[step 452/8250] loss=2.5738 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.2512 tok/s=1,302,895 raw_tok/s=1,305,774 step_tokens=130,783 waste=0.2% h100_mfu=31.77% vram=397/17,340MB peak=16,747/17,340MB
[step 453/8250] loss=2.5790 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.3255 tok/s=1,410,132 raw_tok/s=1,412,438 step_tokens=130,858 waste=0.2% h100_mfu=34.36% vram=397/17,340MB peak=16,747/17,340MB
[step 454/8250] loss=2.5480 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.3736 tok/s=1,507,729 raw_tok/s=1,508,846 step_tokens=130,975 waste=0.1% h100_mfu=36.71% vram=397/17,340MB peak=16,747/17,340MB
[step 455/8250] loss=2.5693 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.2947 tok/s=1,362,901 raw_tok/s=1,364,724 step_tokens=130,897 waste=0.1% h100_mfu=33.20% vram=397/17,340MB peak=16,747/17,340MB
[step 456/8250] loss=2.5890 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.3610 tok/s=1,395,149 raw_tok/s=1,397,388 step_tokens=130,862 waste=0.2% h100_mfu=34.00% vram=397/17,340MB peak=16,747/17,340MB
[step 457/8250] loss=2.5856 lr=9.83e-05 muon_lr=9.83e-04 grad_norm=0.4149 tok/s=1,357,429 raw_tok/s=1,361,189 step_tokens=130,710 waste=0.3% h100_mfu=33.12% vram=397/17,340MB peak=16,747/17,340MB
[step 458/8250] loss=2.5693 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.3406 tok/s=1,342,888 raw_tok/s=1,347,721 step_tokens=130,602 waste=0.4% h100_mfu=32.79% vram=397/17,340MB peak=16,747/17,340MB
[step 459/8250] loss=2.5765 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.3220 tok/s=1,440,967 raw_tok/s=1,442,651 step_tokens=130,919 waste=0.1% h100_mfu=35.10% vram=397/17,340MB peak=16,747/17,340MB
[step 460/8250] loss=2.5600 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.3169 tok/s=1,385,131 raw_tok/s=1,387,555 step_tokens=130,843 waste=0.2% h100_mfu=33.76% vram=397/17,340MB peak=16,747/17,340MB
[step 461/8250] loss=2.5861 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.2112 tok/s=1,410,099 raw_tok/s=1,412,092 step_tokens=130,887 waste=0.1% h100_mfu=34.36% vram=397/17,340MB peak=16,747/17,340MB
[step 462/8250] loss=2.5896 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.2413 tok/s=1,357,165 raw_tok/s=1,359,312 step_tokens=130,865 waste=0.2% h100_mfu=33.07% vram=397/17,340MB peak=16,747/17,340MB
[step 463/8250] loss=2.5655 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.2427 tok/s=1,410,660 raw_tok/s=1,411,317 step_tokens=131,011 waste=0.0% h100_mfu=34.34% vram=397/17,340MB peak=16,747/17,340MB
[step 464/8250] loss=2.5802 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.3417 tok/s=1,410,687 raw_tok/s=1,412,109 step_tokens=130,940 waste=0.1% h100_mfu=34.36% vram=397/17,340MB peak=16,747/17,340MB
[step 465/8250] loss=2.5833 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.2773 tok/s=1,358,620 raw_tok/s=1,362,883 step_tokens=130,662 waste=0.3% h100_mfu=33.16% vram=397/17,340MB peak=16,747/17,340MB
[step 466/8250] loss=2.5924 lr=9.82e-05 muon_lr=9.82e-04 grad_norm=0.2004 tok/s=1,418,301 raw_tok/s=1,422,077 step_tokens=130,724 waste=0.3% h100_mfu=34.60% vram=397/17,340MB peak=16,747/17,340MB
[step 467/8250] loss=2.5628 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2224 tok/s=1,357,245 raw_tok/s=1,363,392 step_tokens=130,481 waste=0.5% h100_mfu=33.17% vram=397/17,340MB peak=16,747/17,340MB
[step 468/8250] loss=2.5791 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2812 tok/s=1,352,016 raw_tok/s=1,354,185 step_tokens=130,862 waste=0.2% h100_mfu=32.95% vram=397/17,340MB peak=16,747/17,340MB
[step 469/8250] loss=2.5722 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2855 tok/s=1,446,170 raw_tok/s=1,450,697 step_tokens=130,663 waste=0.3% h100_mfu=35.30% vram=397/17,340MB peak=16,747/17,340MB
[step 470/8250] loss=2.5879 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2058 tok/s=1,307,317 raw_tok/s=1,312,021 step_tokens=130,602 waste=0.4% h100_mfu=31.92% vram=397/17,340MB peak=16,747/17,340MB
[step 471/8250] loss=2.5774 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2734 tok/s=1,430,646 raw_tok/s=1,433,621 step_tokens=130,800 waste=0.2% h100_mfu=34.88% vram=397/17,340MB peak=16,747/17,340MB
[step 472/8250] loss=2.5764 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.3196 tok/s=1,365,193 raw_tok/s=1,366,956 step_tokens=130,903 waste=0.1% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 473/8250] loss=2.5711 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2939 tok/s=1,466,764 raw_tok/s=1,468,445 step_tokens=130,922 waste=0.1% h100_mfu=35.73% vram=397/17,340MB peak=16,747/17,340MB
[step 474/8250] loss=2.5839 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.3008 tok/s=1,279,619 raw_tok/s=1,280,684 step_tokens=130,963 waste=0.1% h100_mfu=31.16% vram=397/17,340MB peak=16,747/17,340MB
[step 475/8250] loss=2.5643 lr=9.81e-05 muon_lr=9.81e-04 grad_norm=0.2279 tok/s=1,450,714 raw_tok/s=1,453,875 step_tokens=130,787 waste=0.2% h100_mfu=35.37% vram=397/17,340MB peak=16,747/17,340MB
[step 476/8250] loss=2.5411 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.2495 tok/s=1,384,739 raw_tok/s=1,385,532 step_tokens=130,997 waste=0.1% h100_mfu=33.71% vram=397/17,340MB peak=16,747/17,340MB
[step 477/8250] loss=2.5595 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.3629 tok/s=1,467,614 raw_tok/s=1,471,510 step_tokens=130,725 waste=0.3% h100_mfu=35.80% vram=397/17,340MB peak=16,747/17,340MB
[step 478/8250] loss=2.5664 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.2205 tok/s=1,386,965 raw_tok/s=1,394,081 step_tokens=130,403 waste=0.5% h100_mfu=33.92% vram=397/17,340MB peak=16,747/17,340MB
[step 479/8250] loss=2.5404 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.2764 tok/s=1,480,495 raw_tok/s=1,482,553 step_tokens=130,890 waste=0.1% h100_mfu=36.07% vram=397/17,340MB peak=16,747/17,340MB
[step 480/8250] loss=2.5793 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.2953 tok/s=1,402,401 raw_tok/s=1,406,640 step_tokens=130,677 waste=0.3% h100_mfu=34.22% vram=397/17,340MB peak=16,747/17,340MB
[step 481/8250] loss=2.5445 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.3348 tok/s=1,367,277 raw_tok/s=1,368,864 step_tokens=130,920 waste=0.1% h100_mfu=33.30% vram=397/17,340MB peak=16,747/17,340MB
[step 482/8250] loss=2.5758 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.3403 tok/s=1,407,630 raw_tok/s=1,409,049 step_tokens=130,940 waste=0.1% h100_mfu=34.28% vram=397/17,340MB peak=16,747/17,340MB
[step 483/8250] loss=2.5646 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.2993 tok/s=1,516,108 raw_tok/s=1,521,704 step_tokens=130,590 waste=0.4% h100_mfu=37.02% vram=397/17,340MB peak=16,747/17,340MB
[step 484/8250] loss=2.5502 lr=9.80e-05 muon_lr=9.80e-04 grad_norm=0.3813 tok/s=1,489,185 raw_tok/s=1,492,875 step_tokens=130,748 waste=0.2% h100_mfu=36.32% vram=397/17,340MB peak=16,747/17,340MB
[step 485/8250] loss=2.5525 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.3414 tok/s=1,413,898 raw_tok/s=1,417,651 step_tokens=130,725 waste=0.3% h100_mfu=34.49% vram=397/17,340MB peak=16,747/17,340MB
[step 486/8250] loss=2.5515 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.3408 tok/s=1,401,851 raw_tok/s=1,404,981 step_tokens=130,780 waste=0.2% h100_mfu=34.18% vram=397/17,340MB peak=16,747/17,340MB
[step 487/8250] loss=2.5543 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.3654 tok/s=1,512,583 raw_tok/s=1,513,853 step_tokens=130,962 waste=0.1% h100_mfu=36.83% vram=397/17,340MB peak=16,747/17,340MB
[step 488/8250] loss=2.5616 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.2983 tok/s=1,379,510 raw_tok/s=1,383,510 step_tokens=130,693 waste=0.3% h100_mfu=33.66% vram=397/17,340MB peak=16,747/17,340MB
[step 489/8250] loss=2.5754 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.3872 tok/s=1,326,051 raw_tok/s=1,329,571 step_tokens=130,725 waste=0.3% h100_mfu=32.35% vram=397/17,340MB peak=16,747/17,340MB
[step 490/8250] loss=2.5603 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.3345 tok/s=1,363,911 raw_tok/s=1,367,385 step_tokens=130,739 waste=0.3% h100_mfu=33.27% vram=397/17,340MB peak=16,747/17,340MB
[step 491/8250] loss=2.5562 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.2771 tok/s=1,427,459 raw_tok/s=1,430,088 step_tokens=130,831 waste=0.2% h100_mfu=34.79% vram=397/17,340MB peak=16,747/17,340MB
[step 492/8250] loss=2.5549 lr=9.79e-05 muon_lr=9.79e-04 grad_norm=0.2264 tok/s=1,465,539 raw_tok/s=1,471,173 step_tokens=130,570 waste=0.4% h100_mfu=35.79% vram=397/17,340MB peak=16,747/17,340MB
[step 493/8250] loss=2.5554 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.2441 tok/s=1,444,202 raw_tok/s=1,447,117 step_tokens=130,808 waste=0.2% h100_mfu=35.21% vram=397/17,340MB peak=16,747/17,340MB
[step 494/8250] loss=2.5792 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.3647 tok/s=1,391,808 raw_tok/s=1,396,261 step_tokens=130,654 waste=0.3% h100_mfu=33.97% vram=397/17,340MB peak=16,747/17,340MB
[step 495/8250] loss=2.5741 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.3866 tok/s=1,435,870 raw_tok/s=1,442,662 step_tokens=130,455 waste=0.5% h100_mfu=35.10% vram=397/17,340MB peak=16,747/17,340MB
[step 496/8250] loss=2.5287 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.3880 tok/s=1,472,160 raw_tok/s=1,477,639 step_tokens=130,586 waste=0.4% h100_mfu=35.95% vram=397/17,340MB peak=16,747/17,340MB
[step 497/8250] loss=2.5823 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.3100 tok/s=1,435,334 raw_tok/s=1,437,077 step_tokens=130,913 waste=0.1% h100_mfu=34.96% vram=397/17,340MB peak=16,747/17,340MB
[step 498/8250] loss=2.5625 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.2263 tok/s=1,411,248 raw_tok/s=1,412,854 step_tokens=130,923 waste=0.1% h100_mfu=34.37% vram=397/17,340MB peak=16,747/17,340MB
[step 499/8250] loss=2.5664 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.2932 tok/s=1,451,120 raw_tok/s=1,452,783 step_tokens=130,922 waste=0.1% h100_mfu=35.35% vram=397/17,340MB peak=16,747/17,340MB
[step 500/8250] loss=2.5568 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.3056 tok/s=1,379,746 raw_tok/s=1,381,770 step_tokens=130,880 waste=0.1% h100_mfu=33.62% vram=397/17,340MB peak=16,747/17,340MB
[step 500] eval_loss=2.5592
[step 501/8250] loss=2.5733 lr=9.78e-05 muon_lr=9.78e-04 grad_norm=0.2548 tok/s=1,364,782 raw_tok/s=1,367,714 step_tokens=130,791 waste=0.2% h100_mfu=33.28% vram=397/17,340MB peak=16,750/17,340MB
[step 502/8250] loss=2.5580 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.2680 tok/s=1,437,607 raw_tok/s=1,444,916 step_tokens=130,409 waste=0.5% h100_mfu=35.15% vram=397/17,340MB peak=16,747/17,340MB
[step 503/8250] loss=2.5708 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.3366 tok/s=1,433,163 raw_tok/s=1,434,159 step_tokens=130,981 waste=0.1% h100_mfu=34.89% vram=397/17,340MB peak=16,747/17,340MB
[step 504/8250] loss=2.5712 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.3276 tok/s=1,353,598 raw_tok/s=1,362,308 step_tokens=130,234 waste=0.6% h100_mfu=33.15% vram=397/17,340MB peak=16,747/17,340MB
[step 505/8250] loss=2.5613 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.3611 tok/s=1,376,121 raw_tok/s=1,380,745 step_tokens=130,633 waste=0.3% h100_mfu=33.59% vram=397/17,340MB peak=16,747/17,340MB
[step 506/8250] loss=2.5578 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.3258 tok/s=1,476,146 raw_tok/s=1,480,914 step_tokens=130,650 waste=0.3% h100_mfu=36.03% vram=397/17,340MB peak=16,747/17,340MB
[step 507/8250] loss=2.5494 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.2429 tok/s=1,427,817 raw_tok/s=1,437,082 step_tokens=130,227 waste=0.6% h100_mfu=34.96% vram=397/17,340MB peak=16,747/17,340MB
[step 508/8250] loss=2.5815 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.2970 tok/s=1,384,009 raw_tok/s=1,386,792 step_tokens=130,809 waste=0.2% h100_mfu=33.74% vram=397/17,340MB peak=16,747/17,340MB
[step 509/8250] loss=2.5697 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.2805 tok/s=1,394,638 raw_tok/s=1,397,891 step_tokens=130,767 waste=0.2% h100_mfu=34.01% vram=397/17,340MB peak=16,747/17,340MB
[step 510/8250] loss=2.5325 lr=9.77e-05 muon_lr=9.77e-04 grad_norm=0.2903 tok/s=1,425,722 raw_tok/s=1,429,737 step_tokens=130,704 waste=0.3% h100_mfu=34.79% vram=397/17,340MB peak=16,747/17,340MB
[step 511/8250] loss=2.5489 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.2478 tok/s=1,357,231 raw_tok/s=1,358,962 step_tokens=130,905 waste=0.1% h100_mfu=33.06% vram=397/17,340MB peak=16,747/17,340MB
[step 512/8250] loss=2.5335 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.2791 tok/s=1,469,131 raw_tok/s=1,472,996 step_tokens=130,728 waste=0.3% h100_mfu=35.84% vram=397/17,340MB peak=16,747/17,340MB
[step 513/8250] loss=2.5406 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.3744 tok/s=1,359,401 raw_tok/s=1,363,896 step_tokens=130,640 waste=0.3% h100_mfu=33.18% vram=397/17,340MB peak=16,747/17,340MB
[step 514/8250] loss=2.5370 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.2767 tok/s=1,454,790 raw_tok/s=1,456,446 step_tokens=130,923 waste=0.1% h100_mfu=35.44% vram=397/17,340MB peak=16,747/17,340MB
[step 515/8250] loss=2.5344 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.3080 tok/s=1,442,017 raw_tok/s=1,443,504 step_tokens=130,937 waste=0.1% h100_mfu=35.12% vram=397/17,340MB peak=16,747/17,340MB
[step 516/8250] loss=2.5515 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.2333 tok/s=1,397,149 raw_tok/s=1,398,899 step_tokens=130,908 waste=0.1% h100_mfu=34.04% vram=397/17,340MB peak=16,747/17,340MB
[step 517/8250] loss=2.5579 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.2387 tok/s=1,321,079 raw_tok/s=1,324,049 step_tokens=130,778 waste=0.2% h100_mfu=32.21% vram=397/17,340MB peak=16,747/17,340MB
[step 518/8250] loss=2.5528 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.2902 tok/s=1,438,947 raw_tok/s=1,443,594 step_tokens=130,650 waste=0.3% h100_mfu=35.12% vram=397/17,340MB peak=16,747/17,340MB
[step 519/8250] loss=2.5661 lr=9.76e-05 muon_lr=9.76e-04 grad_norm=0.3454 tok/s=1,402,998 raw_tok/s=1,404,455 step_tokens=130,936 waste=0.1% h100_mfu=34.17% vram=397/17,340MB peak=16,747/17,340MB
[step 520/8250] loss=2.5736 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.2509 tok/s=1,446,058 raw_tok/s=1,449,486 step_tokens=130,762 waste=0.2% h100_mfu=35.27% vram=397/17,340MB peak=16,747/17,340MB
[step 521/8250] loss=2.5557 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.2121 tok/s=1,179,168 raw_tok/s=1,182,370 step_tokens=130,717 waste=0.3% h100_mfu=28.77% vram=397/17,340MB peak=16,747/17,340MB
[step 522/8250] loss=2.5478 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.3303 tok/s=1,382,525 raw_tok/s=1,384,320 step_tokens=130,902 waste=0.1% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 523/8250] loss=2.5394 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.3012 tok/s=1,437,419 raw_tok/s=1,440,882 step_tokens=130,757 waste=0.2% h100_mfu=35.06% vram=397/17,340MB peak=16,747/17,340MB
[step 524/8250] loss=2.5251 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.2101 tok/s=1,477,667 raw_tok/s=1,481,250 step_tokens=130,755 waste=0.2% h100_mfu=36.04% vram=397/17,340MB peak=16,747/17,340MB
[step 525/8250] loss=2.5384 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.2190 tok/s=1,383,261 raw_tok/s=1,386,137 step_tokens=130,800 waste=0.2% h100_mfu=33.72% vram=397/17,340MB peak=16,747/17,340MB
[step 526/8250] loss=2.5739 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.3219 tok/s=1,370,858 raw_tok/s=1,376,118 step_tokens=130,571 waste=0.4% h100_mfu=33.48% vram=397/17,340MB peak=16,747/17,340MB
[step 527/8250] loss=2.5421 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.4068 tok/s=1,283,828 raw_tok/s=1,286,095 step_tokens=130,841 waste=0.2% h100_mfu=31.29% vram=397/17,340MB peak=16,747/17,340MB
[step 528/8250] loss=2.5367 lr=9.75e-05 muon_lr=9.75e-04 grad_norm=0.4533 tok/s=1,399,957 raw_tok/s=1,400,758 step_tokens=130,997 waste=0.1% h100_mfu=34.08% vram=397/17,340MB peak=16,747/17,340MB
[step 529/8250] loss=2.5642 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.3463 tok/s=1,433,636 raw_tok/s=1,435,487 step_tokens=130,903 waste=0.1% h100_mfu=34.93% vram=397/17,340MB peak=16,747/17,340MB
[step 530/8250] loss=2.5549 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.2874 tok/s=1,417,570 raw_tok/s=1,421,398 step_tokens=130,719 waste=0.3% h100_mfu=34.58% vram=397/17,340MB peak=16,747/17,340MB
[step 531/8250] loss=2.5551 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.2823 tok/s=1,420,528 raw_tok/s=1,424,103 step_tokens=130,743 waste=0.3% h100_mfu=34.65% vram=397/17,340MB peak=16,747/17,340MB
[step 532/8250] loss=2.5400 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.3218 tok/s=1,302,702 raw_tok/s=1,307,781 step_tokens=130,563 waste=0.4% h100_mfu=31.82% vram=397/17,340MB peak=16,747/17,340MB
[step 533/8250] loss=2.5280 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.3384 tok/s=1,437,788 raw_tok/s=1,439,831 step_tokens=130,886 waste=0.1% h100_mfu=35.03% vram=397/17,340MB peak=16,747/17,340MB
[step 534/8250] loss=2.5183 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.3060 tok/s=1,338,392 raw_tok/s=1,342,540 step_tokens=130,667 waste=0.3% h100_mfu=32.66% vram=397/17,340MB peak=16,747/17,340MB
[step 535/8250] loss=2.5249 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.2697 tok/s=1,407,750 raw_tok/s=1,409,363 step_tokens=130,922 waste=0.1% h100_mfu=34.29% vram=397/17,340MB peak=16,747/17,340MB
[step 536/8250] loss=2.5437 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.3366 tok/s=1,407,999 raw_tok/s=1,409,946 step_tokens=130,891 waste=0.1% h100_mfu=34.30% vram=397/17,340MB peak=16,747/17,340MB
[step 537/8250] loss=2.5313 lr=9.74e-05 muon_lr=9.74e-04 grad_norm=0.3207 tok/s=1,419,003 raw_tok/s=1,420,769 step_tokens=130,909 waste=0.1% h100_mfu=34.57% vram=397/17,340MB peak=16,747/17,340MB
[step 538/8250] loss=2.5325 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.3056 tok/s=1,445,643 raw_tok/s=1,448,871 step_tokens=130,780 waste=0.2% h100_mfu=35.25% vram=397/17,340MB peak=16,747/17,340MB
[step 539/8250] loss=2.5021 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.3157 tok/s=1,365,688 raw_tok/s=1,370,551 step_tokens=130,607 waste=0.4% h100_mfu=33.35% vram=397/17,340MB peak=16,747/17,340MB
[step 540/8250] loss=2.5583 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.2558 tok/s=1,348,053 raw_tok/s=1,348,238 step_tokens=131,054 waste=0.0% h100_mfu=32.80% vram=397/17,340MB peak=16,747/17,340MB
[step 541/8250] loss=2.5394 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.3132 tok/s=1,439,780 raw_tok/s=1,440,692 step_tokens=130,989 waste=0.1% h100_mfu=35.05% vram=397/17,340MB peak=16,747/17,340MB
[step 542/8250] loss=2.5676 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.2970 tok/s=1,486,904 raw_tok/s=1,493,960 step_tokens=130,453 waste=0.5% h100_mfu=36.35% vram=397/17,340MB peak=16,747/17,340MB
[step 543/8250] loss=2.5696 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.2606 tok/s=1,391,203 raw_tok/s=1,394,319 step_tokens=130,779 waste=0.2% h100_mfu=33.92% vram=397/17,340MB peak=16,747/17,340MB
[step 544/8250] loss=2.5293 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.3454 tok/s=1,380,524 raw_tok/s=1,382,771 step_tokens=130,859 waste=0.2% h100_mfu=33.64% vram=397/17,340MB peak=16,747/17,340MB
[step 545/8250] loss=2.5627 lr=9.73e-05 muon_lr=9.73e-04 grad_norm=0.3214 tok/s=1,447,452 raw_tok/s=1,453,529 step_tokens=130,524 waste=0.4% h100_mfu=35.36% vram=397/17,340MB peak=16,747/17,340MB
[step 546/8250] loss=2.5441 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.2756 tok/s=1,430,687 raw_tok/s=1,435,880 step_tokens=130,598 waste=0.4% h100_mfu=34.94% vram=397/17,340MB peak=16,747/17,340MB
[step 547/8250] loss=2.5610 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.2935 tok/s=1,443,524 raw_tok/s=1,448,453 step_tokens=130,626 waste=0.3% h100_mfu=35.24% vram=397/17,340MB peak=16,747/17,340MB
[step 548/8250] loss=2.5465 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.2629 tok/s=1,442,366 raw_tok/s=1,450,733 step_tokens=130,316 waste=0.6% h100_mfu=35.30% vram=397/17,340MB peak=16,747/17,340MB
[step 549/8250] loss=2.5692 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.2610 tok/s=1,499,249 raw_tok/s=1,500,703 step_tokens=130,945 waste=0.1% h100_mfu=36.51% vram=397/17,340MB peak=16,747/17,340MB
[step 550/8250] loss=2.5527 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.4074 tok/s=1,430,231 raw_tok/s=1,433,995 step_tokens=130,728 waste=0.3% h100_mfu=34.89% vram=397/17,340MB peak=16,747/17,340MB
[step 551/8250] loss=2.5569 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.3395 tok/s=1,437,621 raw_tok/s=1,440,302 step_tokens=130,828 waste=0.2% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 552/8250] loss=2.5226 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.2561 tok/s=1,398,093 raw_tok/s=1,403,489 step_tokens=130,568 waste=0.4% h100_mfu=34.15% vram=397/17,340MB peak=16,747/17,340MB
[step 553/8250] loss=2.5422 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.2907 tok/s=1,424,379 raw_tok/s=1,426,534 step_tokens=130,874 waste=0.2% h100_mfu=34.71% vram=397/17,340MB peak=16,747/17,340MB
[step 554/8250] loss=2.5473 lr=9.72e-05 muon_lr=9.72e-04 grad_norm=0.3772 tok/s=1,422,935 raw_tok/s=1,426,166 step_tokens=130,775 waste=0.2% h100_mfu=34.70% vram=397/17,340MB peak=16,747/17,340MB
[step 555/8250] loss=2.5558 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.2535 tok/s=1,438,586 raw_tok/s=1,438,982 step_tokens=131,036 waste=0.0% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 556/8250] loss=2.5270 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.2730 tok/s=1,430,897 raw_tok/s=1,434,618 step_tokens=130,732 waste=0.3% h100_mfu=34.90% vram=397/17,340MB peak=16,747/17,340MB
[step 557/8250] loss=2.5367 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.3447 tok/s=1,383,979 raw_tok/s=1,386,836 step_tokens=130,802 waste=0.2% h100_mfu=33.74% vram=397/17,340MB peak=16,747/17,340MB
[step 558/8250] loss=2.5320 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.2566 tok/s=1,394,163 raw_tok/s=1,394,802 step_tokens=131,012 waste=0.0% h100_mfu=33.94% vram=397/17,340MB peak=16,747/17,340MB
[step 559/8250] loss=2.5408 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.3228 tok/s=1,431,445 raw_tok/s=1,433,402 step_tokens=130,893 waste=0.1% h100_mfu=34.87% vram=397/17,340MB peak=16,747/17,340MB
[step 560/8250] loss=2.5472 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.3430 tok/s=1,406,966 raw_tok/s=1,410,442 step_tokens=130,749 waste=0.2% h100_mfu=34.32% vram=397/17,340MB peak=16,747/17,340MB
[step 561/8250] loss=2.5290 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.3704 tok/s=1,392,574 raw_tok/s=1,398,796 step_tokens=130,489 waste=0.4% h100_mfu=34.03% vram=397/17,340MB peak=16,747/17,340MB
[step 562/8250] loss=2.5496 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.3734 tok/s=1,373,351 raw_tok/s=1,380,471 step_tokens=130,396 waste=0.5% h100_mfu=33.59% vram=397/17,340MB peak=16,747/17,340MB
[step 563/8250] loss=2.5394 lr=9.71e-05 muon_lr=9.71e-04 grad_norm=0.3314 tok/s=1,347,325 raw_tok/s=1,351,615 step_tokens=130,656 waste=0.3% h100_mfu=32.88% vram=397/17,340MB peak=16,747/17,340MB
[step 564/8250] loss=2.4887 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.2545 tok/s=1,384,098 raw_tok/s=1,386,128 step_tokens=130,880 waste=0.1% h100_mfu=33.72% vram=397/17,340MB peak=16,747/17,340MB
[step 565/8250] loss=2.5046 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.3424 tok/s=1,310,663 raw_tok/s=1,311,754 step_tokens=130,963 waste=0.1% h100_mfu=31.92% vram=397/17,340MB peak=16,747/17,340MB
[step 566/8250] loss=2.5206 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.2439 tok/s=1,378,615 raw_tok/s=1,379,657 step_tokens=130,973 waste=0.1% h100_mfu=33.57% vram=397/17,340MB peak=16,747/17,340MB
[step 567/8250] loss=2.5286 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.2787 tok/s=1,356,553 raw_tok/s=1,361,477 step_tokens=130,598 waste=0.4% h100_mfu=33.12% vram=397/17,340MB peak=16,747/17,340MB
[step 568/8250] loss=2.5090 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.3187 tok/s=1,338,306 raw_tok/s=1,344,945 step_tokens=130,425 waste=0.5% h100_mfu=32.72% vram=397/17,340MB peak=16,747/17,340MB
[step 569/8250] loss=2.5290 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.3238 tok/s=1,492,161 raw_tok/s=1,500,392 step_tokens=130,353 waste=0.5% h100_mfu=36.50% vram=397/17,340MB peak=16,747/17,340MB
[step 570/8250] loss=2.5213 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.2386 tok/s=1,398,911 raw_tok/s=1,403,763 step_tokens=130,619 waste=0.3% h100_mfu=34.15% vram=397/17,340MB peak=16,747/17,340MB
[step 571/8250] loss=2.5115 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.2699 tok/s=1,381,157 raw_tok/s=1,384,452 step_tokens=130,760 waste=0.2% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 572/8250] loss=2.5300 lr=9.70e-05 muon_lr=9.70e-04 grad_norm=0.3035 tok/s=1,427,907 raw_tok/s=1,433,254 step_tokens=130,583 waste=0.4% h100_mfu=34.87% vram=397/17,340MB peak=16,747/17,340MB
[step 573/8250] loss=2.5037 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.3429 tok/s=1,361,308 raw_tok/s=1,365,297 step_tokens=130,689 waste=0.3% h100_mfu=33.22% vram=397/17,340MB peak=16,747/17,340MB
[step 574/8250] loss=2.5409 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.2721 tok/s=1,435,798 raw_tok/s=1,437,685 step_tokens=130,900 waste=0.1% h100_mfu=34.98% vram=397/17,340MB peak=16,747/17,340MB
[step 575/8250] loss=2.4983 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.3110 tok/s=1,343,919 raw_tok/s=1,348,735 step_tokens=130,604 waste=0.4% h100_mfu=32.81% vram=397/17,340MB peak=16,747/17,340MB
[step 576/8250] loss=2.5559 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.3463 tok/s=1,418,954 raw_tok/s=1,424,519 step_tokens=130,560 waste=0.4% h100_mfu=34.66% vram=397/17,340MB peak=16,747/17,340MB
[step 577/8250] loss=2.5387 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.2914 tok/s=1,472,662 raw_tok/s=1,478,188 step_tokens=130,582 waste=0.4% h100_mfu=35.96% vram=397/17,340MB peak=16,747/17,340MB
[step 578/8250] loss=2.4999 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.3894 tok/s=1,438,199 raw_tok/s=1,439,001 step_tokens=130,999 waste=0.1% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 579/8250] loss=2.5226 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.2894 tok/s=1,408,869 raw_tok/s=1,410,257 step_tokens=130,943 waste=0.1% h100_mfu=34.31% vram=397/17,340MB peak=16,747/17,340MB
[step 580/8250] loss=2.5383 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.2580 tok/s=1,418,036 raw_tok/s=1,421,387 step_tokens=130,763 waste=0.2% h100_mfu=34.58% vram=397/17,340MB peak=16,747/17,340MB
[step 581/8250] loss=2.5177 lr=9.69e-05 muon_lr=9.69e-04 grad_norm=0.2605 tok/s=1,426,717 raw_tok/s=1,428,854 step_tokens=130,876 waste=0.1% h100_mfu=34.76% vram=397/17,340MB peak=16,747/17,340MB
[step 582/8250] loss=2.5216 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.3251 tok/s=1,408,644 raw_tok/s=1,414,797 step_tokens=130,502 waste=0.4% h100_mfu=34.42% vram=397/17,340MB peak=16,747/17,340MB
[step 583/8250] loss=2.5098 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.4192 tok/s=1,442,599 raw_tok/s=1,447,946 step_tokens=130,588 waste=0.4% h100_mfu=35.23% vram=397/17,340MB peak=16,747/17,340MB
[step 584/8250] loss=2.5193 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.3171 tok/s=1,402,336 raw_tok/s=1,404,253 step_tokens=130,893 waste=0.1% h100_mfu=34.17% vram=397/17,340MB peak=16,747/17,340MB
[step 585/8250] loss=2.5360 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.2734 tok/s=1,437,284 raw_tok/s=1,439,095 step_tokens=130,907 waste=0.1% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 586/8250] loss=2.4846 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.3178 tok/s=1,388,295 raw_tok/s=1,391,033 step_tokens=130,814 waste=0.2% h100_mfu=33.84% vram=397/17,340MB peak=16,747/17,340MB
[step 587/8250] loss=2.4975 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.3200 tok/s=1,375,202 raw_tok/s=1,377,041 step_tokens=130,897 waste=0.1% h100_mfu=33.50% vram=397/17,340MB peak=16,747/17,340MB
[step 588/8250] loss=2.5145 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.3091 tok/s=1,365,586 raw_tok/s=1,366,994 step_tokens=130,937 waste=0.1% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 589/8250] loss=2.5117 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.3426 tok/s=1,477,606 raw_tok/s=1,483,401 step_tokens=130,560 waste=0.4% h100_mfu=36.09% vram=397/17,340MB peak=16,747/17,340MB
[step 590/8250] loss=2.5541 lr=9.68e-05 muon_lr=9.68e-04 grad_norm=0.4294 tok/s=1,447,019 raw_tok/s=1,450,239 step_tokens=130,781 waste=0.2% h100_mfu=35.28% vram=397/17,340MB peak=16,747/17,340MB
[step 591/8250] loss=2.5026 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.3186 tok/s=1,419,745 raw_tok/s=1,421,839 step_tokens=130,879 waste=0.1% h100_mfu=34.59% vram=397/17,340MB peak=16,747/17,340MB
[step 592/8250] loss=2.5340 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.3336 tok/s=1,502,949 raw_tok/s=1,507,573 step_tokens=130,670 waste=0.3% h100_mfu=36.68% vram=397/17,340MB peak=16,747/17,340MB
[step 593/8250] loss=2.5088 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.3472 tok/s=1,378,957 raw_tok/s=1,383,146 step_tokens=130,675 waste=0.3% h100_mfu=33.65% vram=397/17,340MB peak=16,747/17,340MB
[step 594/8250] loss=2.4893 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.3595 tok/s=1,417,638 raw_tok/s=1,422,293 step_tokens=130,643 waste=0.3% h100_mfu=34.60% vram=397/17,340MB peak=16,747/17,340MB
[step 595/8250] loss=2.5302 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.2930 tok/s=1,316,680 raw_tok/s=1,317,816 step_tokens=130,959 waste=0.1% h100_mfu=32.06% vram=397/17,340MB peak=16,747/17,340MB
[step 596/8250] loss=2.5347 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.3142 tok/s=1,449,448 raw_tok/s=1,450,389 step_tokens=130,987 waste=0.1% h100_mfu=35.29% vram=397/17,340MB peak=16,747/17,340MB
[step 597/8250] loss=2.5283 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.3074 tok/s=1,358,238 raw_tok/s=1,360,781 step_tokens=130,827 waste=0.2% h100_mfu=33.11% vram=397/17,340MB peak=16,747/17,340MB
[step 598/8250] loss=2.5430 lr=9.67e-05 muon_lr=9.67e-04 grad_norm=0.2628 tok/s=1,513,699 raw_tok/s=1,515,827 step_tokens=130,888 waste=0.1% h100_mfu=36.88% vram=397/17,340MB peak=16,747/17,340MB
[step 599/8250] loss=2.5213 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.2846 tok/s=1,394,273 raw_tok/s=1,395,817 step_tokens=130,927 waste=0.1% h100_mfu=33.96% vram=397/17,340MB peak=16,747/17,340MB
[step 600/8250] loss=2.5317 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.3393 tok/s=1,443,854 raw_tok/s=1,446,657 step_tokens=130,818 waste=0.2% h100_mfu=35.20% vram=397/17,340MB peak=16,747/17,340MB
[step 601/8250] loss=2.4886 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.4333 tok/s=1,405,393 raw_tok/s=1,406,584 step_tokens=130,961 waste=0.1% h100_mfu=34.22% vram=397/17,340MB peak=16,747/17,340MB
[step 602/8250] loss=2.5086 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.3122 tok/s=1,429,320 raw_tok/s=1,435,662 step_tokens=130,493 waste=0.4% h100_mfu=34.93% vram=397/17,340MB peak=16,747/17,340MB
[step 603/8250] loss=2.5327 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.2634 tok/s=1,319,005 raw_tok/s=1,319,538 step_tokens=131,019 waste=0.0% h100_mfu=32.10% vram=397/17,340MB peak=16,747/17,340MB
[step 604/8250] loss=2.5108 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.3139 tok/s=1,444,540 raw_tok/s=1,448,164 step_tokens=130,744 waste=0.3% h100_mfu=35.23% vram=397/17,340MB peak=16,747/17,340MB
[step 605/8250] loss=2.5184 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.2611 tok/s=1,503,746 raw_tok/s=1,508,603 step_tokens=130,650 waste=0.3% h100_mfu=36.70% vram=397/17,340MB peak=16,747/17,340MB
[step 606/8250] loss=2.5083 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.2789 tok/s=1,503,348 raw_tok/s=1,507,639 step_tokens=130,699 waste=0.3% h100_mfu=36.68% vram=397/17,340MB peak=16,747/17,340MB
[step 607/8250] loss=2.5218 lr=9.66e-05 muon_lr=9.66e-04 grad_norm=0.2915 tok/s=1,433,146 raw_tok/s=1,438,855 step_tokens=130,552 waste=0.4% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 608/8250] loss=2.4890 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.3319 tok/s=1,481,584 raw_tok/s=1,487,496 step_tokens=130,551 waste=0.4% h100_mfu=36.19% vram=397/17,340MB peak=16,747/17,340MB
[step 609/8250] loss=2.5477 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.3029 tok/s=1,394,613 raw_tok/s=1,396,776 step_tokens=130,869 waste=0.2% h100_mfu=33.98% vram=397/17,340MB peak=16,747/17,340MB
[step 610/8250] loss=2.5009 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.3029 tok/s=1,238,946 raw_tok/s=1,242,691 step_tokens=130,677 waste=0.3% h100_mfu=30.23% vram=397/17,340MB peak=16,747/17,340MB
[step 611/8250] loss=2.5155 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.3753 tok/s=1,491,247 raw_tok/s=1,498,231 step_tokens=130,461 waste=0.5% h100_mfu=36.45% vram=397/17,340MB peak=16,747/17,340MB
[step 612/8250] loss=2.4770 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.3003 tok/s=1,390,917 raw_tok/s=1,396,607 step_tokens=130,538 waste=0.4% h100_mfu=33.98% vram=397/17,340MB peak=16,747/17,340MB
[step 613/8250] loss=2.5028 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.3029 tok/s=1,451,703 raw_tok/s=1,452,734 step_tokens=130,979 waste=0.1% h100_mfu=35.35% vram=397/17,340MB peak=16,747/17,340MB
[step 614/8250] loss=2.4763 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.2794 tok/s=1,401,628 raw_tok/s=1,402,270 step_tokens=131,012 waste=0.0% h100_mfu=34.12% vram=397/17,340MB peak=16,747/17,340MB
[step 615/8250] loss=2.5093 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.2605 tok/s=1,380,373 raw_tok/s=1,386,136 step_tokens=130,527 waste=0.4% h100_mfu=33.72% vram=397/17,340MB peak=16,747/17,340MB
[step 616/8250] loss=2.4742 lr=9.65e-05 muon_lr=9.65e-04 grad_norm=0.2567 tok/s=1,288,949 raw_tok/s=1,293,934 step_tokens=130,567 waste=0.4% h100_mfu=31.48% vram=397/17,340MB peak=16,747/17,340MB
[step 617/8250] loss=2.5196 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.2998 tok/s=1,450,626 raw_tok/s=1,456,415 step_tokens=130,551 waste=0.4% h100_mfu=35.43% vram=397/17,340MB peak=16,747/17,340MB
[step 618/8250] loss=2.5012 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.3201 tok/s=1,270,211 raw_tok/s=1,271,802 step_tokens=130,908 waste=0.1% h100_mfu=30.94% vram=397/17,340MB peak=16,747/17,340MB
[step 619/8250] loss=2.5116 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.2862 tok/s=1,415,769 raw_tok/s=1,420,799 step_tokens=130,608 waste=0.4% h100_mfu=34.57% vram=397/17,340MB peak=16,747/17,340MB
[step 620/8250] loss=2.4735 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.3053 tok/s=1,341,261 raw_tok/s=1,344,585 step_tokens=130,748 waste=0.2% h100_mfu=32.71% vram=397/17,340MB peak=16,747/17,340MB
[step 621/8250] loss=2.5235 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.3563 tok/s=1,411,659 raw_tok/s=1,417,152 step_tokens=130,564 waste=0.4% h100_mfu=34.48% vram=397/17,340MB peak=16,747/17,340MB
[step 622/8250] loss=2.5235 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.3488 tok/s=1,424,702 raw_tok/s=1,429,009 step_tokens=130,677 waste=0.3% h100_mfu=34.77% vram=397/17,340MB peak=16,747/17,340MB
[step 623/8250] loss=2.5364 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.2867 tok/s=1,395,528 raw_tok/s=1,398,259 step_tokens=130,816 waste=0.2% h100_mfu=34.02% vram=397/17,340MB peak=16,747/17,340MB
[step 624/8250] loss=2.5066 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.3433 tok/s=1,425,375 raw_tok/s=1,430,165 step_tokens=130,633 waste=0.3% h100_mfu=34.80% vram=397/17,340MB peak=16,747/17,340MB
[step 625/8250] loss=2.5103 lr=9.64e-05 muon_lr=9.64e-04 grad_norm=0.3431 tok/s=1,506,747 raw_tok/s=1,512,574 step_tokens=130,567 waste=0.4% h100_mfu=36.80% vram=397/17,340MB peak=16,747/17,340MB
[step 626/8250] loss=2.5031 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.3162 tok/s=1,423,108 raw_tok/s=1,426,973 step_tokens=130,717 waste=0.3% h100_mfu=34.72% vram=397/17,340MB peak=16,747/17,340MB
[step 627/8250] loss=2.4992 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.3415 tok/s=1,464,116 raw_tok/s=1,469,014 step_tokens=130,635 waste=0.3% h100_mfu=35.74% vram=397/17,340MB peak=16,747/17,340MB
[step 628/8250] loss=2.4766 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.3428 tok/s=1,303,112 raw_tok/s=1,304,596 step_tokens=130,923 waste=0.1% h100_mfu=31.74% vram=397/17,340MB peak=16,747/17,340MB
[step 629/8250] loss=2.4750 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.3025 tok/s=1,377,347 raw_tok/s=1,384,976 step_tokens=130,350 waste=0.6% h100_mfu=33.70% vram=397/17,340MB peak=16,747/17,340MB
[step 630/8250] loss=2.4351 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.2779 tok/s=1,184,356 raw_tok/s=1,188,136 step_tokens=130,655 waste=0.3% h100_mfu=28.91% vram=397/17,340MB peak=16,747/17,340MB
[step 631/8250] loss=2.4668 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.2949 tok/s=1,285,083 raw_tok/s=1,287,972 step_tokens=130,778 waste=0.2% h100_mfu=31.34% vram=397/17,340MB peak=16,747/17,340MB
[step 632/8250] loss=2.4844 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.2994 tok/s=1,340,863 raw_tok/s=1,343,785 step_tokens=130,787 waste=0.2% h100_mfu=32.69% vram=397/17,340MB peak=16,747/17,340MB
[step 633/8250] loss=2.5292 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.3841 tok/s=1,442,926 raw_tok/s=1,448,152 step_tokens=130,599 waste=0.4% h100_mfu=35.23% vram=397/17,340MB peak=16,747/17,340MB
[step 634/8250] loss=2.4840 lr=9.63e-05 muon_lr=9.63e-04 grad_norm=0.3458 tok/s=1,377,741 raw_tok/s=1,380,036 step_tokens=130,854 waste=0.2% h100_mfu=33.58% vram=397/17,340MB peak=16,747/17,340MB
[step 635/8250] loss=2.4868 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.3253 tok/s=1,390,668 raw_tok/s=1,392,378 step_tokens=130,911 waste=0.1% h100_mfu=33.88% vram=397/17,340MB peak=16,747/17,340MB
[step 636/8250] loss=2.4873 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.2962 tok/s=1,419,846 raw_tok/s=1,423,114 step_tokens=130,771 waste=0.2% h100_mfu=34.62% vram=397/17,340MB peak=16,747/17,340MB
[step 637/8250] loss=2.5125 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.3595 tok/s=1,394,886 raw_tok/s=1,395,738 step_tokens=130,992 waste=0.1% h100_mfu=33.96% vram=397/17,340MB peak=16,747/17,340MB
[step 638/8250] loss=2.4741 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.3544 tok/s=1,290,311 raw_tok/s=1,295,044 step_tokens=130,593 waste=0.4% h100_mfu=31.51% vram=397/17,340MB peak=16,747/17,340MB
[step 639/8250] loss=2.4560 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.2817 tok/s=1,410,024 raw_tok/s=1,410,874 step_tokens=130,993 waste=0.1% h100_mfu=34.33% vram=397/17,340MB peak=16,747/17,340MB
[step 640/8250] loss=2.4992 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.3561 tok/s=1,422,856 raw_tok/s=1,424,650 step_tokens=130,907 waste=0.1% h100_mfu=34.66% vram=397/17,340MB peak=16,747/17,340MB
[step 641/8250] loss=2.4317 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.4130 tok/s=1,463,193 raw_tok/s=1,467,593 step_tokens=130,679 waste=0.3% h100_mfu=35.71% vram=397/17,340MB peak=16,747/17,340MB
[step 642/8250] loss=2.5254 lr=9.62e-05 muon_lr=9.62e-04 grad_norm=0.2639 tok/s=1,424,082 raw_tok/s=1,424,876 step_tokens=130,999 waste=0.1% h100_mfu=34.67% vram=397/17,340MB peak=16,747/17,340MB
[step 643/8250] loss=2.4893 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.2915 tok/s=1,340,935 raw_tok/s=1,341,191 step_tokens=131,047 waste=0.0% h100_mfu=32.63% vram=397/17,340MB peak=16,747/17,340MB
[step 644/8250] loss=2.5095 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.2778 tok/s=1,461,873 raw_tok/s=1,463,190 step_tokens=130,954 waste=0.1% h100_mfu=35.60% vram=397/17,340MB peak=16,747/17,340MB
[step 645/8250] loss=2.4898 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.3345 tok/s=1,388,688 raw_tok/s=1,391,150 step_tokens=130,840 waste=0.2% h100_mfu=33.85% vram=397/17,340MB peak=16,747/17,340MB
[step 646/8250] loss=2.5008 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.3184 tok/s=1,199,953 raw_tok/s=1,204,031 step_tokens=130,628 waste=0.3% h100_mfu=29.29% vram=397/17,340MB peak=16,747/17,340MB
[step 647/8250] loss=2.4656 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.3595 tok/s=1,261,852 raw_tok/s=1,266,500 step_tokens=130,591 waste=0.4% h100_mfu=30.81% vram=397/17,340MB peak=16,747/17,340MB
[step 648/8250] loss=2.5105 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.3899 tok/s=1,309,477 raw_tok/s=1,311,228 step_tokens=130,897 waste=0.1% h100_mfu=31.90% vram=397/17,340MB peak=16,747/17,340MB
[step 649/8250] loss=2.4350 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.3243 tok/s=1,321,520 raw_tok/s=1,324,501 step_tokens=130,777 waste=0.2% h100_mfu=32.23% vram=397/17,340MB peak=16,747/17,340MB
[step 650/8250] loss=2.5089 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.2753 tok/s=1,269,688 raw_tok/s=1,272,805 step_tokens=130,751 waste=0.2% h100_mfu=30.97% vram=397/17,340MB peak=16,747/17,340MB
[step 651/8250] loss=2.4711 lr=9.61e-05 muon_lr=9.61e-04 grad_norm=0.2701 tok/s=1,339,771 raw_tok/s=1,344,397 step_tokens=130,621 waste=0.3% h100_mfu=32.71% vram=397/17,340MB peak=16,747/17,340MB
[step 652/8250] loss=2.5133 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.2789 tok/s=1,398,275 raw_tok/s=1,403,318 step_tokens=130,601 waste=0.4% h100_mfu=34.14% vram=397/17,340MB peak=16,747/17,340MB
[step 653/8250] loss=2.4910 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3108 tok/s=1,482,068 raw_tok/s=1,485,559 step_tokens=130,764 waste=0.2% h100_mfu=36.14% vram=397/17,340MB peak=16,747/17,340MB
[step 654/8250] loss=2.4976 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3027 tok/s=1,447,460 raw_tok/s=1,451,457 step_tokens=130,711 waste=0.3% h100_mfu=35.31% vram=397/17,340MB peak=16,747/17,340MB
[step 655/8250] loss=2.4979 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3071 tok/s=1,419,709 raw_tok/s=1,423,848 step_tokens=130,691 waste=0.3% h100_mfu=34.64% vram=397/17,340MB peak=16,747/17,340MB
[step 656/8250] loss=2.5034 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3852 tok/s=1,375,074 raw_tok/s=1,376,502 step_tokens=130,936 waste=0.1% h100_mfu=33.49% vram=397/17,340MB peak=16,747/17,340MB
[step 657/8250] loss=2.5299 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3525 tok/s=1,432,070 raw_tok/s=1,434,379 step_tokens=130,861 waste=0.2% h100_mfu=34.90% vram=397/17,340MB peak=16,747/17,340MB
[step 658/8250] loss=2.5265 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3524 tok/s=1,410,234 raw_tok/s=1,415,797 step_tokens=130,557 waste=0.4% h100_mfu=34.45% vram=397/17,340MB peak=16,747/17,340MB
[step 659/8250] loss=2.4537 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3415 tok/s=1,455,235 raw_tok/s=1,458,618 step_tokens=130,768 waste=0.2% h100_mfu=35.49% vram=397/17,340MB peak=16,747/17,340MB
[step 660/8250] loss=2.4967 lr=9.60e-05 muon_lr=9.60e-04 grad_norm=0.3073 tok/s=1,318,104 raw_tok/s=1,320,159 step_tokens=130,868 waste=0.2% h100_mfu=32.12% vram=397/17,340MB peak=16,747/17,340MB
[step 661/8250] loss=2.4913 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3157 tok/s=1,474,220 raw_tok/s=1,478,235 step_tokens=130,716 waste=0.3% h100_mfu=35.97% vram=397/17,340MB peak=16,747/17,340MB
[step 662/8250] loss=2.4904 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3485 tok/s=1,419,324 raw_tok/s=1,420,364 step_tokens=130,976 waste=0.1% h100_mfu=34.56% vram=397/17,340MB peak=16,747/17,340MB
[step 663/8250] loss=2.4909 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3614 tok/s=1,399,936 raw_tok/s=1,404,501 step_tokens=130,646 waste=0.3% h100_mfu=34.17% vram=397/17,340MB peak=16,747/17,340MB
[step 664/8250] loss=2.4639 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3356 tok/s=1,428,497 raw_tok/s=1,430,877 step_tokens=130,854 waste=0.2% h100_mfu=34.81% vram=397/17,340MB peak=16,747/17,340MB
[step 665/8250] loss=2.4080 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3353 tok/s=1,386,574 raw_tok/s=1,392,683 step_tokens=130,497 waste=0.4% h100_mfu=33.88% vram=397/17,340MB peak=16,747/17,340MB
[step 666/8250] loss=2.4962 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3232 tok/s=1,375,934 raw_tok/s=1,378,131 step_tokens=130,863 waste=0.2% h100_mfu=33.53% vram=397/17,340MB peak=16,747/17,340MB
[step 667/8250] loss=2.4957 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3274 tok/s=1,380,764 raw_tok/s=1,383,646 step_tokens=130,799 waste=0.2% h100_mfu=33.66% vram=397/17,340MB peak=16,747/17,340MB
[step 668/8250] loss=2.4731 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.3578 tok/s=1,447,250 raw_tok/s=1,448,432 step_tokens=130,965 waste=0.1% h100_mfu=35.24% vram=397/17,340MB peak=16,747/17,340MB
[step 669/8250] loss=2.4554 lr=9.59e-05 muon_lr=9.59e-04 grad_norm=0.2981 tok/s=1,418,872 raw_tok/s=1,423,738 step_tokens=130,624 waste=0.3% h100_mfu=34.64% vram=397/17,340MB peak=16,747/17,340MB
[step 670/8250] loss=2.4619 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.2930 tok/s=1,414,485 raw_tok/s=1,417,860 step_tokens=130,760 waste=0.2% h100_mfu=34.50% vram=397/17,340MB peak=16,747/17,340MB
[step 671/8250] loss=2.4349 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3932 tok/s=1,424,882 raw_tok/s=1,427,561 step_tokens=130,826 waste=0.2% h100_mfu=34.73% vram=397/17,340MB peak=16,747/17,340MB
[step 672/8250] loss=2.4613 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3942 tok/s=1,439,423 raw_tok/s=1,442,008 step_tokens=130,837 waste=0.2% h100_mfu=35.08% vram=397/17,340MB peak=16,747/17,340MB
[step 673/8250] loss=2.4474 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3354 tok/s=1,436,901 raw_tok/s=1,440,264 step_tokens=130,766 waste=0.2% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 674/8250] loss=2.4695 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3187 tok/s=1,437,691 raw_tok/s=1,440,285 step_tokens=130,836 waste=0.2% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 675/8250] loss=2.4871 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3111 tok/s=1,396,800 raw_tok/s=1,399,191 step_tokens=130,848 waste=0.2% h100_mfu=34.04% vram=397/17,340MB peak=16,747/17,340MB
[step 676/8250] loss=2.4344 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3389 tok/s=1,329,002 raw_tok/s=1,331,847 step_tokens=130,792 waste=0.2% h100_mfu=32.40% vram=397/17,340MB peak=16,747/17,340MB
[step 677/8250] loss=2.4459 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3574 tok/s=1,399,957 raw_tok/s=1,406,729 step_tokens=130,441 waste=0.5% h100_mfu=34.23% vram=397/17,340MB peak=16,747/17,340MB
[step 678/8250] loss=2.4252 lr=9.58e-05 muon_lr=9.58e-04 grad_norm=0.3450 tok/s=1,354,875 raw_tok/s=1,355,733 step_tokens=130,989 waste=0.1% h100_mfu=32.99% vram=397/17,340MB peak=16,747/17,340MB
[step 679/8250] loss=2.4211 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3307 tok/s=1,356,664 raw_tok/s=1,358,042 step_tokens=130,939 waste=0.1% h100_mfu=33.04% vram=397/17,340MB peak=16,747/17,340MB
[step 680/8250] loss=2.4882 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3310 tok/s=1,358,398 raw_tok/s=1,359,695 step_tokens=130,947 waste=0.1% h100_mfu=33.08% vram=397/17,340MB peak=16,747/17,340MB
[step 681/8250] loss=2.5072 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.2886 tok/s=1,465,356 raw_tok/s=1,472,242 step_tokens=130,459 waste=0.5% h100_mfu=35.82% vram=397/17,340MB peak=16,747/17,340MB
[step 682/8250] loss=2.4786 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3239 tok/s=1,425,723 raw_tok/s=1,430,558 step_tokens=130,629 waste=0.3% h100_mfu=34.81% vram=397/17,340MB peak=16,747/17,340MB
[step 683/8250] loss=2.4770 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.2889 tok/s=1,444,353 raw_tok/s=1,446,361 step_tokens=130,890 waste=0.1% h100_mfu=35.19% vram=397/17,340MB peak=16,747/17,340MB
[step 684/8250] loss=2.4414 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3409 tok/s=1,398,294 raw_tok/s=1,402,434 step_tokens=130,685 waste=0.3% h100_mfu=34.12% vram=397/17,340MB peak=16,747/17,340MB
[step 685/8250] loss=2.4823 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3315 tok/s=1,282,277 raw_tok/s=1,286,153 step_tokens=130,677 waste=0.3% h100_mfu=31.29% vram=397/17,340MB peak=16,747/17,340MB
[step 686/8250] loss=2.4680 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3559 tok/s=1,437,785 raw_tok/s=1,443,301 step_tokens=130,571 waste=0.4% h100_mfu=35.12% vram=397/17,340MB peak=16,747/17,340MB
[step 687/8250] loss=2.4630 lr=9.57e-05 muon_lr=9.57e-04 grad_norm=0.3547 tok/s=1,487,247 raw_tok/s=1,490,180 step_tokens=130,814 waste=0.2% h100_mfu=36.26% vram=397/17,340MB peak=16,747/17,340MB
[step 688/8250] loss=2.4607 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.3162 tok/s=1,461,101 raw_tok/s=1,464,990 step_tokens=130,724 waste=0.3% h100_mfu=35.64% vram=397/17,340MB peak=16,747/17,340MB
[step 689/8250] loss=2.4479 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.4676 tok/s=1,318,333 raw_tok/s=1,321,327 step_tokens=130,775 waste=0.2% h100_mfu=32.15% vram=397/17,340MB peak=16,747/17,340MB
[step 690/8250] loss=2.4859 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.3684 tok/s=1,386,570 raw_tok/s=1,390,719 step_tokens=130,681 waste=0.3% h100_mfu=33.84% vram=397/17,340MB peak=16,747/17,340MB
[step 691/8250] loss=2.4512 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.3277 tok/s=1,378,477 raw_tok/s=1,382,094 step_tokens=130,729 waste=0.3% h100_mfu=33.63% vram=397/17,340MB peak=16,747/17,340MB
[step 692/8250] loss=2.4902 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.3571 tok/s=1,382,026 raw_tok/s=1,384,255 step_tokens=130,861 waste=0.2% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 693/8250] loss=2.4446 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.2838 tok/s=1,431,082 raw_tok/s=1,435,035 step_tokens=130,711 waste=0.3% h100_mfu=34.91% vram=397/17,340MB peak=16,747/17,340MB
[step 694/8250] loss=2.4911 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.3238 tok/s=1,392,011 raw_tok/s=1,398,755 step_tokens=130,440 waste=0.5% h100_mfu=34.03% vram=397/17,340MB peak=16,747/17,340MB
[step 695/8250] loss=2.4329 lr=9.56e-05 muon_lr=9.56e-04 grad_norm=0.3812 tok/s=1,388,760 raw_tok/s=1,391,201 step_tokens=130,842 waste=0.2% h100_mfu=33.85% vram=397/17,340MB peak=16,747/17,340MB
[step 696/8250] loss=2.4141 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3584 tok/s=1,373,456 raw_tok/s=1,378,051 step_tokens=130,635 waste=0.3% h100_mfu=33.53% vram=397/17,340MB peak=16,747/17,340MB
[step 697/8250] loss=2.4390 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3042 tok/s=1,423,456 raw_tok/s=1,428,130 step_tokens=130,643 waste=0.3% h100_mfu=34.75% vram=397/17,340MB peak=16,747/17,340MB
[step 698/8250] loss=2.4123 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3054 tok/s=1,475,310 raw_tok/s=1,478,310 step_tokens=130,806 waste=0.2% h100_mfu=35.97% vram=397/17,340MB peak=16,747/17,340MB
[step 699/8250] loss=2.4344 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3130 tok/s=1,472,799 raw_tok/s=1,473,924 step_tokens=130,972 waste=0.1% h100_mfu=35.86% vram=397/17,340MB peak=16,747/17,340MB
[step 700/8250] loss=2.4923 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.2842 tok/s=1,265,496 raw_tok/s=1,267,353 step_tokens=130,880 waste=0.1% h100_mfu=30.83% vram=397/17,340MB peak=16,747/17,340MB
[step 701/8250] loss=2.4822 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.2862 tok/s=1,440,097 raw_tok/s=1,441,891 step_tokens=130,909 waste=0.1% h100_mfu=35.08% vram=397/17,340MB peak=16,747/17,340MB
[step 702/8250] loss=2.4512 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3066 tok/s=1,380,920 raw_tok/s=1,383,432 step_tokens=130,834 waste=0.2% h100_mfu=33.66% vram=397/17,340MB peak=16,747/17,340MB
[step 703/8250] loss=2.4526 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3482 tok/s=1,439,939 raw_tok/s=1,441,489 step_tokens=130,931 waste=0.1% h100_mfu=35.07% vram=397/17,340MB peak=16,747/17,340MB
[step 704/8250] loss=2.4763 lr=9.55e-05 muon_lr=9.55e-04 grad_norm=0.3365 tok/s=1,211,643 raw_tok/s=1,213,430 step_tokens=130,879 waste=0.1% h100_mfu=29.52% vram=397/17,340MB peak=16,747/17,340MB
[step 705/8250] loss=2.4613 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3516 tok/s=1,359,249 raw_tok/s=1,366,547 step_tokens=130,372 waste=0.5% h100_mfu=33.25% vram=397/17,340MB peak=16,747/17,340MB
[step 706/8250] loss=2.4695 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3858 tok/s=1,401,054 raw_tok/s=1,403,710 step_tokens=130,824 waste=0.2% h100_mfu=34.15% vram=397/17,340MB peak=16,747/17,340MB
[step 707/8250] loss=2.4418 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.2973 tok/s=1,456,633 raw_tok/s=1,460,410 step_tokens=130,733 waste=0.3% h100_mfu=35.53% vram=397/17,340MB peak=16,747/17,340MB
[step 708/8250] loss=2.4403 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.4294 tok/s=1,440,114 raw_tok/s=1,446,027 step_tokens=130,536 waste=0.4% h100_mfu=35.18% vram=397/17,340MB peak=16,747/17,340MB
[step 709/8250] loss=2.4343 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3024 tok/s=1,389,597 raw_tok/s=1,392,806 step_tokens=130,770 waste=0.2% h100_mfu=33.89% vram=397/17,340MB peak=16,747/17,340MB
[step 710/8250] loss=2.4154 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3206 tok/s=1,367,613 raw_tok/s=1,371,159 step_tokens=130,733 waste=0.3% h100_mfu=33.36% vram=397/17,340MB peak=16,747/17,340MB
[step 711/8250] loss=2.4588 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3455 tok/s=1,505,031 raw_tok/s=1,509,638 step_tokens=130,672 waste=0.3% h100_mfu=36.73% vram=397/17,340MB peak=16,747/17,340MB
[step 712/8250] loss=2.4581 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3784 tok/s=1,376,760 raw_tok/s=1,379,423 step_tokens=130,819 waste=0.2% h100_mfu=33.56% vram=397/17,340MB peak=16,747/17,340MB
[step 713/8250] loss=2.4174 lr=9.54e-05 muon_lr=9.54e-04 grad_norm=0.3639 tok/s=1,422,106 raw_tok/s=1,426,536 step_tokens=130,665 waste=0.3% h100_mfu=34.71% vram=397/17,340MB peak=16,747/17,340MB
[step 714/8250] loss=2.4450 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3162 tok/s=1,370,373 raw_tok/s=1,375,631 step_tokens=130,571 waste=0.4% h100_mfu=33.47% vram=397/17,340MB peak=16,747/17,340MB
[step 715/8250] loss=2.4478 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3084 tok/s=1,422,932 raw_tok/s=1,427,179 step_tokens=130,682 waste=0.3% h100_mfu=34.72% vram=397/17,340MB peak=16,747/17,340MB
[step 716/8250] loss=2.4593 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3200 tok/s=1,485,805 raw_tok/s=1,487,723 step_tokens=130,903 waste=0.1% h100_mfu=36.20% vram=397/17,340MB peak=16,747/17,340MB
[step 717/8250] loss=2.4411 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3916 tok/s=1,369,957 raw_tok/s=1,376,226 step_tokens=130,475 waste=0.5% h100_mfu=33.48% vram=397/17,340MB peak=16,747/17,340MB
[step 718/8250] loss=2.4465 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.4160 tok/s=1,387,802 raw_tok/s=1,390,645 step_tokens=130,804 waste=0.2% h100_mfu=33.83% vram=397/17,340MB peak=16,747/17,340MB
[step 719/8250] loss=2.4271 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.4051 tok/s=1,390,199 raw_tok/s=1,391,898 step_tokens=130,912 waste=0.1% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 720/8250] loss=2.4449 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3760 tok/s=1,437,982 raw_tok/s=1,441,182 step_tokens=130,781 waste=0.2% h100_mfu=35.06% vram=397/17,340MB peak=16,747/17,340MB
[step 721/8250] loss=2.4042 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3463 tok/s=1,361,419 raw_tok/s=1,363,156 step_tokens=130,905 waste=0.1% h100_mfu=33.17% vram=397/17,340MB peak=16,747/17,340MB
[step 722/8250] loss=2.4030 lr=9.53e-05 muon_lr=9.53e-04 grad_norm=0.3330 tok/s=1,401,224 raw_tok/s=1,404,063 step_tokens=130,807 waste=0.2% h100_mfu=34.16% vram=397/17,340MB peak=16,747/17,340MB
[step 723/8250] loss=2.3864 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.2999 tok/s=1,493,840 raw_tok/s=1,496,741 step_tokens=130,818 waste=0.2% h100_mfu=36.42% vram=397/17,340MB peak=16,747/17,340MB
[step 724/8250] loss=2.4999 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.2957 tok/s=1,301,348 raw_tok/s=1,304,303 step_tokens=130,775 waste=0.2% h100_mfu=31.73% vram=397/17,340MB peak=16,747/17,340MB
[step 725/8250] loss=2.4494 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.3124 tok/s=1,431,382 raw_tok/s=1,436,863 step_tokens=130,572 waste=0.4% h100_mfu=34.96% vram=397/17,340MB peak=16,747/17,340MB
[step 726/8250] loss=2.4466 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.3351 tok/s=1,437,461 raw_tok/s=1,441,221 step_tokens=130,730 waste=0.3% h100_mfu=35.07% vram=397/17,340MB peak=16,747/17,340MB
[step 727/8250] loss=2.4121 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.3374 tok/s=1,437,344 raw_tok/s=1,438,837 step_tokens=130,936 waste=0.1% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 728/8250] loss=2.4217 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.3538 tok/s=1,308,329 raw_tok/s=1,312,354 step_tokens=130,670 waste=0.3% h100_mfu=31.93% vram=397/17,340MB peak=16,747/17,340MB
[step 729/8250] loss=2.4455 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.3342 tok/s=1,414,698 raw_tok/s=1,420,943 step_tokens=130,496 waste=0.4% h100_mfu=34.57% vram=397/17,340MB peak=16,747/17,340MB
[step 730/8250] loss=2.4633 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.3219 tok/s=1,412,660 raw_tok/s=1,414,981 step_tokens=130,857 waste=0.2% h100_mfu=34.43% vram=397/17,340MB peak=16,747/17,340MB
[step 731/8250] loss=2.4507 lr=9.52e-05 muon_lr=9.52e-04 grad_norm=0.2905 tok/s=1,385,573 raw_tok/s=1,391,711 step_tokens=130,494 waste=0.4% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 732/8250] loss=2.4101 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.2947 tok/s=1,492,198 raw_tok/s=1,492,836 step_tokens=131,016 waste=0.0% h100_mfu=36.32% vram=397/17,340MB peak=16,747/17,340MB
[step 733/8250] loss=2.4596 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3493 tok/s=1,431,567 raw_tok/s=1,438,978 step_tokens=130,397 waste=0.5% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 734/8250] loss=2.4334 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3574 tok/s=1,409,280 raw_tok/s=1,415,305 step_tokens=130,514 waste=0.4% h100_mfu=34.43% vram=397/17,340MB peak=16,747/17,340MB
[step 735/8250] loss=2.4540 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3131 tok/s=1,373,555 raw_tok/s=1,377,454 step_tokens=130,701 waste=0.3% h100_mfu=33.51% vram=397/17,340MB peak=16,747/17,340MB
[step 736/8250] loss=2.4681 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3042 tok/s=1,411,411 raw_tok/s=1,415,959 step_tokens=130,651 waste=0.3% h100_mfu=34.45% vram=397/17,340MB peak=16,747/17,340MB
[step 737/8250] loss=2.4301 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3086 tok/s=1,391,954 raw_tok/s=1,394,028 step_tokens=130,877 waste=0.1% h100_mfu=33.92% vram=397/17,340MB peak=16,747/17,340MB
[step 738/8250] loss=2.4543 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3259 tok/s=1,436,647 raw_tok/s=1,439,238 step_tokens=130,836 waste=0.2% h100_mfu=35.02% vram=397/17,340MB peak=16,747/17,340MB
[step 739/8250] loss=2.4348 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3451 tok/s=1,379,868 raw_tok/s=1,382,579 step_tokens=130,815 waste=0.2% h100_mfu=33.64% vram=397/17,340MB peak=16,747/17,340MB
[step 740/8250] loss=2.4204 lr=9.51e-05 muon_lr=9.51e-04 grad_norm=0.3489 tok/s=1,452,485 raw_tok/s=1,452,861 step_tokens=131,038 waste=0.0% h100_mfu=35.35% vram=397/17,340MB peak=16,747/17,340MB
[step 741/8250] loss=2.4477 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3448 tok/s=1,434,289 raw_tok/s=1,441,205 step_tokens=130,443 waste=0.5% h100_mfu=35.06% vram=397/17,340MB peak=16,747/17,340MB
[step 742/8250] loss=2.4192 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3417 tok/s=1,487,141 raw_tok/s=1,490,428 step_tokens=130,783 waste=0.2% h100_mfu=36.26% vram=397/17,340MB peak=16,747/17,340MB
[step 743/8250] loss=2.4234 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3268 tok/s=1,264,835 raw_tok/s=1,269,056 step_tokens=130,636 waste=0.3% h100_mfu=30.88% vram=397/17,340MB peak=16,747/17,340MB
[step 744/8250] loss=2.4558 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.2941 tok/s=1,496,265 raw_tok/s=1,499,950 step_tokens=130,750 waste=0.2% h100_mfu=36.49% vram=397/17,340MB peak=16,747/17,340MB
[step 745/8250] loss=2.4342 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3740 tok/s=1,332,883 raw_tok/s=1,337,116 step_tokens=130,657 waste=0.3% h100_mfu=32.53% vram=397/17,340MB peak=16,747/17,340MB
[step 746/8250] loss=2.4387 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3594 tok/s=1,415,801 raw_tok/s=1,419,233 step_tokens=130,755 waste=0.2% h100_mfu=34.53% vram=397/17,340MB peak=16,747/17,340MB
[step 747/8250] loss=2.4151 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3717 tok/s=1,402,528 raw_tok/s=1,405,520 step_tokens=130,793 waste=0.2% h100_mfu=34.20% vram=397/17,340MB peak=16,747/17,340MB
[step 748/8250] loss=2.4462 lr=9.50e-05 muon_lr=9.50e-04 grad_norm=0.3672 tok/s=1,437,423 raw_tok/s=1,440,258 step_tokens=130,814 waste=0.2% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 749/8250] loss=2.4611 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.4472 tok/s=1,366,555 raw_tok/s=1,368,288 step_tokens=130,906 waste=0.1% h100_mfu=33.29% vram=397/17,340MB peak=16,747/17,340MB
[step 750/8250] loss=2.4362 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.4133 tok/s=1,440,578 raw_tok/s=1,444,236 step_tokens=130,740 waste=0.3% h100_mfu=35.14% vram=397/17,340MB peak=16,747/17,340MB
[step 750] eval_loss=2.4320
[step 751/8250] loss=2.4703 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.4033 tok/s=1,158,131 raw_tok/s=1,162,859 step_tokens=130,539 waste=0.4% h100_mfu=28.29% vram=397/17,340MB peak=16,750/17,340MB
[step 752/8250] loss=2.4230 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.3329 tok/s=1,497,864 raw_tok/s=1,501,840 step_tokens=130,725 waste=0.3% h100_mfu=36.54% vram=397/17,340MB peak=16,747/17,340MB
[step 753/8250] loss=2.3972 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.3713 tok/s=1,377,713 raw_tok/s=1,380,547 step_tokens=130,803 waste=0.2% h100_mfu=33.59% vram=397/17,340MB peak=16,747/17,340MB
[step 754/8250] loss=2.4318 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.3307 tok/s=1,382,484 raw_tok/s=1,387,119 step_tokens=130,634 waste=0.3% h100_mfu=33.75% vram=397/17,340MB peak=16,747/17,340MB
[step 755/8250] loss=2.4664 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.3379 tok/s=1,462,788 raw_tok/s=1,470,439 step_tokens=130,390 waste=0.5% h100_mfu=35.78% vram=397/17,340MB peak=16,747/17,340MB
[step 756/8250] loss=2.4299 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.3041 tok/s=1,327,439 raw_tok/s=1,334,126 step_tokens=130,415 waste=0.5% h100_mfu=32.46% vram=397/17,340MB peak=16,747/17,340MB
[step 757/8250] loss=2.4188 lr=9.49e-05 muon_lr=9.49e-04 grad_norm=0.3309 tok/s=1,394,059 raw_tok/s=1,399,558 step_tokens=130,557 waste=0.4% h100_mfu=34.05% vram=397/17,340MB peak=16,747/17,340MB
[step 758/8250] loss=2.4664 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.4338 tok/s=1,363,311 raw_tok/s=1,365,822 step_tokens=130,831 waste=0.2% h100_mfu=33.23% vram=397/17,340MB peak=16,747/17,340MB
[step 759/8250] loss=2.4331 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3882 tok/s=1,428,352 raw_tok/s=1,430,830 step_tokens=130,845 waste=0.2% h100_mfu=34.81% vram=397/17,340MB peak=16,747/17,340MB
[step 760/8250] loss=2.4004 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3383 tok/s=1,372,141 raw_tok/s=1,373,619 step_tokens=130,931 waste=0.1% h100_mfu=33.42% vram=397/17,340MB peak=16,747/17,340MB
[step 761/8250] loss=2.4010 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3734 tok/s=1,349,195 raw_tok/s=1,351,897 step_tokens=130,810 waste=0.2% h100_mfu=32.89% vram=397/17,340MB peak=16,747/17,340MB
[step 762/8250] loss=2.4300 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3898 tok/s=1,439,200 raw_tok/s=1,441,466 step_tokens=130,866 waste=0.2% h100_mfu=35.07% vram=397/17,340MB peak=16,747/17,340MB
[step 763/8250] loss=2.4230 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3674 tok/s=1,386,500 raw_tok/s=1,389,798 step_tokens=130,761 waste=0.2% h100_mfu=33.81% vram=397/17,340MB peak=16,747/17,340MB
[step 764/8250] loss=2.4251 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.4347 tok/s=1,370,047 raw_tok/s=1,372,655 step_tokens=130,823 waste=0.2% h100_mfu=33.40% vram=397/17,340MB peak=16,747/17,340MB
[step 765/8250] loss=2.4240 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3577 tok/s=1,391,880 raw_tok/s=1,395,628 step_tokens=130,720 waste=0.3% h100_mfu=33.96% vram=397/17,340MB peak=16,747/17,340MB
[step 766/8250] loss=2.4328 lr=9.48e-05 muon_lr=9.48e-04 grad_norm=0.3953 tok/s=1,483,546 raw_tok/s=1,490,288 step_tokens=130,479 waste=0.5% h100_mfu=36.26% vram=397/17,340MB peak=16,747/17,340MB
[step 767/8250] loss=2.4306 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3342 tok/s=1,431,327 raw_tok/s=1,433,361 step_tokens=130,886 waste=0.1% h100_mfu=34.87% vram=397/17,340MB peak=16,747/17,340MB
[step 768/8250] loss=2.4361 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3463 tok/s=1,414,331 raw_tok/s=1,418,867 step_tokens=130,653 waste=0.3% h100_mfu=34.52% vram=397/17,340MB peak=16,747/17,340MB
[step 769/8250] loss=2.3714 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3620 tok/s=1,424,845 raw_tok/s=1,427,623 step_tokens=130,817 waste=0.2% h100_mfu=34.73% vram=397/17,340MB peak=16,747/17,340MB
[step 770/8250] loss=2.4410 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.4008 tok/s=1,252,833 raw_tok/s=1,258,884 step_tokens=130,442 waste=0.5% h100_mfu=30.63% vram=397/17,340MB peak=16,747/17,340MB
[step 771/8250] loss=2.4028 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3780 tok/s=1,379,212 raw_tok/s=1,385,756 step_tokens=130,453 waste=0.5% h100_mfu=33.72% vram=397/17,340MB peak=16,747/17,340MB
[step 772/8250] loss=2.3832 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3362 tok/s=1,359,226 raw_tok/s=1,360,316 step_tokens=130,967 waste=0.1% h100_mfu=33.10% vram=397/17,340MB peak=16,747/17,340MB
[step 773/8250] loss=2.4085 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3517 tok/s=1,424,114 raw_tok/s=1,426,225 step_tokens=130,878 waste=0.1% h100_mfu=34.70% vram=397/17,340MB peak=16,747/17,340MB
[step 774/8250] loss=2.3632 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3694 tok/s=1,428,251 raw_tok/s=1,432,601 step_tokens=130,674 waste=0.3% h100_mfu=34.86% vram=397/17,340MB peak=16,747/17,340MB
[step 775/8250] loss=2.4353 lr=9.47e-05 muon_lr=9.47e-04 grad_norm=0.3593 tok/s=1,375,975 raw_tok/s=1,381,773 step_tokens=130,522 waste=0.4% h100_mfu=33.62% vram=397/17,340MB peak=16,747/17,340MB
[step 776/8250] loss=2.4644 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3621 tok/s=1,429,345 raw_tok/s=1,433,040 step_tokens=130,734 waste=0.3% h100_mfu=34.87% vram=397/17,340MB peak=16,747/17,340MB
[step 777/8250] loss=2.4468 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3169 tok/s=1,462,495 raw_tok/s=1,465,928 step_tokens=130,765 waste=0.2% h100_mfu=35.67% vram=397/17,340MB peak=16,747/17,340MB
[step 778/8250] loss=2.3982 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3236 tok/s=1,360,548 raw_tok/s=1,367,769 step_tokens=130,380 waste=0.5% h100_mfu=33.28% vram=397/17,340MB peak=16,747/17,340MB
[step 779/8250] loss=2.4365 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3471 tok/s=1,490,178 raw_tok/s=1,499,087 step_tokens=130,293 waste=0.6% h100_mfu=36.47% vram=397/17,340MB peak=16,747/17,340MB
[step 780/8250] loss=2.3830 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3407 tok/s=1,336,555 raw_tok/s=1,339,273 step_tokens=130,806 waste=0.2% h100_mfu=32.58% vram=397/17,340MB peak=16,747/17,340MB
[step 781/8250] loss=2.3903 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3643 tok/s=1,360,184 raw_tok/s=1,362,617 step_tokens=130,838 waste=0.2% h100_mfu=33.15% vram=397/17,340MB peak=16,747/17,340MB
[step 782/8250] loss=2.4067 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3572 tok/s=1,433,987 raw_tok/s=1,436,496 step_tokens=130,843 waste=0.2% h100_mfu=34.95% vram=397/17,340MB peak=16,747/17,340MB
[step 783/8250] loss=2.4642 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.3846 tok/s=1,403,768 raw_tok/s=1,406,731 step_tokens=130,796 waste=0.2% h100_mfu=34.23% vram=397/17,340MB peak=16,747/17,340MB
[step 784/8250] loss=2.3164 lr=9.46e-05 muon_lr=9.46e-04 grad_norm=0.4108 tok/s=1,366,402 raw_tok/s=1,369,579 step_tokens=130,768 waste=0.2% h100_mfu=33.32% vram=397/17,340MB peak=16,747/17,340MB
[step 785/8250] loss=2.4410 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3797 tok/s=1,431,096 raw_tok/s=1,432,790 step_tokens=130,917 waste=0.1% h100_mfu=34.86% vram=397/17,340MB peak=16,747/17,340MB
[step 786/8250] loss=2.3549 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.2960 tok/s=1,394,658 raw_tok/s=1,397,163 step_tokens=130,837 waste=0.2% h100_mfu=33.99% vram=397/17,340MB peak=16,747/17,340MB
[step 787/8250] loss=2.3680 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3526 tok/s=1,434,651 raw_tok/s=1,437,931 step_tokens=130,773 waste=0.2% h100_mfu=34.98% vram=397/17,340MB peak=16,747/17,340MB
[step 788/8250] loss=2.3778 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3672 tok/s=1,270,913 raw_tok/s=1,276,767 step_tokens=130,471 waste=0.5% h100_mfu=31.06% vram=397/17,340MB peak=16,747/17,340MB
[step 789/8250] loss=2.4069 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3869 tok/s=1,508,572 raw_tok/s=1,513,839 step_tokens=130,616 waste=0.3% h100_mfu=36.83% vram=397/17,340MB peak=16,747/17,340MB
[step 790/8250] loss=2.3768 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3113 tok/s=1,415,056 raw_tok/s=1,417,761 step_tokens=130,822 waste=0.2% h100_mfu=34.49% vram=397/17,340MB peak=16,747/17,340MB
[step 791/8250] loss=2.3688 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3304 tok/s=1,324,305 raw_tok/s=1,326,735 step_tokens=130,832 waste=0.2% h100_mfu=32.28% vram=397/17,340MB peak=16,747/17,340MB
[step 792/8250] loss=2.4642 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3109 tok/s=1,428,001 raw_tok/s=1,430,304 step_tokens=130,861 waste=0.2% h100_mfu=34.80% vram=397/17,340MB peak=16,747/17,340MB
[step 793/8250] loss=2.4051 lr=9.45e-05 muon_lr=9.45e-04 grad_norm=0.3297 tok/s=1,441,771 raw_tok/s=1,448,379 step_tokens=130,474 waste=0.5% h100_mfu=35.24% vram=397/17,340MB peak=16,747/17,340MB
[step 794/8250] loss=2.3466 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.4053 tok/s=1,377,332 raw_tok/s=1,378,331 step_tokens=130,977 waste=0.1% h100_mfu=33.53% vram=397/17,340MB peak=16,747/17,340MB
[step 795/8250] loss=2.3958 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.3814 tok/s=1,467,973 raw_tok/s=1,472,873 step_tokens=130,636 waste=0.3% h100_mfu=35.84% vram=397/17,340MB peak=16,747/17,340MB
[step 796/8250] loss=2.3881 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.4002 tok/s=1,443,999 raw_tok/s=1,447,201 step_tokens=130,782 waste=0.2% h100_mfu=35.21% vram=397/17,340MB peak=16,747/17,340MB
[step 797/8250] loss=2.4304 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.3491 tok/s=1,363,941 raw_tok/s=1,367,593 step_tokens=130,722 waste=0.3% h100_mfu=33.27% vram=397/17,340MB peak=16,747/17,340MB
[step 798/8250] loss=2.3863 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.3211 tok/s=1,464,808 raw_tok/s=1,470,992 step_tokens=130,521 waste=0.4% h100_mfu=35.79% vram=397/17,340MB peak=16,747/17,340MB
[step 799/8250] loss=2.3919 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.3600 tok/s=1,355,920 raw_tok/s=1,357,235 step_tokens=130,945 waste=0.1% h100_mfu=33.02% vram=397/17,340MB peak=16,747/17,340MB
[step 800/8250] loss=2.4090 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.3505 tok/s=1,364,012 raw_tok/s=1,367,005 step_tokens=130,785 waste=0.2% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 801/8250] loss=2.4101 lr=9.44e-05 muon_lr=9.44e-04 grad_norm=0.3472 tok/s=1,509,704 raw_tok/s=1,514,580 step_tokens=130,650 waste=0.3% h100_mfu=36.85% vram=397/17,340MB peak=16,747/17,340MB
[step 802/8250] loss=2.4054 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3252 tok/s=1,390,378 raw_tok/s=1,396,793 step_tokens=130,470 waste=0.5% h100_mfu=33.98% vram=397/17,340MB peak=16,747/17,340MB
[step 803/8250] loss=2.4375 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3558 tok/s=1,400,429 raw_tok/s=1,401,124 step_tokens=131,007 waste=0.0% h100_mfu=34.09% vram=397/17,340MB peak=16,747/17,340MB
[step 804/8250] loss=2.3902 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3985 tok/s=1,237,968 raw_tok/s=1,244,271 step_tokens=130,408 waste=0.5% h100_mfu=30.27% vram=397/17,340MB peak=16,747/17,340MB
[step 805/8250] loss=2.4206 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3272 tok/s=1,471,060 raw_tok/s=1,478,425 step_tokens=130,419 waste=0.5% h100_mfu=35.97% vram=397/17,340MB peak=16,747/17,340MB
[step 806/8250] loss=2.4267 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3741 tok/s=1,456,821 raw_tok/s=1,459,471 step_tokens=130,834 waste=0.2% h100_mfu=35.51% vram=397/17,340MB peak=16,747/17,340MB
[step 807/8250] loss=2.4221 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.4245 tok/s=1,258,285 raw_tok/s=1,258,967 step_tokens=131,001 waste=0.1% h100_mfu=30.63% vram=397/17,340MB peak=16,747/17,340MB
[step 808/8250] loss=2.3355 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3866 tok/s=1,421,205 raw_tok/s=1,425,981 step_tokens=130,633 waste=0.3% h100_mfu=34.69% vram=397/17,340MB peak=16,747/17,340MB
[step 809/8250] loss=2.4185 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3476 tok/s=1,355,778 raw_tok/s=1,358,711 step_tokens=130,789 waste=0.2% h100_mfu=33.06% vram=397/17,340MB peak=16,747/17,340MB
[step 810/8250] loss=2.3770 lr=9.43e-05 muon_lr=9.43e-04 grad_norm=0.3228 tok/s=1,384,703 raw_tok/s=1,387,402 step_tokens=130,817 waste=0.2% h100_mfu=33.76% vram=397/17,340MB peak=16,747/17,340MB
[step 811/8250] loss=2.4025 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3562 tok/s=1,432,764 raw_tok/s=1,434,143 step_tokens=130,946 waste=0.1% h100_mfu=34.89% vram=397/17,340MB peak=16,747/17,340MB
[step 812/8250] loss=2.4119 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3200 tok/s=1,411,008 raw_tok/s=1,411,137 step_tokens=131,060 waste=0.0% h100_mfu=34.33% vram=397/17,340MB peak=16,747/17,340MB
[step 813/8250] loss=2.3803 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3453 tok/s=1,411,690 raw_tok/s=1,414,929 step_tokens=130,772 waste=0.2% h100_mfu=34.43% vram=397/17,340MB peak=16,747/17,340MB
[step 814/8250] loss=2.4350 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.4019 tok/s=1,355,706 raw_tok/s=1,360,689 step_tokens=130,592 waste=0.4% h100_mfu=33.11% vram=397/17,340MB peak=16,747/17,340MB
[step 815/8250] loss=2.3994 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3870 tok/s=1,243,846 raw_tok/s=1,246,795 step_tokens=130,762 waste=0.2% h100_mfu=30.33% vram=397/17,340MB peak=16,747/17,340MB
[step 816/8250] loss=2.3611 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3924 tok/s=1,331,667 raw_tok/s=1,334,508 step_tokens=130,793 waste=0.2% h100_mfu=32.47% vram=397/17,340MB peak=16,747/17,340MB
[step 817/8250] loss=2.3924 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3244 tok/s=1,418,075 raw_tok/s=1,419,331 step_tokens=130,956 waste=0.1% h100_mfu=34.53% vram=397/17,340MB peak=16,747/17,340MB
[step 818/8250] loss=2.4137 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3761 tok/s=1,491,056 raw_tok/s=1,495,289 step_tokens=130,701 waste=0.3% h100_mfu=36.38% vram=397/17,340MB peak=16,747/17,340MB
[step 819/8250] loss=2.3827 lr=9.42e-05 muon_lr=9.42e-04 grad_norm=0.3829 tok/s=1,425,940 raw_tok/s=1,430,918 step_tokens=130,616 waste=0.3% h100_mfu=34.81% vram=397/17,340MB peak=16,747/17,340MB
[step 820/8250] loss=2.3948 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.4082 tok/s=1,422,710 raw_tok/s=1,428,169 step_tokens=130,571 waste=0.4% h100_mfu=34.75% vram=397/17,340MB peak=16,747/17,340MB
[step 821/8250] loss=2.3775 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.4056 tok/s=1,429,014 raw_tok/s=1,432,884 step_tokens=130,718 waste=0.3% h100_mfu=34.86% vram=397/17,340MB peak=16,747/17,340MB
[step 822/8250] loss=2.3444 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.4289 tok/s=1,386,278 raw_tok/s=1,389,436 step_tokens=130,774 waste=0.2% h100_mfu=33.81% vram=397/17,340MB peak=16,747/17,340MB
[step 823/8250] loss=2.3549 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.3052 tok/s=1,357,723 raw_tok/s=1,360,994 step_tokens=130,757 waste=0.2% h100_mfu=33.11% vram=397/17,340MB peak=16,747/17,340MB
[step 824/8250] loss=2.3903 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.3645 tok/s=1,279,486 raw_tok/s=1,279,915 step_tokens=131,028 waste=0.0% h100_mfu=31.14% vram=397/17,340MB peak=16,747/17,340MB
[step 825/8250] loss=2.3584 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.4272 tok/s=1,101,385 raw_tok/s=1,102,614 step_tokens=130,926 waste=0.1% h100_mfu=26.83% vram=397/17,340MB peak=16,747/17,340MB
[step 826/8250] loss=2.4274 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.3264 tok/s=1,307,924 raw_tok/s=1,309,063 step_tokens=130,958 waste=0.1% h100_mfu=31.85% vram=397/17,340MB peak=16,747/17,340MB
[step 827/8250] loss=2.3729 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.3755 tok/s=1,247,369 raw_tok/s=1,248,455 step_tokens=130,958 waste=0.1% h100_mfu=30.38% vram=397/17,340MB peak=16,747/17,340MB
[step 828/8250] loss=2.4081 lr=9.41e-05 muon_lr=9.41e-04 grad_norm=0.3326 tok/s=1,274,928 raw_tok/s=1,276,126 step_tokens=130,949 waste=0.1% h100_mfu=31.05% vram=397/17,340MB peak=16,747/17,340MB
[step 829/8250] loss=2.4026 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3627 tok/s=1,374,685 raw_tok/s=1,375,377 step_tokens=131,006 waste=0.1% h100_mfu=33.46% vram=397/17,340MB peak=16,747/17,340MB
[step 830/8250] loss=2.3483 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3431 tok/s=1,516,987 raw_tok/s=1,518,505 step_tokens=130,941 waste=0.1% h100_mfu=36.95% vram=397/17,340MB peak=16,747/17,340MB
[step 831/8250] loss=2.3856 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3409 tok/s=1,304,278 raw_tok/s=1,309,743 step_tokens=130,525 waste=0.4% h100_mfu=31.87% vram=397/17,340MB peak=16,747/17,340MB
[step 832/8250] loss=2.3729 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3304 tok/s=1,438,719 raw_tok/s=1,440,213 step_tokens=130,936 waste=0.1% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 833/8250] loss=2.4013 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3442 tok/s=1,421,234 raw_tok/s=1,424,168 step_tokens=130,802 waste=0.2% h100_mfu=34.65% vram=397/17,340MB peak=16,747/17,340MB
[step 834/8250] loss=2.3801 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3654 tok/s=1,387,217 raw_tok/s=1,389,592 step_tokens=130,848 waste=0.2% h100_mfu=33.81% vram=397/17,340MB peak=16,747/17,340MB
[step 835/8250] loss=2.3823 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3358 tok/s=1,342,722 raw_tok/s=1,346,049 step_tokens=130,748 waste=0.2% h100_mfu=32.75% vram=397/17,340MB peak=16,747/17,340MB
[step 836/8250] loss=2.3674 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3487 tok/s=1,442,256 raw_tok/s=1,447,579 step_tokens=130,590 waste=0.4% h100_mfu=35.22% vram=397/17,340MB peak=16,747/17,340MB
[step 837/8250] loss=2.3946 lr=9.40e-05 muon_lr=9.40e-04 grad_norm=0.3504 tok/s=1,338,670 raw_tok/s=1,342,090 step_tokens=130,738 waste=0.3% h100_mfu=32.65% vram=397/17,340MB peak=16,747/17,340MB
[step 838/8250] loss=2.3653 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.4082 tok/s=1,448,698 raw_tok/s=1,450,823 step_tokens=130,880 waste=0.1% h100_mfu=35.30% vram=397/17,340MB peak=16,747/17,340MB
[step 839/8250] loss=2.4024 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.4309 tok/s=1,451,896 raw_tok/s=1,454,493 step_tokens=130,838 waste=0.2% h100_mfu=35.39% vram=397/17,340MB peak=16,747/17,340MB
[step 840/8250] loss=2.3447 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3743 tok/s=1,370,023 raw_tok/s=1,374,564 step_tokens=130,639 waste=0.3% h100_mfu=33.44% vram=397/17,340MB peak=16,747/17,340MB
[step 841/8250] loss=2.4001 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3495 tok/s=1,378,342 raw_tok/s=1,383,133 step_tokens=130,618 waste=0.3% h100_mfu=33.65% vram=397/17,340MB peak=16,747/17,340MB
[step 842/8250] loss=2.4247 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3758 tok/s=1,445,254 raw_tok/s=1,449,168 step_tokens=130,718 waste=0.3% h100_mfu=35.26% vram=397/17,340MB peak=16,747/17,340MB
[step 843/8250] loss=2.4072 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3818 tok/s=1,294,913 raw_tok/s=1,296,605 step_tokens=130,901 waste=0.1% h100_mfu=31.55% vram=397/17,340MB peak=16,747/17,340MB
[step 844/8250] loss=2.3793 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3556 tok/s=1,403,877 raw_tok/s=1,406,323 step_tokens=130,844 waste=0.2% h100_mfu=34.22% vram=397/17,340MB peak=16,747/17,340MB
[step 845/8250] loss=2.3570 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3173 tok/s=1,483,197 raw_tok/s=1,488,216 step_tokens=130,630 waste=0.3% h100_mfu=36.21% vram=397/17,340MB peak=16,747/17,340MB
[step 846/8250] loss=2.3695 lr=9.39e-05 muon_lr=9.39e-04 grad_norm=0.3389 tok/s=1,440,258 raw_tok/s=1,446,016 step_tokens=130,550 waste=0.4% h100_mfu=35.18% vram=397/17,340MB peak=16,747/17,340MB
[step 847/8250] loss=2.3742 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3526 tok/s=1,495,632 raw_tok/s=1,500,957 step_tokens=130,607 waste=0.4% h100_mfu=36.52% vram=397/17,340MB peak=16,747/17,340MB
[step 848/8250] loss=2.3475 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3519 tok/s=1,450,630 raw_tok/s=1,454,136 step_tokens=130,756 waste=0.2% h100_mfu=35.38% vram=397/17,340MB peak=16,747/17,340MB
[step 849/8250] loss=2.3522 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3444 tok/s=1,418,629 raw_tok/s=1,420,851 step_tokens=130,867 waste=0.2% h100_mfu=34.57% vram=397/17,340MB peak=16,747/17,340MB
[step 850/8250] loss=2.3687 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3558 tok/s=1,497,023 raw_tok/s=1,500,148 step_tokens=130,799 waste=0.2% h100_mfu=36.50% vram=397/17,340MB peak=16,747/17,340MB
[step 851/8250] loss=2.3619 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3318 tok/s=1,439,873 raw_tok/s=1,444,590 step_tokens=130,644 waste=0.3% h100_mfu=35.15% vram=397/17,340MB peak=16,747/17,340MB
[step 852/8250] loss=2.3747 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3721 tok/s=1,452,953 raw_tok/s=1,453,629 step_tokens=131,011 waste=0.0% h100_mfu=35.37% vram=397/17,340MB peak=16,747/17,340MB
[step 853/8250] loss=2.3477 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.3987 tok/s=1,448,791 raw_tok/s=1,452,304 step_tokens=130,755 waste=0.2% h100_mfu=35.33% vram=397/17,340MB peak=16,747/17,340MB
[step 854/8250] loss=2.3573 lr=9.38e-05 muon_lr=9.38e-04 grad_norm=0.4255 tok/s=1,507,928 raw_tok/s=1,511,491 step_tokens=130,763 waste=0.2% h100_mfu=36.77% vram=397/17,340MB peak=16,747/17,340MB
[step 855/8250] loss=2.3773 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.4325 tok/s=1,354,802 raw_tok/s=1,354,977 step_tokens=131,055 waste=0.0% h100_mfu=32.97% vram=397/17,340MB peak=16,747/17,340MB
[step 856/8250] loss=2.3702 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.4026 tok/s=1,376,493 raw_tok/s=1,382,505 step_tokens=130,502 waste=0.4% h100_mfu=33.64% vram=397/17,340MB peak=16,747/17,340MB
[step 857/8250] loss=2.4293 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3571 tok/s=1,360,579 raw_tok/s=1,365,454 step_tokens=130,604 waste=0.4% h100_mfu=33.22% vram=397/17,340MB peak=16,747/17,340MB
[step 858/8250] loss=2.3802 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3726 tok/s=1,378,594 raw_tok/s=1,382,137 step_tokens=130,736 waste=0.3% h100_mfu=33.63% vram=397/17,340MB peak=16,747/17,340MB
[step 859/8250] loss=2.3406 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3578 tok/s=1,372,211 raw_tok/s=1,376,117 step_tokens=130,700 waste=0.3% h100_mfu=33.48% vram=397/17,340MB peak=16,747/17,340MB
[step 860/8250] loss=2.3831 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3510 tok/s=1,422,648 raw_tok/s=1,425,302 step_tokens=130,828 waste=0.2% h100_mfu=34.68% vram=397/17,340MB peak=16,747/17,340MB
[step 861/8250] loss=2.3538 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3778 tok/s=1,487,855 raw_tok/s=1,493,369 step_tokens=130,588 waste=0.4% h100_mfu=36.33% vram=397/17,340MB peak=16,747/17,340MB
[step 862/8250] loss=2.3456 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3342 tok/s=1,367,368 raw_tok/s=1,373,803 step_tokens=130,458 waste=0.5% h100_mfu=33.42% vram=397/17,340MB peak=16,747/17,340MB
[step 863/8250] loss=2.3956 lr=9.37e-05 muon_lr=9.37e-04 grad_norm=0.3927 tok/s=1,391,600 raw_tok/s=1,393,174 step_tokens=130,924 waste=0.1% h100_mfu=33.90% vram=397/17,340MB peak=16,747/17,340MB
[step 864/8250] loss=2.4066 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.3846 tok/s=1,441,914 raw_tok/s=1,446,139 step_tokens=130,689 waste=0.3% h100_mfu=35.18% vram=397/17,340MB peak=16,747/17,340MB
[step 865/8250] loss=2.4129 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.4202 tok/s=1,365,874 raw_tok/s=1,367,523 step_tokens=130,914 waste=0.1% h100_mfu=33.27% vram=397/17,340MB peak=16,747/17,340MB
[step 866/8250] loss=2.3924 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.4214 tok/s=1,468,570 raw_tok/s=1,471,140 step_tokens=130,843 waste=0.2% h100_mfu=35.79% vram=397/17,340MB peak=16,747/17,340MB
[step 867/8250] loss=2.3807 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.4096 tok/s=1,416,198 raw_tok/s=1,422,100 step_tokens=130,528 waste=0.4% h100_mfu=34.60% vram=397/17,340MB peak=16,747/17,340MB
[step 868/8250] loss=2.3389 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.3675 tok/s=1,409,917 raw_tok/s=1,413,778 step_tokens=130,714 waste=0.3% h100_mfu=34.40% vram=397/17,340MB peak=16,747/17,340MB
[step 869/8250] loss=2.3354 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.3950 tok/s=1,447,692 raw_tok/s=1,447,769 step_tokens=131,065 waste=0.0% h100_mfu=35.22% vram=397/17,340MB peak=16,747/17,340MB
[step 870/8250] loss=2.3443 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.4238 tok/s=1,442,890 raw_tok/s=1,446,809 step_tokens=130,717 waste=0.3% h100_mfu=35.20% vram=397/17,340MB peak=16,747/17,340MB
[step 871/8250] loss=2.3619 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.3850 tok/s=1,448,908 raw_tok/s=1,449,593 step_tokens=131,010 waste=0.0% h100_mfu=35.27% vram=397/17,340MB peak=16,747/17,340MB
[step 872/8250] loss=2.3691 lr=9.36e-05 muon_lr=9.36e-04 grad_norm=0.3516 tok/s=1,446,472 raw_tok/s=1,450,390 step_tokens=130,718 waste=0.3% h100_mfu=35.29% vram=397/17,340MB peak=16,747/17,340MB
[step 873/8250] loss=2.4480 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3556 tok/s=1,417,547 raw_tok/s=1,423,346 step_tokens=130,538 waste=0.4% h100_mfu=34.63% vram=397/17,340MB peak=16,747/17,340MB
[step 874/8250] loss=2.3854 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3778 tok/s=1,412,556 raw_tok/s=1,417,412 step_tokens=130,623 waste=0.3% h100_mfu=34.49% vram=397/17,340MB peak=16,747/17,340MB
[step 875/8250] loss=2.3404 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3907 tok/s=1,400,148 raw_tok/s=1,402,373 step_tokens=130,864 waste=0.2% h100_mfu=34.12% vram=397/17,340MB peak=16,747/17,340MB
[step 876/8250] loss=2.3455 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3666 tok/s=1,427,007 raw_tok/s=1,428,697 step_tokens=130,917 waste=0.1% h100_mfu=34.76% vram=397/17,340MB peak=16,747/17,340MB
[step 877/8250] loss=2.4029 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3773 tok/s=1,434,858 raw_tok/s=1,437,073 step_tokens=130,870 waste=0.2% h100_mfu=34.96% vram=397/17,340MB peak=16,747/17,340MB
[step 878/8250] loss=2.3471 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3614 tok/s=1,450,954 raw_tok/s=1,452,672 step_tokens=130,917 waste=0.1% h100_mfu=35.34% vram=397/17,340MB peak=16,747/17,340MB
[step 879/8250] loss=2.3871 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3482 tok/s=1,449,793 raw_tok/s=1,452,852 step_tokens=130,796 waste=0.2% h100_mfu=35.35% vram=397/17,340MB peak=16,747/17,340MB
[step 880/8250] loss=2.3463 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3975 tok/s=1,515,869 raw_tok/s=1,518,499 step_tokens=130,845 waste=0.2% h100_mfu=36.95% vram=397/17,340MB peak=16,747/17,340MB
[step 881/8250] loss=2.3712 lr=9.35e-05 muon_lr=9.35e-04 grad_norm=0.3900 tok/s=1,346,060 raw_tok/s=1,350,439 step_tokens=130,647 waste=0.3% h100_mfu=32.86% vram=397/17,340MB peak=16,747/17,340MB
[step 882/8250] loss=2.4315 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3375 tok/s=1,486,675 raw_tok/s=1,490,336 step_tokens=130,750 waste=0.2% h100_mfu=36.26% vram=397/17,340MB peak=16,747/17,340MB
[step 883/8250] loss=2.3548 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3474 tok/s=1,313,062 raw_tok/s=1,315,239 step_tokens=130,855 waste=0.2% h100_mfu=32.00% vram=397/17,340MB peak=16,747/17,340MB
[step 884/8250] loss=2.3742 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3783 tok/s=1,434,288 raw_tok/s=1,438,679 step_tokens=130,672 waste=0.3% h100_mfu=35.00% vram=397/17,340MB peak=16,747/17,340MB
[step 885/8250] loss=2.3393 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3677 tok/s=1,385,945 raw_tok/s=1,386,537 step_tokens=131,016 waste=0.0% h100_mfu=33.73% vram=397/17,340MB peak=16,747/17,340MB
[step 886/8250] loss=2.3499 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3921 tok/s=1,432,992 raw_tok/s=1,435,741 step_tokens=130,821 waste=0.2% h100_mfu=34.93% vram=397/17,340MB peak=16,747/17,340MB
[step 887/8250] loss=2.3351 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3830 tok/s=1,390,725 raw_tok/s=1,395,207 step_tokens=130,651 waste=0.3% h100_mfu=33.95% vram=397/17,340MB peak=16,747/17,340MB
[step 888/8250] loss=2.4235 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3384 tok/s=1,434,509 raw_tok/s=1,436,713 step_tokens=130,871 waste=0.2% h100_mfu=34.96% vram=397/17,340MB peak=16,747/17,340MB
[step 889/8250] loss=2.3916 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3544 tok/s=1,278,970 raw_tok/s=1,280,582 step_tokens=130,907 waste=0.1% h100_mfu=31.16% vram=397/17,340MB peak=16,747/17,340MB
[step 890/8250] loss=2.3701 lr=9.34e-05 muon_lr=9.34e-04 grad_norm=0.3605 tok/s=1,266,946 raw_tok/s=1,271,360 step_tokens=130,617 waste=0.3% h100_mfu=30.93% vram=397/17,340MB peak=16,747/17,340MB
[step 891/8250] loss=2.3782 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.5034 tok/s=1,350,994 raw_tok/s=1,353,597 step_tokens=130,820 waste=0.2% h100_mfu=32.93% vram=397/17,340MB peak=16,747/17,340MB
[step 892/8250] loss=2.3646 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.4258 tok/s=1,332,253 raw_tok/s=1,334,983 step_tokens=130,804 waste=0.2% h100_mfu=32.48% vram=397/17,340MB peak=16,747/17,340MB
[step 893/8250] loss=2.3491 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.3770 tok/s=1,434,187 raw_tok/s=1,439,943 step_tokens=130,548 waste=0.4% h100_mfu=35.03% vram=397/17,340MB peak=16,747/17,340MB
[step 894/8250] loss=2.3274 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.3662 tok/s=1,259,795 raw_tok/s=1,264,483 step_tokens=130,586 waste=0.4% h100_mfu=30.76% vram=397/17,340MB peak=16,747/17,340MB
[step 895/8250] loss=2.2960 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.3067 tok/s=1,455,945 raw_tok/s=1,456,356 step_tokens=131,035 waste=0.0% h100_mfu=35.43% vram=397/17,340MB peak=16,747/17,340MB
[step 896/8250] loss=2.3704 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.3670 tok/s=1,435,245 raw_tok/s=1,439,286 step_tokens=130,704 waste=0.3% h100_mfu=35.02% vram=397/17,340MB peak=16,747/17,340MB
[step 897/8250] loss=2.4063 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.3497 tok/s=1,439,637 raw_tok/s=1,445,659 step_tokens=130,526 waste=0.4% h100_mfu=35.17% vram=397/17,340MB peak=16,747/17,340MB
[step 898/8250] loss=2.3239 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.4100 tok/s=1,461,131 raw_tok/s=1,465,446 step_tokens=130,686 waste=0.3% h100_mfu=35.65% vram=397/17,340MB peak=16,747/17,340MB
[step 899/8250] loss=2.3325 lr=9.33e-05 muon_lr=9.33e-04 grad_norm=0.4127 tok/s=1,424,535 raw_tok/s=1,426,853 step_tokens=130,859 waste=0.2% h100_mfu=34.72% vram=397/17,340MB peak=16,747/17,340MB
[step 900/8250] loss=2.3587 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.3849 tok/s=1,323,768 raw_tok/s=1,331,530 step_tokens=130,308 waste=0.6% h100_mfu=32.40% vram=397/17,340MB peak=16,747/17,340MB
[step 901/8250] loss=2.3763 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.3700 tok/s=1,464,675 raw_tok/s=1,468,473 step_tokens=130,733 waste=0.3% h100_mfu=35.73% vram=397/17,340MB peak=16,747/17,340MB
[step 902/8250] loss=2.3685 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.3501 tok/s=1,300,239 raw_tok/s=1,304,349 step_tokens=130,659 waste=0.3% h100_mfu=31.73% vram=397/17,340MB peak=16,747/17,340MB
[step 903/8250] loss=2.3417 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.4334 tok/s=1,208,351 raw_tok/s=1,211,531 step_tokens=130,728 waste=0.3% h100_mfu=29.48% vram=397/17,340MB peak=16,747/17,340MB
[step 904/8250] loss=2.2761 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.4252 tok/s=1,422,310 raw_tok/s=1,427,833 step_tokens=130,565 waste=0.4% h100_mfu=34.74% vram=397/17,340MB peak=16,747/17,340MB
[step 905/8250] loss=2.2986 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.4232 tok/s=1,433,793 raw_tok/s=1,439,802 step_tokens=130,525 waste=0.4% h100_mfu=35.03% vram=397/17,340MB peak=16,747/17,340MB
[step 906/8250] loss=2.3687 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.4010 tok/s=1,335,997 raw_tok/s=1,341,822 step_tokens=130,503 waste=0.4% h100_mfu=32.65% vram=397/17,340MB peak=16,747/17,340MB
[step 907/8250] loss=2.3027 lr=9.32e-05 muon_lr=9.32e-04 grad_norm=0.4001 tok/s=1,364,224 raw_tok/s=1,366,894 step_tokens=130,816 waste=0.2% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 908/8250] loss=2.3174 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3692 tok/s=1,492,288 raw_tok/s=1,496,821 step_tokens=130,675 waste=0.3% h100_mfu=36.42% vram=397/17,340MB peak=16,747/17,340MB
[step 909/8250] loss=2.3462 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3655 tok/s=1,285,080 raw_tok/s=1,291,921 step_tokens=130,378 waste=0.5% h100_mfu=31.43% vram=397/17,340MB peak=16,747/17,340MB
[step 910/8250] loss=2.3243 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3572 tok/s=1,418,408 raw_tok/s=1,421,108 step_tokens=130,823 waste=0.2% h100_mfu=34.58% vram=397/17,340MB peak=16,747/17,340MB
[step 911/8250] loss=2.3262 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3705 tok/s=1,479,488 raw_tok/s=1,485,688 step_tokens=130,525 waste=0.4% h100_mfu=36.15% vram=397/17,340MB peak=16,747/17,340MB
[step 912/8250] loss=2.3286 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3563 tok/s=1,310,417 raw_tok/s=1,312,992 step_tokens=130,815 waste=0.2% h100_mfu=31.95% vram=397/17,340MB peak=16,747/17,340MB
[step 913/8250] loss=2.3018 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3338 tok/s=1,396,330 raw_tok/s=1,401,624 step_tokens=130,577 waste=0.4% h100_mfu=34.10% vram=397/17,340MB peak=16,747/17,340MB
[step 914/8250] loss=2.3525 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3427 tok/s=1,315,745 raw_tok/s=1,320,005 step_tokens=130,649 waste=0.3% h100_mfu=32.12% vram=397/17,340MB peak=16,747/17,340MB
[step 915/8250] loss=2.3906 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3164 tok/s=1,403,011 raw_tok/s=1,406,649 step_tokens=130,733 waste=0.3% h100_mfu=34.22% vram=397/17,340MB peak=16,747/17,340MB
[step 916/8250] loss=2.2985 lr=9.31e-05 muon_lr=9.31e-04 grad_norm=0.3495 tok/s=1,358,651 raw_tok/s=1,364,804 step_tokens=130,481 waste=0.5% h100_mfu=33.21% vram=397/17,340MB peak=16,747/17,340MB
[step 917/8250] loss=2.3299 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3907 tok/s=1,412,024 raw_tok/s=1,412,552 step_tokens=131,023 waste=0.0% h100_mfu=34.37% vram=397/17,340MB peak=16,747/17,340MB
[step 918/8250] loss=2.4028 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.4640 tok/s=1,440,372 raw_tok/s=1,442,397 step_tokens=130,888 waste=0.1% h100_mfu=35.09% vram=397/17,340MB peak=16,747/17,340MB
[step 919/8250] loss=2.3446 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3931 tok/s=1,308,095 raw_tok/s=1,308,914 step_tokens=130,990 waste=0.1% h100_mfu=31.85% vram=397/17,340MB peak=16,747/17,340MB
[step 920/8250] loss=2.3528 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3443 tok/s=1,354,758 raw_tok/s=1,358,916 step_tokens=130,671 waste=0.3% h100_mfu=33.06% vram=397/17,340MB peak=16,747/17,340MB
[step 921/8250] loss=2.3494 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.4579 tok/s=1,378,923 raw_tok/s=1,381,675 step_tokens=130,811 waste=0.2% h100_mfu=33.62% vram=397/17,340MB peak=16,747/17,340MB
[step 922/8250] loss=2.3836 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3888 tok/s=1,411,904 raw_tok/s=1,414,105 step_tokens=130,868 waste=0.2% h100_mfu=34.41% vram=397/17,340MB peak=16,747/17,340MB
[step 923/8250] loss=2.3381 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3654 tok/s=1,468,222 raw_tok/s=1,469,265 step_tokens=130,979 waste=0.1% h100_mfu=35.75% vram=397/17,340MB peak=16,747/17,340MB
[step 924/8250] loss=2.2573 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3570 tok/s=1,357,077 raw_tok/s=1,359,380 step_tokens=130,850 waste=0.2% h100_mfu=33.07% vram=397/17,340MB peak=16,747/17,340MB
[step 925/8250] loss=2.3876 lr=9.30e-05 muon_lr=9.30e-04 grad_norm=0.3723 tok/s=1,434,768 raw_tok/s=1,438,785 step_tokens=130,706 waste=0.3% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 926/8250] loss=2.2967 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3560 tok/s=1,387,052 raw_tok/s=1,389,787 step_tokens=130,814 waste=0.2% h100_mfu=33.81% vram=397/17,340MB peak=16,747/17,340MB
[step 927/8250] loss=2.3543 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3727 tok/s=1,421,183 raw_tok/s=1,424,792 step_tokens=130,740 waste=0.3% h100_mfu=34.67% vram=397/17,340MB peak=16,747/17,340MB
[step 928/8250] loss=2.3402 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.4422 tok/s=1,407,379 raw_tok/s=1,409,907 step_tokens=130,837 waste=0.2% h100_mfu=34.30% vram=397/17,340MB peak=16,747/17,340MB
[step 929/8250] loss=2.2930 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3881 tok/s=1,423,219 raw_tok/s=1,425,002 step_tokens=130,908 waste=0.1% h100_mfu=34.67% vram=397/17,340MB peak=16,747/17,340MB
[step 930/8250] loss=2.3535 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3671 tok/s=1,443,893 raw_tok/s=1,445,890 step_tokens=130,891 waste=0.1% h100_mfu=35.18% vram=397/17,340MB peak=16,747/17,340MB
[step 931/8250] loss=2.3060 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.4282 tok/s=1,474,779 raw_tok/s=1,477,405 step_tokens=130,839 waste=0.2% h100_mfu=35.95% vram=397/17,340MB peak=16,747/17,340MB
[step 932/8250] loss=2.3384 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3647 tok/s=1,369,953 raw_tok/s=1,374,168 step_tokens=130,670 waste=0.3% h100_mfu=33.43% vram=397/17,340MB peak=16,747/17,340MB
[step 933/8250] loss=2.2985 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3743 tok/s=1,407,304 raw_tok/s=1,411,882 step_tokens=130,647 waste=0.3% h100_mfu=34.35% vram=397/17,340MB peak=16,747/17,340MB
[step 934/8250] loss=2.3580 lr=9.29e-05 muon_lr=9.29e-04 grad_norm=0.3678 tok/s=1,484,272 raw_tok/s=1,487,689 step_tokens=130,771 waste=0.2% h100_mfu=36.20% vram=397/17,340MB peak=16,747/17,340MB
[step 935/8250] loss=2.3283 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3529 tok/s=1,426,893 raw_tok/s=1,429,019 step_tokens=130,877 waste=0.1% h100_mfu=34.77% vram=397/17,340MB peak=16,747/17,340MB
[step 936/8250] loss=2.3086 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3622 tok/s=1,385,495 raw_tok/s=1,390,502 step_tokens=130,600 waste=0.4% h100_mfu=33.83% vram=397/17,340MB peak=16,747/17,340MB
[step 937/8250] loss=2.3127 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3547 tok/s=1,292,671 raw_tok/s=1,295,736 step_tokens=130,762 waste=0.2% h100_mfu=31.53% vram=397/17,340MB peak=16,747/17,340MB
[step 938/8250] loss=2.3394 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3565 tok/s=1,408,171 raw_tok/s=1,409,903 step_tokens=130,911 waste=0.1% h100_mfu=34.30% vram=397/17,340MB peak=16,747/17,340MB
[step 939/8250] loss=2.3458 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3468 tok/s=1,437,771 raw_tok/s=1,438,935 step_tokens=130,966 waste=0.1% h100_mfu=35.01% vram=397/17,340MB peak=16,747/17,340MB
[step 940/8250] loss=2.3817 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3580 tok/s=1,375,352 raw_tok/s=1,377,211 step_tokens=130,895 waste=0.1% h100_mfu=33.51% vram=397/17,340MB peak=16,747/17,340MB
[step 941/8250] loss=2.3331 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.3735 tok/s=1,375,105 raw_tok/s=1,379,019 step_tokens=130,700 waste=0.3% h100_mfu=33.55% vram=397/17,340MB peak=16,747/17,340MB
[step 942/8250] loss=2.3425 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.4286 tok/s=1,380,011 raw_tok/s=1,383,801 step_tokens=130,713 waste=0.3% h100_mfu=33.67% vram=397/17,340MB peak=16,747/17,340MB
[step 943/8250] loss=2.3204 lr=9.28e-05 muon_lr=9.28e-04 grad_norm=0.4400 tok/s=1,443,539 raw_tok/s=1,444,961 step_tokens=130,943 waste=0.1% h100_mfu=35.16% vram=397/17,340MB peak=16,747/17,340MB
[step 944/8250] loss=2.3466 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.4148 tok/s=1,445,106 raw_tok/s=1,448,045 step_tokens=130,806 waste=0.2% h100_mfu=35.23% vram=397/17,340MB peak=16,747/17,340MB
[step 945/8250] loss=2.3494 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3855 tok/s=1,444,970 raw_tok/s=1,449,493 step_tokens=130,663 waste=0.3% h100_mfu=35.27% vram=397/17,340MB peak=16,747/17,340MB
[step 946/8250] loss=2.2845 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3936 tok/s=1,481,913 raw_tok/s=1,486,131 step_tokens=130,700 waste=0.3% h100_mfu=36.16% vram=397/17,340MB peak=16,747/17,340MB
[step 947/8250] loss=2.3673 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3649 tok/s=1,405,883 raw_tok/s=1,414,137 step_tokens=130,307 waste=0.6% h100_mfu=34.41% vram=397/17,340MB peak=16,747/17,340MB
[step 948/8250] loss=2.2528 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3305 tok/s=1,238,667 raw_tok/s=1,241,319 step_tokens=130,792 waste=0.2% h100_mfu=30.20% vram=397/17,340MB peak=16,747/17,340MB
[step 949/8250] loss=2.3910 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3200 tok/s=1,426,777 raw_tok/s=1,431,539 step_tokens=130,636 waste=0.3% h100_mfu=34.83% vram=397/17,340MB peak=16,747/17,340MB
[step 950/8250] loss=2.3100 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3761 tok/s=1,386,855 raw_tok/s=1,390,175 step_tokens=130,759 waste=0.2% h100_mfu=33.82% vram=397/17,340MB peak=16,747/17,340MB
[step 951/8250] loss=2.3708 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3788 tok/s=1,413,573 raw_tok/s=1,414,522 step_tokens=130,984 waste=0.1% h100_mfu=34.42% vram=397/17,340MB peak=16,747/17,340MB
[step 952/8250] loss=2.2927 lr=9.27e-05 muon_lr=9.27e-04 grad_norm=0.3876 tok/s=1,431,098 raw_tok/s=1,431,753 step_tokens=131,012 waste=0.0% h100_mfu=34.83% vram=397/17,340MB peak=16,747/17,340MB
[step 953/8250] loss=2.3339 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.4181 tok/s=1,431,520 raw_tok/s=1,437,210 step_tokens=130,553 waste=0.4% h100_mfu=34.97% vram=397/17,340MB peak=16,747/17,340MB
[step 954/8250] loss=2.3277 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3710 tok/s=1,389,774 raw_tok/s=1,393,644 step_tokens=130,708 waste=0.3% h100_mfu=33.91% vram=397/17,340MB peak=16,747/17,340MB
[step 955/8250] loss=2.3414 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3193 tok/s=1,386,511 raw_tok/s=1,387,453 step_tokens=130,983 waste=0.1% h100_mfu=33.76% vram=397/17,340MB peak=16,747/17,340MB
[step 956/8250] loss=2.3526 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3675 tok/s=1,403,968 raw_tok/s=1,407,135 step_tokens=130,777 waste=0.2% h100_mfu=34.24% vram=397/17,340MB peak=16,747/17,340MB
[step 957/8250] loss=2.3359 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3618 tok/s=1,376,339 raw_tok/s=1,379,211 step_tokens=130,799 waste=0.2% h100_mfu=33.56% vram=397/17,340MB peak=16,747/17,340MB
[step 958/8250] loss=2.3656 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3445 tok/s=1,501,385 raw_tok/s=1,505,555 step_tokens=130,709 waste=0.3% h100_mfu=36.63% vram=397/17,340MB peak=16,747/17,340MB
[step 959/8250] loss=2.2817 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3523 tok/s=1,391,031 raw_tok/s=1,393,476 step_tokens=130,842 waste=0.2% h100_mfu=33.90% vram=397/17,340MB peak=16,747/17,340MB
[step 960/8250] loss=2.3213 lr=9.26e-05 muon_lr=9.26e-04 grad_norm=0.3478 tok/s=1,298,032 raw_tok/s=1,303,701 step_tokens=130,502 waste=0.4% h100_mfu=31.72% vram=397/17,340MB peak=16,747/17,340MB
[step 961/8250] loss=2.3416 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.3796 tok/s=1,404,434 raw_tok/s=1,407,721 step_tokens=130,766 waste=0.2% h100_mfu=34.25% vram=397/17,340MB peak=16,747/17,340MB
[step 962/8250] loss=2.3059 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.4097 tok/s=1,404,874 raw_tok/s=1,407,226 step_tokens=130,853 waste=0.2% h100_mfu=34.24% vram=397/17,340MB peak=16,747/17,340MB
[step 963/8250] loss=2.3610 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.4245 tok/s=1,467,462 raw_tok/s=1,471,515 step_tokens=130,711 waste=0.3% h100_mfu=35.80% vram=397/17,340MB peak=16,747/17,340MB
[step 964/8250] loss=2.2849 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.4421 tok/s=1,441,834 raw_tok/s=1,447,023 step_tokens=130,602 waste=0.4% h100_mfu=35.21% vram=397/17,340MB peak=16,747/17,340MB
[step 965/8250] loss=2.4153 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.4264 tok/s=1,384,349 raw_tok/s=1,387,886 step_tokens=130,738 waste=0.3% h100_mfu=33.77% vram=397/17,340MB peak=16,747/17,340MB
[step 966/8250] loss=2.3072 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.4894 tok/s=1,378,039 raw_tok/s=1,381,750 step_tokens=130,720 waste=0.3% h100_mfu=33.62% vram=397/17,340MB peak=16,747/17,340MB
[step 967/8250] loss=2.3560 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.3961 tok/s=1,364,954 raw_tok/s=1,370,790 step_tokens=130,514 waste=0.4% h100_mfu=33.35% vram=397/17,340MB peak=16,747/17,340MB
[step 968/8250] loss=2.2867 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.4125 tok/s=1,341,560 raw_tok/s=1,345,142 step_tokens=130,723 waste=0.3% h100_mfu=32.73% vram=397/17,340MB peak=16,747/17,340MB
[step 969/8250] loss=2.3277 lr=9.25e-05 muon_lr=9.25e-04 grad_norm=0.3804 tok/s=1,410,507 raw_tok/s=1,412,274 step_tokens=130,908 waste=0.1% h100_mfu=34.36% vram=397/17,340MB peak=16,747/17,340MB
[step 970/8250] loss=2.3530 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.3601 tok/s=1,431,096 raw_tok/s=1,437,908 step_tokens=130,451 waste=0.5% h100_mfu=34.98% vram=397/17,340MB peak=16,747/17,340MB
[step 971/8250] loss=2.3271 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.4222 tok/s=1,451,957 raw_tok/s=1,453,288 step_tokens=130,952 waste=0.1% h100_mfu=35.36% vram=397/17,340MB peak=16,747/17,340MB
[step 972/8250] loss=2.3024 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.4278 tok/s=1,403,013 raw_tok/s=1,410,644 step_tokens=130,363 waste=0.5% h100_mfu=34.32% vram=397/17,340MB peak=16,747/17,340MB
[step 973/8250] loss=2.3150 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.3846 tok/s=1,396,067 raw_tok/s=1,402,681 step_tokens=130,454 waste=0.5% h100_mfu=34.13% vram=397/17,340MB peak=16,747/17,340MB
[step 974/8250] loss=2.2528 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.4058 tok/s=1,442,347 raw_tok/s=1,447,139 step_tokens=130,638 waste=0.3% h100_mfu=35.21% vram=397/17,340MB peak=16,747/17,340MB
[step 975/8250] loss=2.3307 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.3936 tok/s=1,436,995 raw_tok/s=1,439,686 step_tokens=130,827 waste=0.2% h100_mfu=35.03% vram=397/17,340MB peak=16,747/17,340MB
[step 976/8250] loss=2.2662 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.4065 tok/s=1,473,304 raw_tok/s=1,478,640 step_tokens=130,599 waste=0.4% h100_mfu=35.98% vram=397/17,340MB peak=16,747/17,340MB
[step 977/8250] loss=2.2788 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.3722 tok/s=1,429,723 raw_tok/s=1,432,171 step_tokens=130,848 waste=0.2% h100_mfu=34.84% vram=397/17,340MB peak=16,747/17,340MB
[step 978/8250] loss=2.3048 lr=9.24e-05 muon_lr=9.24e-04 grad_norm=0.3655 tok/s=1,393,362 raw_tok/s=1,395,865 step_tokens=130,837 waste=0.2% h100_mfu=33.96% vram=397/17,340MB peak=16,747/17,340MB
[step 979/8250] loss=2.2993 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.3799 tok/s=1,325,912 raw_tok/s=1,329,025 step_tokens=130,765 waste=0.2% h100_mfu=32.34% vram=397/17,340MB peak=16,747/17,340MB
[step 980/8250] loss=2.2447 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.3596 tok/s=1,359,260 raw_tok/s=1,365,259 step_tokens=130,496 waste=0.4% h100_mfu=33.22% vram=397/17,340MB peak=16,747/17,340MB
[step 981/8250] loss=2.3671 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.3834 tok/s=1,424,359 raw_tok/s=1,427,671 step_tokens=130,768 waste=0.2% h100_mfu=34.74% vram=397/17,340MB peak=16,747/17,340MB
[step 982/8250] loss=2.2780 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.4256 tok/s=1,410,590 raw_tok/s=1,415,547 step_tokens=130,613 waste=0.4% h100_mfu=34.44% vram=397/17,340MB peak=16,747/17,340MB
[step 983/8250] loss=2.2784 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.3711 tok/s=1,413,833 raw_tok/s=1,414,470 step_tokens=131,013 waste=0.0% h100_mfu=34.41% vram=397/17,340MB peak=16,747/17,340MB
[step 984/8250] loss=2.3168 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.4047 tok/s=1,373,388 raw_tok/s=1,377,455 step_tokens=130,685 waste=0.3% h100_mfu=33.51% vram=397/17,340MB peak=16,747/17,340MB
[step 985/8250] loss=2.3852 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.4300 tok/s=1,460,790 raw_tok/s=1,464,612 step_tokens=130,730 waste=0.3% h100_mfu=35.63% vram=397/17,340MB peak=16,747/17,340MB
[step 986/8250] loss=2.3483 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.3910 tok/s=1,349,088 raw_tok/s=1,355,719 step_tokens=130,431 waste=0.5% h100_mfu=32.98% vram=397/17,340MB peak=16,747/17,340MB
[step 987/8250] loss=2.2806 lr=9.23e-05 muon_lr=9.23e-04 grad_norm=0.4465 tok/s=1,290,796 raw_tok/s=1,292,285 step_tokens=130,921 waste=0.1% h100_mfu=31.44% vram=397/17,340MB peak=16,747/17,340MB
[step 988/8250] loss=2.3674 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.4137 tok/s=1,516,400 raw_tok/s=1,519,646 step_tokens=130,792 waste=0.2% h100_mfu=36.97% vram=397/17,340MB peak=16,747/17,340MB
[step 989/8250] loss=2.2830 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.4116 tok/s=1,363,225 raw_tok/s=1,368,089 step_tokens=130,606 waste=0.4% h100_mfu=33.29% vram=397/17,340MB peak=16,747/17,340MB
[step 990/8250] loss=2.2863 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.3841 tok/s=1,324,799 raw_tok/s=1,325,922 step_tokens=130,961 waste=0.1% h100_mfu=32.26% vram=397/17,340MB peak=16,747/17,340MB
[step 991/8250] loss=2.3536 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.3602 tok/s=1,337,403 raw_tok/s=1,338,209 step_tokens=130,993 waste=0.1% h100_mfu=32.56% vram=397/17,340MB peak=16,747/17,340MB
[step 992/8250] loss=2.3197 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.3485 tok/s=1,380,299 raw_tok/s=1,382,493 step_tokens=130,864 waste=0.2% h100_mfu=33.64% vram=397/17,340MB peak=16,747/17,340MB
[step 993/8250] loss=2.2807 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.4257 tok/s=1,331,956 raw_tok/s=1,336,094 step_tokens=130,666 waste=0.3% h100_mfu=32.51% vram=397/17,340MB peak=16,747/17,340MB
[step 994/8250] loss=2.3426 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.3720 tok/s=1,380,912 raw_tok/s=1,388,987 step_tokens=130,310 waste=0.6% h100_mfu=33.79% vram=397/17,340MB peak=16,747/17,340MB
[step 995/8250] loss=2.3295 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.4230 tok/s=1,478,339 raw_tok/s=1,483,171 step_tokens=130,645 waste=0.3% h100_mfu=36.09% vram=397/17,340MB peak=16,747/17,340MB
[step 996/8250] loss=2.3485 lr=9.22e-05 muon_lr=9.22e-04 grad_norm=0.4383 tok/s=1,341,641 raw_tok/s=1,345,223 step_tokens=130,723 waste=0.3% h100_mfu=32.73% vram=397/17,340MB peak=16,747/17,340MB
[step 997/8250] loss=2.3315 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.3944 tok/s=1,443,321 raw_tok/s=1,445,361 step_tokens=130,887 waste=0.1% h100_mfu=35.17% vram=397/17,340MB peak=16,747/17,340MB
[step 998/8250] loss=2.2938 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.4081 tok/s=1,433,805 raw_tok/s=1,438,744 step_tokens=130,622 waste=0.3% h100_mfu=35.00% vram=397/17,340MB peak=16,747/17,340MB
[step 999/8250] loss=2.3180 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.4413 tok/s=1,439,681 raw_tok/s=1,443,436 step_tokens=130,731 waste=0.3% h100_mfu=35.12% vram=397/17,340MB peak=16,747/17,340MB
[step 1000/8250] loss=2.2854 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.3634 tok/s=1,396,017 raw_tok/s=1,403,265 step_tokens=130,395 waste=0.5% h100_mfu=34.14% vram=397/17,340MB peak=16,747/17,340MB
[step 1000] eval_loss=2.3097
[step 1001/8250] loss=2.3206 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.4165 tok/s=1,318,581 raw_tok/s=1,323,083 step_tokens=130,626 waste=0.3% h100_mfu=32.19% vram=397/17,340MB peak=16,750/17,340MB
[step 1002/8250] loss=2.3526 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.3678 tok/s=1,382,699 raw_tok/s=1,384,188 step_tokens=130,931 waste=0.1% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 1003/8250] loss=2.2834 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.3923 tok/s=1,256,571 raw_tok/s=1,263,008 step_tokens=130,404 waste=0.5% h100_mfu=30.73% vram=397/17,340MB peak=16,747/17,340MB
[step 1004/8250] loss=2.2311 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.4012 tok/s=1,435,742 raw_tok/s=1,440,137 step_tokens=130,672 waste=0.3% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 1005/8250] loss=2.3140 lr=9.21e-05 muon_lr=9.21e-04 grad_norm=0.3416 tok/s=1,489,404 raw_tok/s=1,493,015 step_tokens=130,755 waste=0.2% h100_mfu=36.33% vram=397/17,340MB peak=16,747/17,340MB
[step 1006/8250] loss=2.1987 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.4380 tok/s=1,429,155 raw_tok/s=1,431,339 step_tokens=130,872 waste=0.2% h100_mfu=34.82% vram=397/17,340MB peak=16,747/17,340MB
[step 1007/8250] loss=2.2495 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.3722 tok/s=1,398,952 raw_tok/s=1,399,881 step_tokens=130,985 waste=0.1% h100_mfu=34.06% vram=397/17,340MB peak=16,747/17,340MB
[step 1008/8250] loss=2.3149 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.4038 tok/s=1,433,759 raw_tok/s=1,436,686 step_tokens=130,805 waste=0.2% h100_mfu=34.95% vram=397/17,340MB peak=16,747/17,340MB
[step 1009/8250] loss=2.3300 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.4227 tok/s=1,322,979 raw_tok/s=1,324,020 step_tokens=130,969 waste=0.1% h100_mfu=32.21% vram=397/17,340MB peak=16,747/17,340MB
[step 1010/8250] loss=2.3043 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.3952 tok/s=1,417,776 raw_tok/s=1,423,206 step_tokens=130,572 waste=0.4% h100_mfu=34.63% vram=397/17,340MB peak=16,747/17,340MB
[step 1011/8250] loss=2.2544 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.3881 tok/s=1,464,223 raw_tok/s=1,467,492 step_tokens=130,780 waste=0.2% h100_mfu=35.70% vram=397/17,340MB peak=16,747/17,340MB
[step 1012/8250] loss=2.3163 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.4164 tok/s=1,399,795 raw_tok/s=1,402,106 step_tokens=130,856 waste=0.2% h100_mfu=34.11% vram=397/17,340MB peak=16,747/17,340MB
[step 1013/8250] loss=2.2919 lr=9.20e-05 muon_lr=9.20e-04 grad_norm=0.4290 tok/s=1,363,836 raw_tok/s=1,365,368 step_tokens=130,925 waste=0.1% h100_mfu=33.22% vram=397/17,340MB peak=16,747/17,340MB
[step 1014/8250] loss=2.2896 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.3895 tok/s=1,434,391 raw_tok/s=1,437,813 step_tokens=130,760 waste=0.2% h100_mfu=34.98% vram=397/17,340MB peak=16,747/17,340MB
[step 1015/8250] loss=2.3140 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.4195 tok/s=1,302,171 raw_tok/s=1,304,001 step_tokens=130,888 waste=0.1% h100_mfu=31.73% vram=397/17,340MB peak=16,747/17,340MB
[step 1016/8250] loss=2.3074 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.5666 tok/s=1,479,726 raw_tok/s=1,484,948 step_tokens=130,611 waste=0.4% h100_mfu=36.13% vram=397/17,340MB peak=16,747/17,340MB
[step 1017/8250] loss=2.2859 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.4369 tok/s=1,332,548 raw_tok/s=1,338,953 step_tokens=130,445 waste=0.5% h100_mfu=32.58% vram=397/17,340MB peak=16,747/17,340MB
[step 1018/8250] loss=2.2787 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.4095 tok/s=1,433,831 raw_tok/s=1,436,823 step_tokens=130,799 waste=0.2% h100_mfu=34.96% vram=397/17,340MB peak=16,747/17,340MB
[step 1019/8250] loss=2.3357 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.4039 tok/s=1,362,884 raw_tok/s=1,366,899 step_tokens=130,687 waste=0.3% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 1020/8250] loss=2.3366 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.3913 tok/s=1,355,643 raw_tok/s=1,363,215 step_tokens=130,344 waste=0.6% h100_mfu=33.17% vram=397/17,340MB peak=16,747/17,340MB
[step 1021/8250] loss=2.3346 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.4228 tok/s=1,277,968 raw_tok/s=1,284,061 step_tokens=130,450 waste=0.5% h100_mfu=31.24% vram=397/17,340MB peak=16,747/17,340MB
[step 1022/8250] loss=2.3096 lr=9.19e-05 muon_lr=9.19e-04 grad_norm=0.4586 tok/s=1,428,854 raw_tok/s=1,431,727 step_tokens=130,809 waste=0.2% h100_mfu=34.83% vram=397/17,340MB peak=16,747/17,340MB
[step 1023/8250] loss=2.2816 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.4636 tok/s=1,423,287 raw_tok/s=1,426,268 step_tokens=130,798 waste=0.2% h100_mfu=34.70% vram=397/17,340MB peak=16,747/17,340MB
[step 1024/8250] loss=2.2879 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.4436 tok/s=1,378,196 raw_tok/s=1,380,523 step_tokens=130,851 waste=0.2% h100_mfu=33.59% vram=397/17,340MB peak=16,747/17,340MB
[step 1025/8250] loss=2.3099 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.4024 tok/s=1,359,136 raw_tok/s=1,360,153 step_tokens=130,974 waste=0.1% h100_mfu=33.09% vram=397/17,340MB peak=16,747/17,340MB
[step 1026/8250] loss=2.3202 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.4308 tok/s=1,264,498 raw_tok/s=1,265,338 step_tokens=130,985 waste=0.1% h100_mfu=30.79% vram=397/17,340MB peak=16,747/17,340MB
[step 1027/8250] loss=2.2731 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.4035 tok/s=1,401,298 raw_tok/s=1,406,276 step_tokens=130,608 waste=0.4% h100_mfu=34.21% vram=397/17,340MB peak=16,747/17,340MB
[step 1028/8250] loss=2.2528 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.3992 tok/s=1,402,475 raw_tok/s=1,406,305 step_tokens=130,715 waste=0.3% h100_mfu=34.22% vram=397/17,340MB peak=16,747/17,340MB
[step 1029/8250] loss=2.2232 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.3875 tok/s=1,417,101 raw_tok/s=1,420,059 step_tokens=130,799 waste=0.2% h100_mfu=34.55% vram=397/17,340MB peak=16,747/17,340MB
[step 1030/8250] loss=2.2991 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.4217 tok/s=1,439,131 raw_tok/s=1,442,223 step_tokens=130,791 waste=0.2% h100_mfu=35.09% vram=397/17,340MB peak=16,747/17,340MB
[step 1031/8250] loss=2.3374 lr=9.18e-05 muon_lr=9.18e-04 grad_norm=0.3872 tok/s=1,367,557 raw_tok/s=1,369,783 step_tokens=130,859 waste=0.2% h100_mfu=33.33% vram=397/17,340MB peak=16,747/17,340MB
[step 1032/8250] loss=2.3799 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.3460 tok/s=1,506,732 raw_tok/s=1,510,061 step_tokens=130,783 waste=0.2% h100_mfu=36.74% vram=397/17,340MB peak=16,747/17,340MB
[step 1033/8250] loss=2.3124 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.3531 tok/s=1,472,934 raw_tok/s=1,473,935 step_tokens=130,983 waste=0.1% h100_mfu=35.86% vram=397/17,340MB peak=16,747/17,340MB
[step 1034/8250] loss=2.2572 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.3794 tok/s=1,317,319 raw_tok/s=1,323,245 step_tokens=130,485 waste=0.4% h100_mfu=32.19% vram=397/17,340MB peak=16,747/17,340MB
[step 1035/8250] loss=2.2866 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.3691 tok/s=1,461,418 raw_tok/s=1,463,160 step_tokens=130,916 waste=0.1% h100_mfu=35.60% vram=397/17,340MB peak=16,747/17,340MB
[step 1036/8250] loss=2.3384 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.4017 tok/s=1,325,547 raw_tok/s=1,326,539 step_tokens=130,974 waste=0.1% h100_mfu=32.27% vram=397/17,340MB peak=16,747/17,340MB
[step 1037/8250] loss=2.3393 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.4716 tok/s=1,301,106 raw_tok/s=1,302,497 step_tokens=130,932 waste=0.1% h100_mfu=31.69% vram=397/17,340MB peak=16,747/17,340MB
[step 1038/8250] loss=2.3339 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.4413 tok/s=1,322,451 raw_tok/s=1,324,189 step_tokens=130,900 waste=0.1% h100_mfu=32.22% vram=397/17,340MB peak=16,747/17,340MB
[step 1039/8250] loss=2.2572 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.3658 tok/s=1,387,008 raw_tok/s=1,391,786 step_tokens=130,622 waste=0.3% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 1040/8250] loss=2.2946 lr=9.17e-05 muon_lr=9.17e-04 grad_norm=0.3675 tok/s=1,401,617 raw_tok/s=1,403,620 step_tokens=130,885 waste=0.1% h100_mfu=34.15% vram=397/17,340MB peak=16,747/17,340MB
[step 1041/8250] loss=2.2838 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3804 tok/s=1,487,732 raw_tok/s=1,487,994 step_tokens=131,049 waste=0.0% h100_mfu=36.20% vram=397/17,340MB peak=16,747/17,340MB
[step 1042/8250] loss=2.2636 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3714 tok/s=1,383,102 raw_tok/s=1,383,282 step_tokens=131,055 waste=0.0% h100_mfu=33.66% vram=397/17,340MB peak=16,747/17,340MB
[step 1043/8250] loss=2.2720 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3493 tok/s=1,374,287 raw_tok/s=1,378,631 step_tokens=130,659 waste=0.3% h100_mfu=33.54% vram=397/17,340MB peak=16,747/17,340MB
[step 1044/8250] loss=2.3218 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3636 tok/s=1,379,956 raw_tok/s=1,384,064 step_tokens=130,683 waste=0.3% h100_mfu=33.67% vram=397/17,340MB peak=16,747/17,340MB
[step 1045/8250] loss=2.3331 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3529 tok/s=1,406,814 raw_tok/s=1,413,836 step_tokens=130,421 waste=0.5% h100_mfu=34.40% vram=397/17,340MB peak=16,747/17,340MB
[step 1046/8250] loss=2.2630 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3536 tok/s=1,420,470 raw_tok/s=1,424,993 step_tokens=130,656 waste=0.3% h100_mfu=34.67% vram=397/17,340MB peak=16,747/17,340MB
[step 1047/8250] loss=2.3273 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3631 tok/s=1,476,353 raw_tok/s=1,479,627 step_tokens=130,782 waste=0.2% h100_mfu=36.00% vram=397/17,340MB peak=16,747/17,340MB
[step 1048/8250] loss=2.2886 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.3665 tok/s=1,408,570 raw_tok/s=1,411,230 step_tokens=130,825 waste=0.2% h100_mfu=34.34% vram=397/17,340MB peak=16,747/17,340MB
[step 1049/8250] loss=2.3028 lr=9.16e-05 muon_lr=9.16e-04 grad_norm=0.4528 tok/s=1,400,406 raw_tok/s=1,404,188 step_tokens=130,719 waste=0.3% h100_mfu=34.16% vram=397/17,340MB peak=16,747/17,340MB
[step 1050/8250] loss=2.3274 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.3926 tok/s=1,381,638 raw_tok/s=1,385,412 step_tokens=130,715 waste=0.3% h100_mfu=33.71% vram=397/17,340MB peak=16,747/17,340MB
[step 1051/8250] loss=2.2605 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4290 tok/s=1,460,874 raw_tok/s=1,462,939 step_tokens=130,887 waste=0.1% h100_mfu=35.59% vram=397/17,340MB peak=16,747/17,340MB
[step 1052/8250] loss=2.2999 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4731 tok/s=1,365,064 raw_tok/s=1,368,039 step_tokens=130,787 waste=0.2% h100_mfu=33.28% vram=397/17,340MB peak=16,747/17,340MB
[step 1053/8250] loss=2.2950 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4180 tok/s=1,371,403 raw_tok/s=1,375,096 step_tokens=130,720 waste=0.3% h100_mfu=33.46% vram=397/17,340MB peak=16,747/17,340MB
[step 1054/8250] loss=2.3082 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.3880 tok/s=1,390,844 raw_tok/s=1,392,034 step_tokens=130,960 waste=0.1% h100_mfu=33.87% vram=397/17,340MB peak=16,747/17,340MB
[step 1055/8250] loss=2.2542 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4334 tok/s=1,418,559 raw_tok/s=1,420,032 step_tokens=130,936 waste=0.1% h100_mfu=34.55% vram=397/17,340MB peak=16,747/17,340MB
[step 1056/8250] loss=2.2917 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4171 tok/s=1,350,464 raw_tok/s=1,354,587 step_tokens=130,673 waste=0.3% h100_mfu=32.96% vram=397/17,340MB peak=16,747/17,340MB
[step 1057/8250] loss=2.3079 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4479 tok/s=1,498,259 raw_tok/s=1,503,524 step_tokens=130,613 waste=0.4% h100_mfu=36.58% vram=397/17,340MB peak=16,747/17,340MB
[step 1058/8250] loss=2.3303 lr=9.15e-05 muon_lr=9.15e-04 grad_norm=0.4295 tok/s=1,488,689 raw_tok/s=1,490,611 step_tokens=130,903 waste=0.1% h100_mfu=36.27% vram=397/17,340MB peak=16,747/17,340MB
[step 1059/8250] loss=2.3103 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.4328 tok/s=1,418,996 raw_tok/s=1,422,697 step_tokens=130,731 waste=0.3% h100_mfu=34.61% vram=397/17,340MB peak=16,747/17,340MB
[step 1060/8250] loss=2.2431 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.3996 tok/s=1,363,094 raw_tok/s=1,366,002 step_tokens=130,793 waste=0.2% h100_mfu=33.23% vram=397/17,340MB peak=16,747/17,340MB
[step 1061/8250] loss=2.2709 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.4522 tok/s=1,331,789 raw_tok/s=1,333,457 step_tokens=130,908 waste=0.1% h100_mfu=32.44% vram=397/17,340MB peak=16,747/17,340MB
[step 1062/8250] loss=2.2678 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.5287 tok/s=1,377,713 raw_tok/s=1,381,571 step_tokens=130,706 waste=0.3% h100_mfu=33.61% vram=397/17,340MB peak=16,747/17,340MB
[step 1063/8250] loss=2.2982 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.4030 tok/s=1,375,693 raw_tok/s=1,379,947 step_tokens=130,668 waste=0.3% h100_mfu=33.57% vram=397/17,340MB peak=16,747/17,340MB
[step 1064/8250] loss=2.2504 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.4100 tok/s=1,385,816 raw_tok/s=1,391,687 step_tokens=130,519 waste=0.4% h100_mfu=33.86% vram=397/17,340MB peak=16,747/17,340MB
[step 1065/8250] loss=2.2784 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.3999 tok/s=1,326,406 raw_tok/s=1,329,174 step_tokens=130,799 waste=0.2% h100_mfu=32.34% vram=397/17,340MB peak=16,747/17,340MB
[step 1066/8250] loss=2.2979 lr=9.14e-05 muon_lr=9.14e-04 grad_norm=0.4284 tok/s=1,365,634 raw_tok/s=1,367,428 step_tokens=130,900 waste=0.1% h100_mfu=33.27% vram=397/17,340MB peak=16,747/17,340MB
[step 1067/8250] loss=2.3029 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.3757 tok/s=1,439,758 raw_tok/s=1,444,818 step_tokens=130,613 waste=0.4% h100_mfu=35.15% vram=397/17,340MB peak=16,747/17,340MB
[step 1068/8250] loss=2.3021 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.3450 tok/s=1,442,655 raw_tok/s=1,445,103 step_tokens=130,850 waste=0.2% h100_mfu=35.16% vram=397/17,340MB peak=16,747/17,340MB
[step 1069/8250] loss=2.2728 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.3921 tok/s=1,437,908 raw_tok/s=1,441,063 step_tokens=130,785 waste=0.2% h100_mfu=35.06% vram=397/17,340MB peak=16,747/17,340MB
[step 1070/8250] loss=2.2526 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.3744 tok/s=1,498,725 raw_tok/s=1,500,316 step_tokens=130,933 waste=0.1% h100_mfu=36.50% vram=397/17,340MB peak=16,747/17,340MB
[step 1071/8250] loss=2.2685 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.4118 tok/s=1,362,961 raw_tok/s=1,365,858 step_tokens=130,794 waste=0.2% h100_mfu=33.23% vram=397/17,340MB peak=16,747/17,340MB
[step 1072/8250] loss=2.3127 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.4079 tok/s=1,399,619 raw_tok/s=1,403,173 step_tokens=130,740 waste=0.3% h100_mfu=34.14% vram=397/17,340MB peak=16,747/17,340MB
[step 1073/8250] loss=2.2976 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.4149 tok/s=1,391,473 raw_tok/s=1,396,951 step_tokens=130,558 waste=0.4% h100_mfu=33.99% vram=397/17,340MB peak=16,747/17,340MB
[step 1074/8250] loss=2.2330 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.4234 tok/s=1,436,417 raw_tok/s=1,438,744 step_tokens=130,860 waste=0.2% h100_mfu=35.00% vram=397/17,340MB peak=16,747/17,340MB
[step 1075/8250] loss=2.3454 lr=9.13e-05 muon_lr=9.13e-04 grad_norm=0.4322 tok/s=1,416,884 raw_tok/s=1,423,291 step_tokens=130,482 waste=0.5% h100_mfu=34.63% vram=397/17,340MB peak=16,747/17,340MB
[step 1076/8250] loss=2.2875 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.4842 tok/s=1,382,574 raw_tok/s=1,386,923 step_tokens=130,661 waste=0.3% h100_mfu=33.74% vram=397/17,340MB peak=16,747/17,340MB
[step 1077/8250] loss=2.2440 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.5042 tok/s=1,467,059 raw_tok/s=1,468,527 step_tokens=130,941 waste=0.1% h100_mfu=35.73% vram=397/17,340MB peak=16,747/17,340MB
[step 1078/8250] loss=2.2308 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.4499 tok/s=1,378,354 raw_tok/s=1,384,290 step_tokens=130,510 waste=0.4% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 1079/8250] loss=2.2603 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.4613 tok/s=1,388,381 raw_tok/s=1,388,900 step_tokens=131,023 waste=0.0% h100_mfu=33.79% vram=397/17,340MB peak=16,747/17,340MB
[step 1080/8250] loss=2.2379 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.5280 tok/s=1,367,173 raw_tok/s=1,370,917 step_tokens=130,714 waste=0.3% h100_mfu=33.35% vram=397/17,340MB peak=16,747/17,340MB
[step 1081/8250] loss=2.3173 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.4331 tok/s=1,373,089 raw_tok/s=1,373,236 step_tokens=131,058 waste=0.0% h100_mfu=33.41% vram=397/17,340MB peak=16,747/17,340MB
[step 1082/8250] loss=2.2717 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.3973 tok/s=1,382,234 raw_tok/s=1,383,807 step_tokens=130,923 waste=0.1% h100_mfu=33.67% vram=397/17,340MB peak=16,747/17,340MB
[step 1083/8250] loss=2.2582 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.4630 tok/s=1,437,944 raw_tok/s=1,440,406 step_tokens=130,848 waste=0.2% h100_mfu=35.05% vram=397/17,340MB peak=16,747/17,340MB
[step 1084/8250] loss=2.2799 lr=9.12e-05 muon_lr=9.12e-04 grad_norm=0.4493 tok/s=1,433,788 raw_tok/s=1,436,495 step_tokens=130,825 waste=0.2% h100_mfu=34.95% vram=397/17,340MB peak=16,747/17,340MB
[step 1085/8250] loss=2.2511 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4230 tok/s=1,399,367 raw_tok/s=1,401,013 step_tokens=130,918 waste=0.1% h100_mfu=34.09% vram=397/17,340MB peak=16,747/17,340MB
[step 1086/8250] loss=2.3070 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4017 tok/s=1,360,335 raw_tok/s=1,361,134 step_tokens=130,995 waste=0.1% h100_mfu=33.12% vram=397/17,340MB peak=16,747/17,340MB
[step 1087/8250] loss=2.2885 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4083 tok/s=1,494,792 raw_tok/s=1,496,573 step_tokens=130,916 waste=0.1% h100_mfu=36.41% vram=397/17,340MB peak=16,747/17,340MB
[step 1088/8250] loss=2.2949 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.3845 tok/s=1,487,671 raw_tok/s=1,491,791 step_tokens=130,710 waste=0.3% h100_mfu=36.30% vram=397/17,340MB peak=16,747/17,340MB
[step 1089/8250] loss=2.2673 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4505 tok/s=1,412,892 raw_tok/s=1,416,307 step_tokens=130,756 waste=0.2% h100_mfu=34.46% vram=397/17,340MB peak=16,747/17,340MB
[step 1090/8250] loss=2.2706 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4574 tok/s=1,427,171 raw_tok/s=1,428,556 step_tokens=130,945 waste=0.1% h100_mfu=34.76% vram=397/17,340MB peak=16,747/17,340MB
[step 1091/8250] loss=2.2621 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4027 tok/s=1,375,245 raw_tok/s=1,376,505 step_tokens=130,952 waste=0.1% h100_mfu=33.49% vram=397/17,340MB peak=16,747/17,340MB
[step 1092/8250] loss=2.2581 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4083 tok/s=1,440,445 raw_tok/s=1,442,326 step_tokens=130,901 waste=0.1% h100_mfu=35.09% vram=397/17,340MB peak=16,747/17,340MB
[step 1093/8250] loss=2.3055 lr=9.11e-05 muon_lr=9.11e-04 grad_norm=0.4183 tok/s=1,427,841 raw_tok/s=1,430,493 step_tokens=130,829 waste=0.2% h100_mfu=34.80% vram=397/17,340MB peak=16,747/17,340MB
[step 1094/8250] loss=2.2709 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.3620 tok/s=1,438,697 raw_tok/s=1,441,700 step_tokens=130,799 waste=0.2% h100_mfu=35.08% vram=397/17,340MB peak=16,747/17,340MB
[step 1095/8250] loss=2.3061 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.3542 tok/s=1,426,443 raw_tok/s=1,430,744 step_tokens=130,678 waste=0.3% h100_mfu=34.81% vram=397/17,340MB peak=16,747/17,340MB
[step 1096/8250] loss=2.2806 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.3607 tok/s=1,417,578 raw_tok/s=1,423,573 step_tokens=130,520 waste=0.4% h100_mfu=34.64% vram=397/17,340MB peak=16,747/17,340MB
[step 1097/8250] loss=2.2502 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.3711 tok/s=1,421,321 raw_tok/s=1,423,939 step_tokens=130,831 waste=0.2% h100_mfu=34.64% vram=397/17,340MB peak=16,747/17,340MB
[step 1098/8250] loss=2.2633 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.3885 tok/s=1,446,550 raw_tok/s=1,448,982 step_tokens=130,852 waste=0.2% h100_mfu=35.25% vram=397/17,340MB peak=16,747/17,340MB
[step 1099/8250] loss=2.2638 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.4319 tok/s=1,345,611 raw_tok/s=1,347,688 step_tokens=130,870 waste=0.2% h100_mfu=32.79% vram=397/17,340MB peak=16,747/17,340MB
[step 1100/8250] loss=2.2173 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.4063 tok/s=1,433,594 raw_tok/s=1,434,426 step_tokens=130,996 waste=0.1% h100_mfu=34.90% vram=397/17,340MB peak=16,747/17,340MB
[step 1101/8250] loss=2.2306 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.4164 tok/s=1,444,671 raw_tok/s=1,447,753 step_tokens=130,793 waste=0.2% h100_mfu=35.22% vram=397/17,340MB peak=16,747/17,340MB
[step 1102/8250] loss=2.2346 lr=9.10e-05 muon_lr=9.10e-04 grad_norm=0.4065 tok/s=1,389,626 raw_tok/s=1,394,350 step_tokens=130,628 waste=0.3% h100_mfu=33.92% vram=397/17,340MB peak=16,747/17,340MB
[step 1103/8250] loss=2.2856 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.4636 tok/s=1,416,828 raw_tok/s=1,420,176 step_tokens=130,763 waste=0.2% h100_mfu=34.55% vram=397/17,340MB peak=16,747/17,340MB
[step 1104/8250] loss=2.2822 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3967 tok/s=1,426,358 raw_tok/s=1,429,937 step_tokens=130,744 waste=0.3% h100_mfu=34.79% vram=397/17,340MB peak=16,747/17,340MB
[step 1105/8250] loss=2.2343 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3875 tok/s=1,262,546 raw_tok/s=1,268,964 step_tokens=130,409 waste=0.5% h100_mfu=30.87% vram=397/17,340MB peak=16,747/17,340MB
[step 1106/8250] loss=2.2529 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3715 tok/s=1,270,361 raw_tok/s=1,273,850 step_tokens=130,713 waste=0.3% h100_mfu=30.99% vram=397/17,340MB peak=16,747/17,340MB
[step 1107/8250] loss=2.3228 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3636 tok/s=1,443,429 raw_tok/s=1,445,778 step_tokens=130,859 waste=0.2% h100_mfu=35.18% vram=397/17,340MB peak=16,747/17,340MB
[step 1108/8250] loss=2.2275 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3357 tok/s=1,380,670 raw_tok/s=1,384,271 step_tokens=130,731 waste=0.3% h100_mfu=33.68% vram=397/17,340MB peak=16,747/17,340MB
[step 1109/8250] loss=2.2721 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3924 tok/s=1,432,682 raw_tok/s=1,436,221 step_tokens=130,749 waste=0.2% h100_mfu=34.94% vram=397/17,340MB peak=16,747/17,340MB
[step 1110/8250] loss=2.2829 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.3769 tok/s=1,438,882 raw_tok/s=1,442,073 step_tokens=130,782 waste=0.2% h100_mfu=35.09% vram=397/17,340MB peak=16,747/17,340MB
[step 1111/8250] loss=2.2378 lr=9.09e-05 muon_lr=9.09e-04 grad_norm=0.4428 tok/s=1,413,143 raw_tok/s=1,414,200 step_tokens=130,974 waste=0.1% h100_mfu=34.41% vram=397/17,340MB peak=16,747/17,340MB
[step 1112/8250] loss=2.2558 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.3759 tok/s=1,404,160 raw_tok/s=1,411,342 step_tokens=130,405 waste=0.5% h100_mfu=34.34% vram=397/17,340MB peak=16,747/17,340MB
[step 1113/8250] loss=2.2692 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.3883 tok/s=1,339,189 raw_tok/s=1,342,877 step_tokens=130,712 waste=0.3% h100_mfu=32.67% vram=397/17,340MB peak=16,747/17,340MB
[step 1114/8250] loss=2.2855 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.3874 tok/s=1,489,468 raw_tok/s=1,490,127 step_tokens=131,014 waste=0.0% h100_mfu=36.25% vram=397/17,340MB peak=16,747/17,340MB
[step 1115/8250] loss=2.2033 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.3833 tok/s=1,439,423 raw_tok/s=1,441,105 step_tokens=130,919 waste=0.1% h100_mfu=35.06% vram=397/17,340MB peak=16,747/17,340MB
[step 1116/8250] loss=2.2747 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.4088 tok/s=1,343,172 raw_tok/s=1,349,886 step_tokens=130,420 waste=0.5% h100_mfu=32.84% vram=397/17,340MB peak=16,747/17,340MB
[step 1117/8250] loss=2.3130 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.3592 tok/s=1,436,513 raw_tok/s=1,440,359 step_tokens=130,722 waste=0.3% h100_mfu=35.04% vram=397/17,340MB peak=16,747/17,340MB
[step 1118/8250] loss=2.2492 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.3950 tok/s=1,282,422 raw_tok/s=1,283,715 step_tokens=130,940 waste=0.1% h100_mfu=31.23% vram=397/17,340MB peak=16,747/17,340MB
[step 1119/8250] loss=2.3182 lr=9.08e-05 muon_lr=9.08e-04 grad_norm=0.4180 tok/s=1,417,630 raw_tok/s=1,419,818 step_tokens=130,870 waste=0.2% h100_mfu=34.54% vram=397/17,340MB peak=16,747/17,340MB
[step 1120/8250] loss=2.2484 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.4497 tok/s=1,367,591 raw_tok/s=1,371,284 step_tokens=130,719 waste=0.3% h100_mfu=33.36% vram=397/17,340MB peak=16,747/17,340MB
[step 1121/8250] loss=2.2639 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3883 tok/s=1,429,174 raw_tok/s=1,432,310 step_tokens=130,785 waste=0.2% h100_mfu=34.85% vram=397/17,340MB peak=16,747/17,340MB
[step 1122/8250] loss=2.2795 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3930 tok/s=1,378,642 raw_tok/s=1,381,266 step_tokens=130,823 waste=0.2% h100_mfu=33.61% vram=397/17,340MB peak=16,747/17,340MB
[step 1123/8250] loss=2.1980 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3857 tok/s=1,436,136 raw_tok/s=1,437,331 step_tokens=130,963 waste=0.1% h100_mfu=34.97% vram=397/17,340MB peak=16,747/17,340MB
[step 1124/8250] loss=2.2631 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3739 tok/s=1,489,628 raw_tok/s=1,491,745 step_tokens=130,886 waste=0.1% h100_mfu=36.29% vram=397/17,340MB peak=16,747/17,340MB
[step 1125/8250] loss=2.2019 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.4035 tok/s=1,439,643 raw_tok/s=1,444,017 step_tokens=130,675 waste=0.3% h100_mfu=35.13% vram=397/17,340MB peak=16,747/17,340MB
[step 1126/8250] loss=2.3189 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.4494 tok/s=1,508,948 raw_tok/s=1,516,177 step_tokens=130,447 waste=0.5% h100_mfu=36.89% vram=397/17,340MB peak=16,747/17,340MB
[step 1127/8250] loss=2.3065 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3634 tok/s=1,414,122 raw_tok/s=1,415,202 step_tokens=130,972 waste=0.1% h100_mfu=34.43% vram=397/17,340MB peak=16,747/17,340MB
[step 1128/8250] loss=2.2509 lr=9.07e-05 muon_lr=9.07e-04 grad_norm=0.3936 tok/s=1,465,364 raw_tok/s=1,466,785 step_tokens=130,945 waste=0.1% h100_mfu=35.69% vram=397/17,340MB peak=16,747/17,340MB
[step 1129/8250] loss=2.3360 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4687 tok/s=1,465,029 raw_tok/s=1,467,772 step_tokens=130,827 waste=0.2% h100_mfu=35.71% vram=397/17,340MB peak=16,747/17,340MB
[step 1130/8250] loss=2.2387 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4394 tok/s=1,441,311 raw_tok/s=1,447,285 step_tokens=130,531 waste=0.4% h100_mfu=35.21% vram=397/17,340MB peak=16,747/17,340MB
[step 1131/8250] loss=2.2337 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4311 tok/s=1,465,746 raw_tok/s=1,468,401 step_tokens=130,835 waste=0.2% h100_mfu=35.73% vram=397/17,340MB peak=16,747/17,340MB
[step 1132/8250] loss=2.2532 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4849 tok/s=1,447,591 raw_tok/s=1,450,978 step_tokens=130,766 waste=0.2% h100_mfu=35.30% vram=397/17,340MB peak=16,747/17,340MB
[step 1133/8250] loss=2.3240 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4346 tok/s=1,398,286 raw_tok/s=1,401,708 step_tokens=130,752 waste=0.2% h100_mfu=34.10% vram=397/17,340MB peak=16,747/17,340MB
[step 1134/8250] loss=2.2607 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4258 tok/s=1,397,607 raw_tok/s=1,400,481 step_tokens=130,803 waste=0.2% h100_mfu=34.07% vram=397/17,340MB peak=16,747/17,340MB
[step 1135/8250] loss=2.2994 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4689 tok/s=1,378,919 raw_tok/s=1,380,341 step_tokens=130,937 waste=0.1% h100_mfu=33.58% vram=397/17,340MB peak=16,747/17,340MB
[step 1136/8250] loss=2.2355 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.4253 tok/s=1,325,255 raw_tok/s=1,328,772 step_tokens=130,725 waste=0.3% h100_mfu=32.33% vram=397/17,340MB peak=16,747/17,340MB
[step 1137/8250] loss=2.1701 lr=9.06e-05 muon_lr=9.06e-04 grad_norm=0.3956 tok/s=1,407,897 raw_tok/s=1,410,318 step_tokens=130,847 waste=0.2% h100_mfu=34.31% vram=397/17,340MB peak=16,747/17,340MB
[step 1138/8250] loss=2.2279 lr=9.05e-05 muon_lr=9.05e-04 grad_norm=0.4207 tok/s=1,412,936 raw_tok/s=1,416,578 step_tokens=130,735 waste=0.3% h100_mfu=34.47% vram=397/17,340MB peak=16,747/17,340MB
[step 1139/8250] loss=2.3004 lr=9.05e-05 muon_lr=9.05e-04 grad_norm=0.3356 tok/s=1,350,241 raw_tok/s=1,353,939 step_tokens=130,714 waste=0.3% h100_mfu=32.94% vram=397/17,340MB peak=16,747/17,340MB
[step 1140/8250] loss=2.2550 lr=9.05e-05 muon_lr=9.05e-04 grad_norm=0.3924 tok/s=1,429,943 raw_tok/s=1,432,205 step_tokens=130,865 waste=0.2% h100_mfu=34.85% vram=397/17,340MB peak=16,747/17,340MB
[step 1141/8250] loss=2.2179 lr=9.05e-05 muon_lr=9.05e-04 grad_norm=0.3951 tok/s=1,363,710 raw_tok/s=1,367,111 step_tokens=130,746 waste=0.2% h100_mfu=33.26% vram=397/17,340MB peak=16,747/17,340MB
[step 1142/8250] loss=2.2416 lr=9.05e-05 muon_lr=9.05e-04 grad_norm=0.3686 tok/s=1,396,719 raw_tok/s=1,398,213 step_tokens=130,932 waste=0.1% h100_mfu=34.02% vram=397/17,340MB peak=16,747/17,340MB
