# Pretraining configuration for nanoPLM
#
# IMPORTANT: Before running pretraining, ensure you have prepared your data with:
#   1. Set pipeline_mode: 'pretrain' in params.yaml
#   2. Run: nanoplm data from-yaml
# This will generate binary shards and a .data_manifest file.

model:
  hidden_size: 768
  intermediate_size: 1536
  num_hidden_layers: 12
  num_attention_heads: 8
  vocab_size: 32
  mlp_activation: "swiglu"
  mlp_dropout: 0.0
  mlp_bias: false
  attention_bias: false
  attention_dropout: 0.0
  classifier_activation: "gelu"
  # The options below only work on pure-torch and TE pipelines
  use_resid_lambdas: false  # scales residual stream per layer
  use_x0_lambdas: false  # blends initial embedding x0 per layer
  use_qk_norm: false  # applies RMS norm to Q/K in attention

pretraining:
  # Dataset directory (contains .data_manifest from nanoplm data from-yaml)
  # Note: paths are RELATIVE to where you RUN the command, NOT the YAML file.
  dataset_dir: "output/data/pretrain_data"

  # Output model path
  ckp_dir: "output/pretraining_checkpoints"

  # Hyperparameters
  #   micro_batch_size: samples per GPU per forward pass (limited by GPU memory)
  #   global_batch_size: total tokens per optimizer step across all GPUs
  #   gradient_accumulation_steps is inferred automatically:
  #     grad_accum = ceil(global_batch_size / (micro_batch_size * max_seq_len * num_gpus))
  micro_batch_size: 128
  global_batch_size: 128000  # 2^20 â‰ˆ 1M tokens/step (based on PLM best practices)
  num_epochs: 10

  optimizer: "normuon"  # adamw, stable_adamw, muon, normuon
  # AdamW hyperparameters (also used for AdamW side [1D and embedding/unembed params] when optimizer=muon or normuon)
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  learning_rate: 1e-4  # AdamW LR (Muon uses muon_learning_rate)
  warmup_steps: 302
  lr_decay_to_fraction: 0.1
  lr_schedule: "cosine"
  weight_decay: 0.0
  # Muon/NorMuon hyperparameters (used only when optimizer: muon or normuon)
  muon_learning_rate: 1e-3
  muon_weight_decay: 0.01
  muon_cautious_weight_decay: true
  muon_use_polar_express: false
  muon_momentum: 0.95
  muon_nesterov: true
  muon_eps: 1e-7
  mlm_probability: 0.3
  mask_replace_prob: 0.8
  random_token_prob: 0.1
  keep_probability: 0.1
  logging_steps: 1
  eval_steps: 250
  save_steps: 5000
  seed: 42
  num_workers: "auto"
  prefetch_factor: 2
  # Sequence packing: concatenates shorter sequences into fewer rows to eliminate
  # padding waste and increase GPU utilization. Requires flash attention and --pure-torch/--pure-te
  use_packing: true
  # Experimental throughput optimization: with packing, enables static input sizes which enables the use of torch.compile(dynamic=False) and cudagraphs
  use_static_inp_size: true 

  # Mixed precision training (recommended: keep enabled for 1.5-3x speedup)
  # When bf16 is true, automatically selects the best precision for your hardware:
  #   - CUDA Ampere+ (A100, RTX 3090+): bf16 + TF32
  #   - CUDA Volta/Turing (V100, RTX 2080): fp16 fallback
  #   - Apple Silicon (M1/M2/M3): fp16 (hardware accelerated)
  #   - CPU: fp32 (no mixed precision)
  bf16: true
  tf32: true  # TF32 mode on Ampere+ CUDA GPUs only (automatically not used on MPS/CPU)
             # Provides 3x faster fp32 matmuls with negligible precision loss
  fp8: true  # Enable FP8 Linear matmuls in pure_torch/pure_te paths (CUDA, best on H100+)

  multi_gpu: true
  world_size: 2  # Use "auto" if you want to use all available GPUs
  project_name: "nanoplm-pretraining"

resume:
  # Set is_resume: true to resume training from a checkpoint
  # When resuming, the model, tokenizer, and training state will be loaded from checkpoint_dir
  # extra_epochs: adds to 'pretraining.num_epochs' to define total epochs.
  is_resume: false
  checkpoint_dir: "output/pretraining_checkpoints/run-1/checkpoint-1"
  extra_epochs: 0

# Set pure_torch: true to use the custom pure-torch model and training loop
# instead of HF Trainer. CLI equivalent: --pure-torch
# pure_torch: false
# Set pure_te: true to use Transformer Engine model and training loop.
# CLI equivalent: --pure-te (mutually exclusive with pure_torch)
# pure_te: false
