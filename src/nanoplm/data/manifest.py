"""
Data manifest utilities for nanoPLM.

The manifest serves as a contract between data preparation and training pipelines,
storing dataset metadata to eliminate configuration duplication.
"""

from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Union
import yaml


MANIFEST_FILENAME = ".data_manifest"


@dataclass
class DataManifestBase:
    """Base class containing common fields for all manifests."""

    # Pipeline identification
    pipeline_mode: str  # "distillation" or "pretrain"

    # Data parameters
    seqs_num: int
    min_seq_len: int
    max_seq_len: int
    val_ratio: float

    # Directory structure
    train_dir: str
    val_dir: str

    # Sequence counts
    train_sequences: int
    val_sequences: int

    # Metadata
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> Dict:
        """Convert manifest to dictionary for serialization."""
        return asdict(self)


@dataclass
class PretrainManifest(DataManifestBase):
    """Manifest for pretraining pipeline.

    Contains shard configuration for HDF5 files used in MLM pretraining.
    """

    # Shard configuration
    sharded: bool = True
    samples_per_shard: int = 0

    def __post_init__(self):
        """Set pipeline mode and validate."""
        self.pipeline_mode = "pretrain"
        if not self.sharded:
            raise ValueError("Pretraining requires sharded=True")
        if self.samples_per_shard <= 0:
            raise ValueError("Pretraining requires samples_per_shard > 0")


@dataclass
class DistillationManifest(DataManifestBase):
    """Manifest for distillation pipeline.

    Supports two modes:
    1. Pre-computed mode (on_the_fly=False): Teacher embeddings stored in HDF5 files
    2. On-the-fly mode (on_the_fly=True): Teacher embeddings computed during training
    """

    # Required fields
    teacher_model: str = ""
    on_the_fly: bool = False

    # Pre-computed mode fields (on_the_fly=False)
    sharded: Optional[bool] = None
    samples_per_shard: Optional[int] = None
    train_h5_prefix: Optional[str] = None
    val_h5_prefix: Optional[str] = None

    # On-the-fly mode fields (on_the_fly=True)
    train_fasta: Optional[str] = None
    val_fasta: Optional[str] = None

    def __post_init__(self):
        """Set pipeline mode and validate based on on_the_fly flag."""
        self.pipeline_mode = "distillation"

        if not self.teacher_model:
            raise ValueError("Distillation requires teacher_model to be specified")

        if self.on_the_fly:
            # On-the-fly mode: must have FASTA paths
            if not self.train_fasta or not self.val_fasta:
                raise ValueError(
                    "On-the-fly distillation mode requires train_fasta and val_fasta"
                )
            # Clear pre-computed mode fields for clarity
            self.sharded = None
            self.samples_per_shard = None
            self.train_h5_prefix = None
            self.val_h5_prefix = None
        else:
            # Pre-computed mode: must have shard info and H5 prefixes
            if not self.sharded:
                raise ValueError("Pre-computed distillation mode requires sharded=True")
            if not self.samples_per_shard or self.samples_per_shard <= 0:
                raise ValueError(
                    "Pre-computed distillation mode requires samples_per_shard > 0"
                )
            if not self.train_h5_prefix or not self.val_h5_prefix:
                raise ValueError(
                    "Pre-computed distillation mode requires train_h5_prefix and val_h5_prefix"
                )
            # Clear on-the-fly mode fields for clarity
            self.train_fasta = None
            self.val_fasta = None


# Type alias for any manifest type
DataManifest = Union[PretrainManifest, DistillationManifest]


def write_manifest(output_dir: Union[str, Path], manifest: DataManifest) -> Path:
    """Write manifest to the output directory.

    Args:
        output_dir: Directory to write manifest to
        manifest: PretrainManifest or DistillationManifest object to write

    Returns:
        Path to the written manifest file
    """
    output_dir = Path(output_dir)
    manifest_path = output_dir / MANIFEST_FILENAME

    # Create directory if it doesn't exist
    output_dir.mkdir(parents=True, exist_ok=True)

    with open(manifest_path, "w", encoding="utf-8") as f:
        # Write autogenerated warning header
        f.write("# This file is AUTO-GENERATED by 'nanoplm data from-yaml'.\n")
        f.write("# DO NOT EDIT THIS FILE MANUALLY.\n")
        f.write("# To regenerate, run 'nanoplm data from-yaml' again with updated params.yaml.\n")
        f.write("\n")

        # Convert to dict and filter out None values for cleaner YAML
        manifest_dict = {k: v for k, v in manifest.to_dict().items() if v is not None}
        yaml.dump(manifest_dict, f, default_flow_style=False, sort_keys=False)

    return manifest_path


def read_manifest(dataset_dir: Union[str, Path]) -> DataManifest:
    """Read manifest from a dataset directory.

    Args:
        dataset_dir: Directory containing the manifest file

    Returns:
        PretrainManifest or DistillationManifest object based on pipeline_mode

    Raises:
        FileNotFoundError: If manifest file doesn't exist
        ValueError: If manifest file is invalid or has unknown pipeline_mode
    """
    dataset_dir = Path(dataset_dir)
    manifest_path = dataset_dir / MANIFEST_FILENAME

    if not manifest_path.exists():
        raise FileNotFoundError(
            f"Dataset manifest not found at {manifest_path}.\n"
            "Please run 'nanoplm data from-yaml' to generate the dataset first."
        )

    with open(manifest_path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)

    if data is None:
        raise ValueError(f"Manifest file is empty: {manifest_path}")

    # Determine which manifest type to create based on pipeline_mode
    pipeline_mode = data.get("pipeline_mode")
    if not pipeline_mode:
        raise ValueError(
            f"Manifest missing required field 'pipeline_mode': {manifest_path}"
        )

    try:
        if pipeline_mode == "pretrain":
            return PretrainManifest(**data)
        elif pipeline_mode == "distillation":
            return DistillationManifest(**data)
        else:
            raise ValueError(
                f"Unknown pipeline_mode '{pipeline_mode}'. "
                f"Expected 'pretrain' or 'distillation'."
            )
    except TypeError as e:
        raise ValueError(f"Invalid manifest format for {pipeline_mode} mode: {e}")


def validate_manifest_for_pipeline(manifest: DataManifest, expected_mode: str) -> None:
    """Validate that manifest matches expected pipeline mode.

    Args:
        manifest: DataManifest to validate
        expected_mode: Expected pipeline_mode ("distillation" or "pretrain")

    Raises:
        ValueError: If manifest doesn't match expected mode
    """
    if manifest.pipeline_mode != expected_mode:
        raise ValueError(
            f"Dataset was prepared for '{manifest.pipeline_mode}' pipeline, "
            f"but '{expected_mode}' was expected.\n"
            f"Please use a dataset prepared with pipeline_mode: '{expected_mode}' in params.yaml."
        )


def get_dataset_paths(
    dataset_dir: Union[str, Path], manifest: DataManifest
) -> Dict[str, Path]:
    """Get resolved dataset paths from manifest.

    Args:
        dataset_dir: Root directory containing the dataset
        manifest: PretrainManifest or DistillationManifest with path information

    Returns:
        Dictionary with resolved paths:
        - train_dir: Path to training data directory
        - val_dir: Path to validation data directory

        For PretrainManifest or pre-computed DistillationManifest:
        - train_h5_prefix: Path to training H5 prefix (if applicable)
        - val_h5_prefix: Path to validation H5 prefix (if applicable)

        For on-the-fly DistillationManifest:
        - train_fasta: Path to training FASTA file
        - val_fasta: Path to validation FASTA file
    """
    dataset_dir = Path(dataset_dir)

    paths = {
        "train_dir": dataset_dir / manifest.train_dir,
        "val_dir": dataset_dir / manifest.val_dir,
    }

    # Add pipeline-specific paths
    if isinstance(manifest, DistillationManifest):
        if manifest.on_the_fly:
            # On-the-fly mode: add FASTA paths
            if manifest.train_fasta:
                paths["train_fasta"] = dataset_dir / manifest.train_fasta
            if manifest.val_fasta:
                paths["val_fasta"] = dataset_dir / manifest.val_fasta
        else:
            # Pre-computed mode: add H5 prefix paths
            if manifest.train_h5_prefix:
                paths["train_h5_prefix"] = (
                    dataset_dir / manifest.train_dir / manifest.train_h5_prefix
                )
            if manifest.val_h5_prefix:
                paths["val_h5_prefix"] = (
                    dataset_dir / manifest.val_dir / manifest.val_h5_prefix
                )

    return paths
